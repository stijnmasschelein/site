---
title: "Research Design 2: Event Studies and Difference-in-Difference"
execute:
    dpi: 200
    dev: svg"
    fig-height: 5
    fig-width: 10
---

```{r}
#| label: setup
#| echo: false
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot(font_size = 18))
library(DiagrammeR)
library(fixest)
library(modelsummary)
library(here)
library(kableExtra)
i_am("slides/slides5.qmd")
uwa_blue <- "#003087"
uwa_gold <- "#DAAA00"
gof_omit <- "Adj|IC|Log|Pseudo"
stars <- c("*" = 0.1, "**" = 0.05, "***" = 0.01)
```

# Event Studies
## A basic before - after comparison

```{dot}
//|echo: false
digraph event{
node [shape = box]
subgraph{
  rank = same; "Event Happened", Treatment, Outcome
}
"Event Happened" -> Treatment -> Outcome
Time -> {"Event Happened", Outcome}
}
```

[Chapter 17 Event Studies](https://theeffectbook.net/ch-EventStudies.html) in @huntington-klein2021

::: {.notes}
Where Time could be standing in for a lot of other annoying things that might happen.
:::

## Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)

```{r}
#| warning: false
#| echo: false
days <- -60:60
log_return_prior <- rnorm(60, 1.15, .3)
log_return_control <- rnorm(61, 1.15, .3)
log_return_treatment <- c(rnorm(61, 1.85, .3))
no_time_trend <-
  ggplot(mapping = aes(x = days,
                       y = c(log_return_prior,log_return_treatment))) +
  geom_line(colour = uwa_blue) +
  geom_line(aes(y = c(rep(NA, 60), log_return_control)), colour = uwa_gold) +
  ylab("log(return)") +
  ggtitle("No Time Trend") +
  geom_vline(xintercept = 0,  linetype = "longdash", colour = uwa_blue)
```

```{r}
#| warning: false
#| echo: false
log_return_prior_time <- rnorm(60, days * 0.01 + 1.15, .3)
log_return_control_time <- rnorm(61, (61 + days) * 0.01 + 1.15, .3)
log_return_treatment_time <- c(rnorm(61, (61 + days) * 0.01 + 0.6, .3))
time_trend <-
  ggplot(mapping = aes(x = days,
                       y = c(log_return_prior_time,
                             log_return_treatment_time))) +
  geom_line(colour = uwa_blue) +
  geom_line(aes(y = c(rep(NA, 60), log_return_control_time)),
            colour = uwa_gold) +
  ylab("log(return)") +
  ggtitle("With a Time Trend") +
  geom_vline(xintercept = 0,  linetype = "longdash", colour = uwa_blue)
```

::::: {.columns}
:::: {.column}

```{r}
#| warning: false
#| echo: false
#| fig-width: 5
print(no_time_trend)
```

::::
:::: {.column}

::: {.fragment}
```{r}
#| warning: false
#| echo: false
#| fig-width: 5
print(time_trend)
```
:::
::::
:::::

::: {.notes}
We do not observe the yellow/gold returns. We have to estimate them or convince the reader that there are no trends to be expected for theoretical/institutional reasons. I will not go into the details of the estimation here but I will in week 7.
:::

## Front-running, information leaking, and anticipation are all annoying. {.smaller}

```{dot}
//|echo: false
//|fig-height: 4
digraph event{
node [shape = box]
subgraph{
  rank = same; "Before Block Trade", "Large Block Trade", "Price Impact"
}
"Before Block Trade" -> "Large Block Trade" -> "Price Impact"
Time -> {"Before Block Trade", "Price Impact"}
}
```

. . .

:::{.aside}
To gauge demand from buyers and potentially gin up interest from sellers, bankers send out lists of shares with upcoming lockup expirations, according to market participants. [(Money Stuff, Matt Levine)](https://www.bloomberg.com/opinion/articles/2022-02-16/people-are-worried-about-block-trades)

Sometimes, bankers also engage in hypothetical conversations with buyers before they have a mandate. Asking prospective buyers whether they might be interested in certain stocks is one thing. But if there are indeed plans afoot for block sales, such conversations, even phrased hypothetically, can tip off savvy money managers. [(Money Stuff, Matt Levine)](https://www.bloomberg.com/opinion/articles/2022-02-16/people-are-worried-about-block-trades)
:::

::: {.notes}
Go also back to Assignment 2 where we modeled the same problem when investors anticipate a donation.
:::


# Difference-in-Difference

## What if we had an additional control group to estimate the counterfactual?

```{r}
#| echo: false
did <- tibble(
  days = rep(-60:60),
  log_return_treatment = c(rnorm(60, 1.15, .15), rnorm(61, 1.85, .15)),
  log_return_counterfactual = c(rep(NA, 60), rnorm(61, 1.15, .15)),
  log_return_control = c(rnorm(121, .5, .15))
)

event_study <- ggplot(
  did, aes(x = days, y = log_return_treatment)) +
  geom_line(colour = uwa_blue) +
  geom_line(aes(y = log_return_counterfactual), colour = uwa_gold) +
  ylab("log(return)") +
  ggtitle("Event Study") +
  geom_vline(xintercept = 0,  linetype = "longdash", colour = uwa_blue)
```

::::: {.columns}
:::: {.column}

```{r}
#| warning: false
#| echo: false
#| fig-width: 5
print(event_study)
```

::::
:::: {.column}

::: {.fragment}
```{r}
#| warning: false
#| echo: false
#| fig-width: 5
event_study +
  geom_line(aes(y = log_return_control), colour = uwa_blue) + 
  ggtitle("Difference in Difference")

```
:::
::::
:::::

## A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2 {.smaller}

```{r}
#| code-line-numbers: "1,2|3|6|7|14|16,17"
N <- 500
T <- 2
time_effect <- c(3.5, 0)
rd_did_firm <- tibble(
  firm = 1:N,
  performance = runif(N, 1, 10),
  firm_effect = rnorm(N, 0, 2) + ifelse(performance < 3, 3, 0)
)
rd_did_panel <- tibble(
  firm = rep(1:N, each = T),
  time = rep(1:T, times = N)) %>%
  left_join(rd_did_firm, by = "firm") %>%
  mutate(
    report = ifelse(time == 2, ifelse(performance > 3, 1, 0), 0),
    noise = rnorm(N*T, 0, 3),
    profit_report = 6.5 + time_effect[time] + firm_effect + noise,
    profit_no_report = 1.5 + time_effect[time] + firm_effect + noise,
    actual_profit = ifelse(report == 1, profit_report, profit_no_report))
```

::: {.notes}
The idea is that we have firms who perform well (performance > 3) and firms that perform bad (performance < 3). The firms that perform well will voluntarily disclose a report in time 2. We can see the effect as the difference between time 1 and time 2 for disclosers and non-disclosers.

Important: the cost of misreporting is not in calculated in the profit. The reasoning would be that this might be a litigation cost that would only emerge later on.
:::

## The Causal Effects in Our Simulation {.smaller}

```{r, causal_did_means, warning = FALSE}
rd_did_panel %>%
  mutate(causal_effect = profit_report - profit_no_report) %>%
  group_by(time, report2 = performance > 3) %>%
  summarise(profit_report = mean(profit_report),
            profit_no_report = mean(profit_no_report),
            causal_effect = mean(causal_effect)) %>%
  kable(digits = 1)
```

---

## A Summary of The Actual Profits {.smaller}

```{r visual-did, warning = FALSE}
rd_did_panel %>%
  group_by(time, report2 = performance > 3) %>%
  summarise(actual_profit = mean(actual_profit)) %>%
  pivot_wider(names_from = time, values_from = actual_profit) %>%
  kable(digits = 1)
```


## Regressions {.smaller}

```{r}
did_lm <- feols(actual_profit ~ report, data = rd_did_panel)
did_sub <- feols(actual_profit ~ report, data = filter(rd_did_panel, time == 2))
did_fixed <- feols(actual_profit ~ report | firm, data = rd_did_panel)
did_did <- feols(actual_profit ~ report | firm + time, data = rd_did_panel)
msummary(list(simple = did_lm, "time 2" = did_sub, "firm FE" = did_fixed, "two-way FE" = did_did),
         gof_omit = gof_omit, stars = stars)

```

## What if we have three periods? {.smaller}


::: {.callout-note}
We assume that over time investors and regulators get better at detecting when firms exaggerate in their report.
:::

- Time 1: Reports are not believable, nobody reports
- Time 2: The biggest exaggerations will be caught, only well performing firms will report and communicate that they are doing excellent.
- Time 3: More subtle exaggerations will be caught. The worst performers will not report at all, the moderate performers will report and say that they will do well, the good performers will report that they are doing excellent.


::: {.aside}
See the Appendix of the assignment for the derivation of the exact parameters.
:::


## Setup of three period simulation {.smaller}

```{r}
#| code-line-numbers: "2|3,6|4,7"
N <- 1000
T <- 3
cutoff2 <- 3 # performance cutoff to report for time 1
cutoff3 <- c(4/3, 4 + 2/3) # performance cutoff to report for time 2
profit1 <- 5
profit2 <- c(1.5, 6.5) #Profits for time 2 depending on report
profit3 <- c(2/3, 3, 7 + 1/3) #Profits for time 2 depending on report
rd_did3_firm <- tibble(
  firm = 1:N,
  performance = runif(N, 0, 10),
  firm_effect = rnorm(N, 0, 2) + ifelse(performance < cutoff2, 3, 0)
)
```

## Three period simulation {.smaller}

```{r}
#| code-line-numbers: "7-11|13-17|19-24"
rd_did3_panel <- tibble(
  firm = rep(1:N, each = T),
  time = rep(1:T, times = N)) %>%
  left_join(rd_did3_firm, by = "firm") %>%
  mutate(
    # When will firms report?
    report = case_when(
      time == 1 ~ 0,
      time == 2 & performance < cutoff2 ~ 0,
      time == 3 & performance < cutoff3[1] ~ 0,
      TRUE ~ 1),
    noise = rnorm(T*N, 0, 5),
    profit_no_report = firm_effect + noise +
      case_when(
        time == 1 ~ profit1,
        time == 2 ~ profit2[1],
        time == 3 ~ profit3[1]
    ),
    profit_report = firm_effect + noise +
      case_when(
        time == 1 ~ profit1,
        time == 2 ~ profit2[2],
        time == 3 & performance < cutoff3[2] ~ profit3[2],
        TRUE ~ profit3[3]
      ),
    actual_profit = ifelse(report == 1, profit_report, profit_no_report)
  )
```

## Overview of 4 groups of firms {.smaller}

1. Never reporters
2. Reporters in year 3
3. Reporters in year 2 and 3 (Medium)
4. Reporters in year 2 and 3 (High)

```{r}
#| label: causal-three-years
causal_effects <- rd_did3_panel %>%
  mutate(causal_effect = profit_report - profit_no_report,
         group = case_when(
           performance < cutoff3[1] ~ 1,
           performance < cutoff2 ~ 2,
           performance < cutoff3[2] ~ 3,
           TRUE ~ 4
         )) %>%
  group_by(time, group) %>%
  summarise(report = mean(report),
            N = n(),
            M_report = mean(profit_report),
            M_no_report = mean(profit_no_report),
            M_causal_effect = mean(causal_effect))
```

## Overview of 4 groups of firms {.smaller}

```{r}
#| label: causal-table-three-years
#| echo: false
causal_effects %>% knitr::kable(digits = 1)
```

## Two-way Fixed Effects

```{r}
#| label: two-way-three-years
twoway12 <- feols(actual_profit ~ report | firm + time,
                  data = filter(rd_did3_panel, time != 3))
twoway13 <- feols(actual_profit ~ report | firm + time,
                  data = filter(rd_did3_panel, time != 2))
twoway123 <- feols(actual_profit ~ report | firm + time,
                  data = rd_did3_panel)
```

## Separate 2 by 2 effects are larger than the total sample effect {.smaller}


```{r}
msummary(list("time 1 and 2" = twoway12, "time 1 and 3" = twoway13,
              "time 1, 2 and 3" = twoway123), gof_omit = gof_omit,
         stars = c("*" = .1, "**" = .05, "***" = .01))
```


::: {.notes}
This is not what I would expect. Why would the full sample lead to a smaller effect than *all* the subsamples?
:::


# @baker2022

The paper is forthcoming in JFE but [available on ssrn](http://dx.doi.org/10.2139/ssrn.3794018 )


## Problem Statement

> Finally, when research settings combine staggered timing of treatment effects and treatment effect heterogeneity across firms or over time, staggered DiD estimates are likely to be biased. In fact, these estimates can produce the wrong sign altogether compared to the true average treatment effects.

## Solution

> While the literature has not settled on a standard, the proposed solutions all deal with the biases arising from the *“bad comparisons”* problem inherent in TWFE DiD regressions by modifying the set of effective comparison units in the treatment effect estimation process. For example, each alternative estimator ensures that *firms receiving treatment are not compared to those that previously received it.*

::: {.notes}
Again, the solution to all our problems is to make sure that we make the right comparison.
:::

## Simulation Setup - The True Average Treatment Effect of Three Groups

![](figures/baker_effects.svg)

::: {.notes}
It's clear that the average treatment effect should be positive. It's positive for every group.
:::


## The Estimated Effect by Twoway Fixed Effects of 500 Simulations

![](figures/simulation_twoway.svg)

## The @sun2021 Solution - Restrict The Sample

![](figures/baker_sun_sample.svg)

## The Estimated Effect with the Sun and Abraham Solution

![](figures/simulation_sun_abraham.svg)

## Sun and Abraham in Practice

```{r}
#| label: sun_abraham_in_r
sa_new <- readRDS(here("data", "sa_new.RDS"))
sa_fe <- feols(roa ~ 1 + sunab(treatment_group, year) | firm + year,
               cluster = "state", data = sa_new)
sa_fe_att <- summary(sa_fe, agg = "ATT")
sa_fe_group <- summary(sa_fe, agg = "cohort")
```
- `treatment_group`: first year of treatment
- `year`: calendar year

```{r}
names(sa_fe$coefficients)
```

## Sun and Abraham - Relative Year {.smaller}

```{r sun_abraham_rel_year}
msummary(sa_fe, gof_omit = gof_omit, stars = stars, statistic = NULL,
         estimate = "{estimate} ({std.error}) {stars}", coef_omit = "-1")
```

## Sun and Abraham - Relative Year {.smaller}

```{r sun_abraham_rel_year_plot, fig.height = 5}
iplot(sa_fe)
```

## Sun and Abraham - ATT {.smaller}

```{r sun_abraham_att}
msummary(sa_fe_att, gof_omit = gof_omit, stars = stars)
```

## Sun and Abraham - Cohort Effects {.smaller}

```{r sun_abraham_cohort}
msummary(sa_fe_group, gof_omit = gof_omit, stars = stars)
```

## Take-away Lessons

::: {.callout-note}
- Simulations are good!
- Everything is a regression (Ok, not really)
- Not all the data should go in the regression
:::

# When should you cluster standard errors?

## @abadie2017

::: {.callout-note}
What is the level of the treatment variable? What is the comparison?
:::

- Mixed *race* or same-sex *race*
- *State* legislation
- *Country* legislation
- *Firm* corporate governance changes

# References

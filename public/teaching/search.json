[
  {
    "objectID": "method_package.html",
    "href": "method_package.html",
    "title": "Content",
    "section": "",
    "text": "The slides contain the lecture slides for the first half of the semester.\n\nSlides 1\nSlides 2\nSlides 3\nSlides 4\nSlides 5"
  },
  {
    "objectID": "method_package.html#freaky-friday",
    "href": "method_package.html#freaky-friday",
    "title": "Content",
    "section": "Freaky Friday",
    "text": "Freaky Friday\nContains an attempt at a replication from downloading the data to analyzing for Dellavigna and Pollet (2009).\nIntroduction to the Replication"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "",
    "text": "This setup code loads a package, here, that helps to navigate the folder structure in which I will create files. The tidyverse package is the main package, we will use to manipulate datasets. There are other ways to program in R. I think that to start of the tidyverse way of looking at data is quite intuitive and it is very well supported. The intuition and how quickly you can do meaningful things will hopefully be clear by the end of this document."
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#with-the-tidyverse",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#with-the-tidyverse",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "With the tidyverse",
    "text": "With the tidyverse\n\nus_comp <- readRDS(here(\"data\", \"us-compensation-new.RDS\"))\nglimpse(us_comp)\n\nRows: 23,096\nColumns: 22\n$ year                    <dbl> 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018…\n$ gvkey                   <chr> \"001004\", \"001004\", \"001004\", \"001004\", \"00100…\n$ cusip                   <chr> \"00036110\", \"00036110\", \"00036110\", \"00036110\"…\n$ exec_fullname           <chr> \"David P. Storch\", \"David P. Storch\", \"David P…\n$ coname                  <chr> \"AAR CORP\", \"AAR CORP\", \"AAR CORP\", \"AAR CORP\"…\n$ ceoann                  <chr> \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO…\n$ execid                  <chr> \"09249\", \"09249\", \"09249\", \"09249\", \"09249\", \"…\n$ bonus                   <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.00…\n$ salary                  <dbl> 867.000, 877.838, 906.449, 906.449, 755.250, 8…\n$ stock_awards_fv         <dbl> 2664.745, 619.200, 1342.704, 1695.200, 1150.50…\n$ stock_unvest_val        <dbl> 4227.273, 6018.000, 4244.165, 4103.283, 1334.0…\n$ eip_unearn_num          <dbl> 32.681, 56.681, 66.929, 83.415, 91.969, 157.96…\n$ eip_unearn_val          <dbl> 393.806, 1137.021, 1626.375, 2464.079, 2244.96…\n$ option_awards           <dbl> 578.460, 695.520, 1622.016, 0.000, 1150.500, 1…\n$ option_awards_blk_value <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ option_awards_num       <dbl> 49.022, 144.000, 158.400, 0.000, 153.810, 225.…\n$ tdc1                    <dbl> 5786.400, 4182.832, 5247.779, 5234.648, 4674.4…\n$ tdc2                    <dbl> 6105.117, 3487.312, 3809.626, 10428.375, 3523.…\n$ shrown_tot_pct          <dbl> 2.964, 2.893, 3.444, 3.877, 4.597, 5.417, 3.71…\n$ becameceo               <date> 1996-10-09, 1996-10-09, 1996-10-09, 1996-10-0…\n$ joined_co               <date> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ reason                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nfilter(us_comp, gvkey == \"001004\")\n\n# A tibble: 11 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   <dbl> <chr>  <chr>    <chr>                 <chr>  <chr>  <chr>  <dbl>  <dbl>\n 1  2011 001004 00036110 David P. Storch       AAR C… CEO    09249      0   867 \n 2  2012 001004 00036110 David P. Storch       AAR C… CEO    09249      0   878.\n 3  2013 001004 00036110 David P. Storch       AAR C… CEO    09249      0   906.\n 4  2014 001004 00036110 David P. Storch       AAR C… CEO    09249      0   906.\n 5  2015 001004 00036110 David P. Storch       AAR C… CEO    09249      0   755.\n 6  2016 001004 00036110 David P. Storch       AAR C… CEO    09249      0   835 \n 7  2017 001004 00036110 David P. Storch       AAR C… CEO    09249      0   941 \n 8  2018 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   750 \n 9  2019 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   801.\n10  2020 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   781.\n11  2021 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   925 \n# ℹ 13 more variables: stock_awards_fv <dbl>, stock_unvest_val <dbl>,\n#   eip_unearn_num <dbl>, eip_unearn_val <dbl>, option_awards <dbl>,\n#   option_awards_blk_value <dbl>, option_awards_num <dbl>, tdc1 <dbl>,\n#   tdc2 <dbl>, shrown_tot_pct <dbl>, becameceo <date>, joined_co <date>,\n#   reason <chr>\n\n\n\nfilter(us_comp, bonus == 0, year == 2012)\n\n# A tibble: 1,678 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   <dbl> <chr>  <chr>    <chr>                 <chr>  <chr>  <chr>  <dbl>  <dbl>\n 1  2012 001004 00036110 David P. Storch       AAR C… CEO    09249      0   878.\n 2  2012 001045 02376R10 Thomas W. Horton      AMERI… CEO    26059      0   618.\n 3  2012 001075 72348410 Donald E. Brandt, CPA PINNA… CEO    05835      0  1146 \n 4  2012 001076 74319R10 Ronald W. Allen       PROG … CEO    00283      0   850 \n 5  2012 001078 00282410 Miles D. White, M.B.… ABBOT… CEO    14300      0  1900 \n 6  2012 001094 00444610 Albert L. Eilender    ACETO… CEO    46204      0   626.\n 7  2012 001161 00790310 Rory P. Read          ADVAN… CEO    42390      0  1000.\n 8  2012 001177 00817Y10 Mark Thomas Bertolini AETNA… CEO    31029      0   977.\n 9  2012 001209 00915810 John E. McGlade       AIR P… CEO    27315      0  1200 \n10  2012 001230 01165910 Bradley D. Tilden     ALASK… CEO    21308      0   420.\n# ℹ 1,668 more rows\n# ℹ 13 more variables: stock_awards_fv <dbl>, stock_unvest_val <dbl>,\n#   eip_unearn_num <dbl>, eip_unearn_val <dbl>, option_awards <dbl>,\n#   option_awards_blk_value <dbl>, option_awards_num <dbl>, tdc1 <dbl>,\n#   tdc2 <dbl>, shrown_tot_pct <dbl>, becameceo <date>, joined_co <date>,\n#   reason <chr>"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#a-gamestop-to-introduce-the-pipe",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#a-gamestop-to-introduce-the-pipe",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "A Gamestop to introduce the pipe",
    "text": "A Gamestop to introduce the pipe\n\nLook for any company where “gamestop” is in the name of the company.\nLook for the gvkey of Gamestop. Then use filter with exactly that key.\n\n\nfilter(us_comp, str_detect(tolower(coname),\n                           \"gamestop\"))\n\n# A tibble: 11 × 22\n    year gvkey  cusip    exec_fullname     coname     ceoann execid bonus salary\n   <dbl> <chr>  <chr>    <chr>             <chr>      <chr>  <chr>  <dbl>  <dbl>\n 1  2011 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  2254.  1028.\n 2  2012 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  1515   1050.\n 3  2013 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719   975   1059.\n 4  2014 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1201.\n 5  2015 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1247.\n 6  2016 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1285.\n 7  2017 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1293.\n 8  2018 145049 36467W10 Shane S. Kim      GAMESTOP … CEO    56940    25    963.\n 9  2019 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080   150    846.\n10  2020 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080     0   1005.\n11  2021 145049 36467W10 Matthew Furlong   GAMESTOP … CEO    61979  1595.   115.\n# ℹ 13 more variables: stock_awards_fv <dbl>, stock_unvest_val <dbl>,\n#   eip_unearn_num <dbl>, eip_unearn_val <dbl>, option_awards <dbl>,\n#   option_awards_blk_value <dbl>, option_awards_num <dbl>, tdc1 <dbl>,\n#   tdc2 <dbl>, shrown_tot_pct <dbl>, becameceo <date>, joined_co <date>,\n#   reason <chr>\n\nfilter(us_comp, gvkey == \"145049\")\n\n# A tibble: 11 × 22\n    year gvkey  cusip    exec_fullname     coname     ceoann execid bonus salary\n   <dbl> <chr>  <chr>    <chr>             <chr>      <chr>  <chr>  <dbl>  <dbl>\n 1  2011 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  2254.  1028.\n 2  2012 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  1515   1050.\n 3  2013 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719   975   1059.\n 4  2014 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1201.\n 5  2015 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1247.\n 6  2016 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1285.\n 7  2017 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1293.\n 8  2018 145049 36467W10 Shane S. Kim      GAMESTOP … CEO    56940    25    963.\n 9  2019 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080   150    846.\n10  2020 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080     0   1005.\n11  2021 145049 36467W10 Matthew Furlong   GAMESTOP … CEO    61979  1595.   115.\n# ℹ 13 more variables: stock_awards_fv <dbl>, stock_unvest_val <dbl>,\n#   eip_unearn_num <dbl>, eip_unearn_val <dbl>, option_awards <dbl>,\n#   option_awards_blk_value <dbl>, option_awards_num <dbl>, tdc1 <dbl>,\n#   tdc2 <dbl>, shrown_tot_pct <dbl>, becameceo <date>, joined_co <date>,\n#   reason <chr>"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#moving-on",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#moving-on",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "Moving on",
    "text": "Moving on\n\nselect(us_comp, year, coname, bonus, salary, total = tdc1)\n\n# A tibble: 23,096 × 5\n    year coname   bonus salary total\n   <dbl> <chr>    <dbl>  <dbl> <dbl>\n 1  2011 AAR CORP     0   867  5786.\n 2  2012 AAR CORP     0   878. 4183.\n 3  2013 AAR CORP     0   906. 5248.\n 4  2014 AAR CORP     0   906. 5235.\n 5  2015 AAR CORP     0   755. 4674.\n 6  2016 AAR CORP     0   835  6073.\n 7  2017 AAR CORP     0   941  6284.\n 8  2018 AAR CORP     0   750  3344.\n 9  2019 AAR CORP     0   801. 4736.\n10  2020 AAR CORP     0   781. 4123.\n# ℹ 23,086 more rows\n\n\nWe first build up the dataset with select and mutate and the pipe %>%. When we are satisfied with the result, we can save the dataset as an R object with the name us_comp_small.\n\nus_comp_small <-\n  select(us_comp, year, coname, bonus,\n       salary, total = tdc1) %>%\n    mutate(salary_percentage = salary/total)\nprint(us_comp_small)\n\n# A tibble: 23,096 × 6\n    year coname   bonus salary total salary_percentage\n   <dbl> <chr>    <dbl>  <dbl> <dbl>             <dbl>\n 1  2011 AAR CORP     0   867  5786.             0.150\n 2  2012 AAR CORP     0   878. 4183.             0.210\n 3  2013 AAR CORP     0   906. 5248.             0.173\n 4  2014 AAR CORP     0   906. 5235.             0.173\n 5  2015 AAR CORP     0   755. 4674.             0.162\n 6  2016 AAR CORP     0   835  6073.             0.138\n 7  2017 AAR CORP     0   941  6284.             0.150\n 8  2018 AAR CORP     0   750  3344.             0.224\n 9  2019 AAR CORP     0   801. 4736.             0.169\n10  2020 AAR CORP     0   781. 4123.             0.189\n# ℹ 23,086 more rows"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#quick-descriptive-statistics",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#quick-descriptive-statistics",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "Quick descriptive statistics",
    "text": "Quick descriptive statistics\n\ngroup_by(us_comp, gvkey) %>%\n    summarise(N = n(), N_CEO = n_distinct(execid),\n              average = mean(salary),\n              sd = sd(salary),\n              med = median(salary),\n              min = min(salary),\n              max = max(salary)) %>%\n    ungroup() %>%\n  filter(med > 1000)\n\n# A tibble: 599 × 8\n   gvkey      N N_CEO average    sd   med   min   max\n   <chr>  <int> <int>   <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 001075    12     2   1219. 111.  1222. 1091  1395 \n 2 001078    12     2   1788. 224.  1900  1298. 1973.\n 3 001161    12     3    961. 151.  1000.  566. 1149.\n 4 001209    12     2   1239. 131.  1200   905. 1402.\n 5 001274    11     1   1082. 101.  1030  1000  1250 \n 6 001300    12     2   1714. 152.  1750  1415. 1890 \n 7 001380    12     1   1500    0   1500  1500  1500 \n 8 001440    12     2   1339. 176.  1350.  903. 1522.\n 9 001447    12     2   1794. 262.  2000  1488. 2038.\n10 001449    12     1   1438.  12.1 1441. 1399. 1441.\n# ℹ 589 more rows\n\n\n\nsummarise(us_comp, N = n(), N_CEO = n_distinct(execid),\n              average = mean(salary),\n              sd = sd(salary),\n              med = median(salary),\n              min = min(salary),\n              max = max(salary),\n          .by = gvkey) %>%\n  filter(med > 1000)\n\n# A tibble: 599 × 8\n   gvkey      N N_CEO average    sd   med   min   max\n   <chr>  <int> <int>   <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 001075    12     2   1219. 111.  1222. 1091  1395 \n 2 001078    12     2   1788. 224.  1900  1298. 1973.\n 3 001161    12     3    961. 151.  1000.  566. 1149.\n 4 001209    12     2   1239. 131.  1200   905. 1402.\n 5 001274    11     1   1082. 101.  1030  1000  1250 \n 6 001300    12     2   1714. 152.  1750  1415. 1890 \n 7 001380    12     1   1500    0   1500  1500  1500 \n 8 001440    12     2   1339. 176.  1350.  903. 1522.\n 9 001447    12     2   1794. 262.  2000  1488. 2038.\n10 001449    12     1   1438.  12.1 1441. 1399. 1441.\n# ℹ 589 more rows"
  },
  {
    "objectID": "slides/slides1.html#how-to-do-empirical-research",
    "href": "slides/slides1.html#how-to-do-empirical-research",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "How to do empirical research",
    "text": "How to do empirical research\n\nThe connection between theory and the observed data\nThe connection between practical knowledge and what you can investigate\nThe appropriate statistical tests and code\n\n\nEmpirical just means that some data in the broadest possible be sense will be collected or generated. The emphasis on the units will be on the practical and statistical issues of the data analysis part of a thesis. The influence will not be on the accounting and finance part of your thesis. The goal is to be relevant to everyone in the unit. This would be a good time to figure out whether students do a study with a more qualitative approach."
  },
  {
    "objectID": "slides/slides1.html#different-modules",
    "href": "slides/slides1.html#different-modules",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Different Modules",
    "text": "Different Modules\n\nThe research process and basic data skills in R (week 1 - 3)\nResearch design (week 4 - 6)\nAdvanced regression (week 7 - 9)\nTime series analysis (10 - 12)\n\n\nAll modules work as stand-alone units and aim to cover a wide range of topics. Not all of them will in the end be relevant for everyone. However, it is probably a good idea to get to know the different statistical methods, their advantages, and disadvantages. Bringing a new methodology to an old topic can be a valuable contribution. Some problems have already been solved in other research streams."
  },
  {
    "objectID": "slides/slides1.html#assessment",
    "href": "slides/slides1.html#assessment",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Assessment",
    "text": "Assessment\n\nNo Exam\n(Almost weekly) Assignments (70%)\n\n\n- Homework: Practical Issues (0%) (3 March)\n- Assignment 1: Theory and Regressions (10%) (10 March)\n- Assignment 2: Regression and control variables (10%) (17 March)\n- Assignment 3: Research Design (10%) (7 April)\n- Assignment 4: Event Study (10%) (28 April)\n- Assignment 5: Machine Learning (10%) (5 May)\n- Assignment 6: Simulation Research Design (20%) (26 May)\n\n\nProposal and presentation (30%)\n\n\n- Pitch (10%) (31 March)\n- Proposal (10%) (12 May)\n- Presentation (10%) (Probably Thursday 20 July)\n\n\nWe want you to (1) do some data analysis and (2) be well prepared to undertake (the data analysis part of) a research project. So we are going to evaluate you by letting you (1) analyse data and (2) prepare your honours thesis."
  },
  {
    "objectID": "slides/slides1.html#the-first-two-weeks",
    "href": "slides/slides1.html#the-first-two-weeks",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "The first two weeks",
    "text": "The first two weeks\nCEO compensation\n\nThis is not my area of expertise!\nI am not a specialist in the topic nor in this type of data analysis. CEO compensation is something that people in finance, accounting, economics, and outside of academia are interested in. The topic is probably the one with the most commonality. I am comforable with these type of economic theories and I am going to stress the role of theory in data analysis a lot. Some of you will have a topic that is at first sight less theory driven or rely more strongly on very specific knowledge about your setting. I am going to try to convince you that it is going to be useful to think about the underlying story that you are testing."
  },
  {
    "objectID": "slides/slides1.html#topic",
    "href": "slides/slides1.html#topic",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Topic",
    "text": "Topic\nCompletely based on Edmans and Gabaix (2016) in Journal of Economic Literature.\n\nThe level of CEO compensation\nCEO incentives\n\n\nI am going to focus on two topics. 1. How high can we expect the total compensation of a CEO to be (compared to other CEOs) based on some simple economic assumptions. Too high CEO compensation is sometimes seen as a signal of bad corporate goverance. To measure what ‘too high’ means, we first need to establish a baseline of normal levels of compensation. 2. How should CEOs be incentivised: equity or options? How schould we measure whether CEOs have appropriate incentives: $ for $ increases, % for % increases? Incentives are a big topic in Accounting and Finance."
  },
  {
    "objectID": "slides/slides1.html#firm-production-function",
    "href": "slides/slides1.html#firm-production-function",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Firm production function",
    "text": "Firm production function\n\\[\nV = T^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n                 \\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\]\n\\[\n\\alpha_T + \\alpha_K + \\alpha_L = 1\n\\]\n\n\\(V =\\) The value of the firm\n\\(K =\\) Capital of the firm\n\\(L =\\) Labour of the firm\n\\(T =\\) CEO talent/skills/ability/experience\n\n\nWe assume that there is nothing in the structure of the production function that favours a particular firm size, i.e. constant returns to scale."
  },
  {
    "objectID": "slides/slides1.html#ceo-decision",
    "href": "slides/slides1.html#ceo-decision",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "CEO decision",
    "text": "CEO decision\n\\[\n\\max_{K, L} W_T = V - w_L L - rK\n\\]\n\n\\(W_T =\\) wage for CEO with talent T\n\\(w_L =\\) labour unit costs\n\\(r =\\) cost of capital (or return on capital for investors)\n\n\nThe CEO maximises their income \\(W_T\\) by attracting capital at a cost, \\(r\\), and and hiring labour at a wage, \\(w_L\\). The model assumes that the CEO takes the ultimate decision. As it turns out when you assume competitive labour and financial markets, that assumptions does not really matter a lot.\nThis model is too simple to capture reality perfectly. However, that is not the goal of the model and of this exercise. The idea is to see whether we can find a reasonable baseline for CEO compensation that we can test against the data."
  },
  {
    "objectID": "slides/slides1.html#relation-between-size-and-ceo-wage",
    "href": "slides/slides1.html#relation-between-size-and-ceo-wage",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Relation between size and CEO wage",
    "text": "Relation between size and CEO wage\n\\[\nW_T = \\alpha_T V\n\\]\n\nIn this model, the driving force is that more talented CEOs grow the business to a bigger size and they earn more money when they create more value.\n\n\nFirst find the optimal level of capital …\n\\[\n\\begin{aligned}\n\\frac{\\partial W_T}{\\partial K} &=  T^{\\alpha_T}  \n\\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K - 1}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L} - r = 0\n\\\\\nT^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\frac{\\alpha_K}{K}\n&= r\n\\\\\n\\frac{V}{r} &= \\frac{K}{\\alpha_K}\n\\end{aligned}\n\\]\n… and labour\n\\[\n\\begin{aligned}\n\\frac{\\partial W_T}{\\partial K} &=  T^{\\alpha_T}  \n\\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L - 1} - w_L\n\\\\\nT^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\frac{\\alpha_L}{L} &= w_L\n\\\\\n\\frac{V}{w_L} &= \\frac{L}{\\alpha_L}\n\\end{aligned}\n\\]\nNow we can plugin \\(L\\) and \\(K\\) in \\(V\\) …\n\\[\n\\begin{align}\nV = T^{\\alpha_T} \\Bigl( \\frac{V}{r} \\Bigl) ^{\\alpha_K}\n\\Bigl( \\frac{V}{w_L} \\Bigl) ^{\\alpha_L}\n\\\\\nV^{1 - \\alpha_K - \\alpha_L} = \\frac{T^{\\alpha_T}}\n{r^{\\alpha_K} w_L^{\\alpha_L}}\n\\\\\nV^{\\alpha_T} = \\frac{T^{\\alpha_T}}\n{r^{\\alpha_K} w_L^{\\alpha_L}}\n\\\\\nV = \\frac{T}\n{r^{\\frac{\\alpha_K}{\\alpha_T}} w_L^{\\frac{\\alpha_L}{\\alpha_T}}}\n\\end{align}\n\\]\n… and in \\(W_T\\).\n\\[\n\\begin{align}\nW_T = V - V \\alpha_K - V\\alpha_L = (1 - \\alpha_K - \\alpha_L) V\n= \\alpha_T V\n\\end{align}\n\\]\nI like the basic intuition and deriviation of the model. The derivation is straightforward and (some of) the implicit assumptions are relatively easy to accept. The effect of the CEO depends on the size of the firm (\\(V\\)). When there is more capital and labour available a more talented CEO will have a bigger impact. The model also predicts a clear quantitative relationship between firm size, \\(V\\), and CEO compensation, \\(W_T\\), i.e. that relationship should be linear. This is a nice result that we can test with data. In contrast to the linear relationship between firm size and CEO talent. We can measure \\(V\\) but not \\(T\\)."
  },
  {
    "objectID": "slides/slides1.html#data-compensation-value",
    "href": "slides/slides1.html#data-compensation-value",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Data: Compensation-Value",
    "text": "Data: Compensation-Value\n\n\n\n\n\nThe data is downloaded from Compustat and Execucomp. A lot of you will use these are similar databases in your research project. I did not clean or check the data for this exercise. In your own project, you should show a better understanding of how the data are gathered and what they include than what I am displaying here.\n\nCEO compensation is fairly complete. It includes changes in the value of equity and options.\nMarket value also includes all outstanding financial instruments on the company.\n\nThe qualitative relationship holds quite well. Bigger companies have CEOs with higher compensation. However, the relationship is far from linear and looks more like a power function. Clearly there are other effects at play. In this sample, the power coefficient is 0.31. Prior studies have found a coefficient more closely to 0.33 (Baker, Jensen, and Murphy 1988). Remember that in our setup the CEO can grow the firm at will by attracting more capital and more labour. That assumption is probably too strong."
  },
  {
    "objectID": "slides/slides1.html#the-research-process",
    "href": "slides/slides1.html#the-research-process",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "The research process",
    "text": "The research process\n\n\n\n\n\n\nSummary\n\n\n\nMake assumptions\nDerive relationship between measurable quantities\nCompare the theory and the data\n\n\n\n\n\n\nNote what we have just done. We started with some assumptions about the production function of a company and competitive markets to find the theoretical relation between firm size and CEO compensation. We followed up by testing this theory to data from S&P500 firms. These are the steps that you should be following."
  },
  {
    "objectID": "slides/slides1.html#literature-search",
    "href": "slides/slides1.html#literature-search",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Literature search",
    "text": "Literature search\n\nGoogle Scholar\nUWA One Search\nWeb of Knowledge\nEBSCOhost Research Databases\nSocial Science Research Network\nNational Bureau of Economic Research\n\n\nIn the CEO compensation case above, we derived the theoretical prediction. Normally, you will build on prior theoretical and empirical research to build predictions. The\nIn most cases (ssrn is the exception), you will have to be on the university’s network if you want to actually read the full paper.\n\nGoogle Scholar is probably the most comprehensive repository. This search engine work very similar to regular Google search. There are some additional tricks you can use “author:lastname-firstname” will help you to narrow down papers from a specific author. “intitle:keyword” let’s you search for keywords in the title of papers. You can also narrow down your search based on year of publication. The advanced search features hidden in the left side bar give you additional options such as searching for certain journals. If you are on the university network, Google Scholar will tell you for every paper\nOnesearch is the university search engine. It’s the best way to figure out whether there is an easily accessible version of the paper.\nWebofknowledge and EBSCOhost are two publisher driven initiatives. They work pretty well. Each with their own quirks.\nSSRN (Social Science Research Network) and NBER (National Bureau of Economic Research) both provide access to their own not-yet-peer-reviewed paper repositories. Here you go to find cutting edge research."
  },
  {
    "objectID": "slides/slides1.html#start-of-literature-search",
    "href": "slides/slides1.html#start-of-literature-search",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Start of literature search",
    "text": "Start of literature search\n\nDon’t start too narrow!\nReview articles and journals\n\nJournal of Economic Literature\nJournal of Accounting Literature\n\nA*/A journals\n\nAccounting\nFinance\n\n\n\nMy favourite way to start a research project now is to find one or two excellent overview or review papers. A (systematic) review paper provides a state of a research field and identifies interesting new research questions. I am not sure whether my strategy will work for you. I find that a good review paper gives a good list of papers you can build on and they often already compare the most important papers in a field. The trick is to be not too picky. You probably will not find a review for your exact reserch problem but it is unlikely that you will not find a partly relevant overview paper. You can search for review papers by adding “intitle:review” or “intitle:overview” to your Google Scholar search.\nTo find other papers relevant to your topic, you can build on the review paper by (1) looking up the papers referred to in the review paper and (2) search for papers that cite the review paper. You can do the latter via Google Scholar and Webofknowledge.\nTo find good reviews, I think you should start your search in the better journals. Some journals are dedicated to these literature reviews for instance Journal of Economic Literature and Journal of Accounting Literature. I am not aware of a similar journal in finance but I will happily add it if you let me know.\nWhen you start your literature search, you don’t want to narrow. You are not going to find an overview paper about “CEO compensation in Australian mining companies after the GFC”. However, you can start with an overview paper about CEO compensation. Like the one I found: “Executive Compensation: A Modern Primer” by Alex Edmans and Xavier Gabraix in Journal of Economic Literature."
  },
  {
    "objectID": "slides/slides1.html#signs-of-bad-workload-management",
    "href": "slides/slides1.html#signs-of-bad-workload-management",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Signs of bad workload management",
    "text": "Signs of bad workload management\n\nIrregular sleeping habits\nLoss of motivation\nPostponing difficult tasks"
  },
  {
    "objectID": "slides/slides1.html#working-with-a-supervisor",
    "href": "slides/slides1.html#working-with-a-supervisor",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Working with a supervisor",
    "text": "Working with a supervisor\n\nThe role of the supervisor\n\n\nGuide you towards a feasible research project\nHelp you finish the dissertation\n\n\nWork process\n\n\nSchedule weekly or fortnightly meetings\nSubmit writing or data analysis before every meeting.\n\n\nAdd a tl;dr section.\n\n\nYour supervisor is not your copy-editor, let them know when you submit an “early” draft.\nTell your supervisor what has changed\nClarify the sample and the main variables in tables\nTell your supervisor what the main table or figure is"
  },
  {
    "objectID": "slides/slides1.html#managing-the-workload-40-hours-per-week",
    "href": "slides/slides1.html#managing-the-workload-40-hours-per-week",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Managing the workload (40 hours per week)",
    "text": "Managing the workload (40 hours per week)\n\nPlan ahead (with your supervisor) towards major deadlines\nIt’s okay to submit partial assignments, as long as you make progress. (Especially for programming exercises)\nKeep writing!\nReach out when you need help with planning or when you feel overwhelmed.\n\nstijn.masschelein@uwa.edu.au\nUWA Counselling services\n\n\n\nThe plan can be an excel sheet with deadlines and milestones. Breaking down 5000 words into 10 weeks of 500 words is a lot less daunting."
  },
  {
    "objectID": "slides/slides1.html#questions",
    "href": "slides/slides1.html#questions",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Questions",
    "text": "Questions\n\nAnswer in Quarto (.qmd) format. File > New File > Quarto Document ... >\nYou can use the code examples that I used in the video. I have uploaded the file to LMS. Use a different level 2 header for each question. Use R chunks to\n\nLoad the CEO compensation data from LMS so that you can work with it.\nPrint the dataset with only the CEOs without a cash bonus in 2013. You do not need to print the whole dataset. The default number of lines is sufficient.\nCalculate the number of observations, and the average and median bonus per year for the entire dataset.\n\nClick the Render button and upload the qmd and html version to LMS.\n\n\nThere is going to be some trial-and-error and debugging. That is fine. Carefully read the errors you get and use the resouces for help. Don’t be afraid to ask me or each other for help.\n\nGive a name to your document and enter your name\nSee the examples\nSee the render button in RStudio"
  },
  {
    "objectID": "slides/slides2.html#why-simulate-data",
    "href": "slides/slides2.html#why-simulate-data",
    "title": "Simulations, Regressions, and Significance",
    "section": "Why simulate data?",
    "text": "Why simulate data?\n\nVisualising your theory\nExperimenting with and understanding statistical tests\nExperimenting with statistical approaches without peaking at your data\n\nSee also Chapter 15 in Huntington-Klein (2021)\n\n\nVisualising can help you sharpen your intuition for your theory and for which values are reasonable and which are not.\nYou can simulate variables and causal structures that you cannot observe. See also this week’s homework\nYou don’t want to just decide on which statistical test to use because it gives you the “right” answer. If you want to experiment with different statistical models, you can do that with simulated data."
  },
  {
    "objectID": "slides/slides2.html#simulating-distributions-in-r",
    "href": "slides/slides2.html#simulating-distributions-in-r",
    "title": "Simulations, Regressions, and Significance",
    "section": "Simulating distributions in R",
    "text": "Simulating distributions in R\n\nN <- 1000\nrandom <- tibble(\n  normal = rnorm(N, 2, 5),\n  uniform = runif(N, 1, 5),\n  binomial = rbinom(N, 1, .25),\n  sample = sample(1:10, N, replace = T)\n)\nglimpse(random)\n\nRows: 1,000\nColumns: 4\n$ normal   <dbl> 2.46781982, -5.75669489, 1.83979037, 0.29…\n$ uniform  <dbl> 2.574179, 4.010491, 1.030518, 3.872609, 4…\n$ binomial <int> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sample   <int> 1, 4, 1, 7, 5, 3, 10, 2, 1, 10, 10, 4, 2,…"
  },
  {
    "objectID": "slides/slides2.html#better-code-formatting",
    "href": "slides/slides2.html#better-code-formatting",
    "title": "Simulations, Regressions, and Significance",
    "section": "Better Code Formatting",
    "text": "Better Code Formatting\n\np1 <- ggplot(random, aes(x = normal)) +\n  geom_density() +\n  ggtitle(\"normal density\")\np2 <- ggplot(random, aes(x = uniform)) +\n  geom_histogram(bins = 50) +\n  ggtitle(\"uniform histogram\")\np3 <- ggplot(random, aes(x = binomial, y = normal)) +\n  geom_point() +\n  ggtitle(\"binomal-normal\")\np4 <- ggplot(random, aes(x = as.factor(sample), y = uniform)) +\n  geom_jitter(width = .2) +\n  ggtitle(\"sample-uniform\") + labs(x = \"sample\")\nplot_grid(p1, p2, p3, p4, ncol = 4)"
  },
  {
    "objectID": "slides/slides2.html#new-theory",
    "href": "slides/slides2.html#new-theory",
    "title": "Simulations, Regressions, and Significance",
    "section": "New theory",
    "text": "New theory\n\n\n\n\n\n\nSummary\n\n\n\nFirms have different size and CEOs have different talent.\nMore talented CEOs work for bigger firms.\nFirms pay just enough so that the CEO is not tempted to work for a smaller firm.\n\n\n\n\n\n\nThe model is the second one presented in Edmans and Gabaix (2016). A more rigorous proof is shown in Tervio (2008). A simplified explanation is in Chapter 5 of my lecture notes"
  },
  {
    "objectID": "slides/slides2.html#visualisation-of-matching-theory",
    "href": "slides/slides2.html#visualisation-of-matching-theory",
    "title": "Simulations, Regressions, and Significance",
    "section": "Visualisation of matching theory",
    "text": "Visualisation of matching theory\n\n\n\n\nCode\nobs <- 500\nsize_rate <- 1; talent_rate <- 2/3;\nC <- 1/60; w0 = 1;\nn <- c(1:obs)\nsize <-  600 * n ^ (-size_rate)\ntalent <- - 1/talent_rate * n ^ (talent_rate)\n\nwage <- rep(NA, obs)\nwage[obs] <- w0\nfor (i in (obs - 1):1){\n  wage[i] <- wage[i + 1] + C * size[i + 1] *\n      (talent[i] - talent[i + 1])\n}\n\nmatching_plot <- qplot(x = size, y = wage) +\n    labs(x = \"Company Market Value\", y = \"CEO compensation\")\nplot(matching_plot + ggtitle(\"Value - Compensation\"))\n\n\n\n\n\n\n\n\nCode\nplot(matching_plot +\n     scale_x_continuous(trans = \"log\",\n                        breaks = scales::log_breaks(n=5, base=10)) +\n     scale_y_continuous(trans = \"log\",\n                        breaks = scales::log_breaks(n=5, base=5)) +\n     ggtitle(\"log(Value) - log(Compensation)\")\n     )"
  },
  {
    "objectID": "slides/slides2.html#ceo-compensation-data",
    "href": "slides/slides2.html#ceo-compensation-data",
    "title": "Simulations, Regressions, and Significance",
    "section": "CEO compensation data",
    "text": "CEO compensation data\n\nus_comp <- readRDS(here(\"data\", \"us-compensation-new.RDS\")) %>%\n  rename(total_comp = tdc1)\nus_value <- readRDS(here(\"data\", \"us-value-new.RDS\")) %>%\n  rename(year = fyear, market_value = mkvalt)\nsummary(us_value$market_value)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n      0.0     926.5    2570.5   13488.2    8181.1 2324390.2 \n     NA's \n     4182"
  },
  {
    "objectID": "slides/slides2.html#putting-it-all-together-with-_join",
    "href": "slides/slides2.html#putting-it-all-together-with-_join",
    "title": "Simulations, Regressions, and Significance",
    "section": "Putting it all together with _join",
    "text": "Putting it all together with _join\n\nus_comp_value <-\n    select(us_comp, gvkey, year, total_comp) %>% \n    left_join(\n        us_value,\n        by = c(\"year\", \"gvkey\"))\nglimpse(us_comp_value)\n\nRows: 31,692\nColumns: 5\n$ gvkey        <chr> \"001004\", \"001004\", \"001004\", \"001004…\n$ year         <dbl> 2011, 2011, 2012, 2013, 2014, 2015, 2…\n$ total_comp   <dbl> 5786.400, 5786.400, 4182.832, 5247.77…\n$ market_value <dbl> 485.2897, 485.2897, 790.0029, 961.308…\n$ ni           <dbl> 67.723, NA, 55.000, 72.900, 10.200, 4…\n\n\n\n\nMore information on joins to merge two datasets on the tidyverse website. I prefer the left_join function as the default because it indicates that we have a main dataset (left) to which we want to add a second dataset (right). We will also spend more time with these functions in week 7."
  },
  {
    "objectID": "slides/slides2.html#first-plot",
    "href": "slides/slides2.html#first-plot",
    "title": "Simulations, Regressions, and Significance",
    "section": "First plot",
    "text": "First plot\n\nThe BasicsThe scalesZoom in\n\n\n\n\n\nplot_comp_value <- ggplot(\n    us_comp_value,\n    aes(y = total_comp, x = market_value)) +\n    geom_point(alpha = .10) +\n    ylab(\"compensation ($ 000)\") +\n    xlab(\"market value ($ million)\")\n\n\n\nprint(plot_comp_value)\n\n\n\n\n\n\n\n\n\n\n\nplot_log <- plot_comp_value +\n    scale_x_continuous(\n        trans = \"log1p\",\n        breaks = c(1e2, 1e3, 1e4, 1e5, 1e6),\n        labels = function(x)\n            prettyNum(x/1000, digits = 2)) +\n    scale_y_continuous(\n        trans = \"log1p\",\n        breaks = c(1e1, 1e2, 1e3, 1e4, 1e5),\n        labels = function(x)\n            prettyNum(x/1000, digits = 2)) +\n    ylab(\"compensation ($ million)\") +\n    xlab(\"market value ($ billion)\")\n\n\n\nprint(plot_log)\n\n\n\n\n\n\n\n\n\n\n\nplot_zoom <- plot_log +\n    coord_cartesian(\n        xlim = c(1e1, NA), ylim = c(1e2, NA))\n\n\n\nprint(plot_zoom)"
  },
  {
    "objectID": "slides/slides2.html#notation",
    "href": "slides/slides2.html#notation",
    "title": "Simulations, Regressions, and Significance",
    "section": "Notation",
    "text": "Notation\n\n\n\\[\\begin{equation}\ny_i = a + b_1 x_{1i} + ... + b_n x_{ni} + \\epsilon_i\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vec{y} = a + b_1 \\vec{x_1} + ... b_n \\vec{x_n} + \\vec{\\epsilon}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vec{y} = a + \\vec{b} X + \\vec{\\epsilon}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vec{y} = \\mathcal{N}(a + \\vec{b} X, \\sigma)\n\\end{equation}\\]\nSee also Chapter 13 in Huntington-Klein (2021)\n\n\nreg <- lm(y ~ x1 + x2, data = my_data_set)\nsummary(reg)"
  },
  {
    "objectID": "slides/slides2.html#finally-the-regression",
    "href": "slides/slides2.html#finally-the-regression",
    "title": "Simulations, Regressions, and Significance",
    "section": "Finally, the regression",
    "text": "Finally, the regression\n\nreg <- lm(log(total_comp + 1) ~ log(market_value + 1),\n         data = us_comp_value)\n# summary(reg)\nprint(summary(reg)$coefficients, digits = 2)\n\n                      Estimate Std. Error t value Pr(>|t|)\n(Intercept)                5.2     0.0259     201        0\nlog(market_value + 1)      0.4     0.0032     124        0\n\n\n\n\nThe regression uses a trick with the log(... + 1) formulation which is probably not appropriate. We will see the poisson regression later for a more appropriate way of analysing the effect on a positive variable."
  },
  {
    "objectID": "slides/slides2.html#historical-discussion",
    "href": "slides/slides2.html#historical-discussion",
    "title": "Simulations, Regressions, and Significance",
    "section": "Historical Discussion",
    "text": "Historical Discussion\n\nOur estimates of the pay-performance relation (including pay, options, stockholdings, and dismissal) for chief executive officers indicate that CEO wealth changes $3.25 for every $1,000 change in shareholder wealth (Jensen and Murphy 1990).\n\n\n\n[…] The statistic in isolation can present a misleading picture of pay to performance relationships because the denominator - the change in firm value - is so large (Hall and Liebman 1998).\n\n\n\n\nThis article addresses four major concerns about the pay of U.S. CEOs: (1) failure to pay for performance; […]. The authors’ main message is that most if not all of these concerns are exaggerated by the popular tendency to focus on the annual income of CEOs (consisting of salary, bonus, and stock and option grants) while ignoring their existing holdings of company equity (Core, Guay, and Thomas 2005).\n\n\nThis is actually a good example of why a literature review is valuable. It’s not enough to just say that the three papers find different effects. They do more than that. The newer papers gradually build up a better theory of what happens in practice and use better measures to reflect that theory."
  },
  {
    "objectID": "slides/slides2.html#stock-holding-data",
    "href": "slides/slides2.html#stock-holding-data",
    "title": "Simulations, Regressions, and Significance",
    "section": "Stock holding data",
    "text": "Stock holding data\n\nus_comp <- readRDS(here(\"data\", \"us-compensation-new.RDS\")) %>%\n    rename(total_comp = tdc1, shares = shrown_tot_pct) %>%\n    select(gvkey, execid, year, shares, total_comp)\nus_value <- readRDS(here(\"data\", \"us-value-new.RDS\")) %>%\n    rename(year = fyear, market_value = mkvalt) %>%\n    select(-ni) \nus_comp_value <- left_join(\n    us_comp, us_value, by = c(\"year\", \"gvkey\")) %>%\n    filter(!is.na(market_value) & !(is.na(shares))) %>%\n    mutate(wealth = shares * market_value / 100)\nglimpse(us_comp_value)\n\nRows: 27,207\nColumns: 7\n$ gvkey        <chr> \"001004\", \"001004\", \"001004\", \"001004…\n$ execid       <chr> \"09249\", \"09249\", \"09249\", \"09249\", \"…\n$ year         <dbl> 2011, 2011, 2012, 2013, 2014, 2015, 2…\n$ shares       <dbl> 2.964, 2.964, 2.893, 3.444, 3.877, 4.…\n$ total_comp   <dbl> 5786.400, 5786.400, 4182.832, 5247.77…\n$ market_value <dbl> 485.2897, 485.2897, 790.0029, 961.308…\n$ wealth       <dbl> 14.383987, 14.383987, 22.854784, 33.1…"
  },
  {
    "objectID": "slides/slides2.html#shares-to-market-value",
    "href": "slides/slides2.html#shares-to-market-value",
    "title": "Simulations, Regressions, and Significance",
    "section": "Shares to Market Value",
    "text": "Shares to Market Value\n\n\n\nplot_shares <- ggplot(\n    data = us_comp_value,\n    aes(x = market_value/1000, y = shares)) +\n    geom_point(alpha = .10) +\n    ylab(\"CEO Ownership\") +\n    xlab(\"Firm Market Value (in Billions)\") +\n    scale_x_continuous(\n        trans = \"log\",\n        labels = function(x)\n            prettyNum(x, digits = 2),\n        breaks =\n            scales::log_breaks(n = 5,\n                               base = 10)) +\n    scale_y_continuous(\n        trans = \"log\",\n        labels =\n            function(x)\n                prettyNum(x, digits = 2),\n        breaks =\n            scales::log_breaks(n = 5,\n                               base = 10))\n\n\n\nprint(plot_shares)"
  },
  {
    "objectID": "slides/slides2.html#pay-to-performance-sensitivity",
    "href": "slides/slides2.html#pay-to-performance-sensitivity",
    "title": "Simulations, Regressions, and Significance",
    "section": "Pay to Performance Sensitivity",
    "text": "Pay to Performance Sensitivity\n\nDataPlot\n\n\n\nus_sens <- us_comp_value %>%\n    group_by(gvkey, execid) %>%\n    arrange(year) %>%\n    mutate(prev_market_value = lag(market_value),\n            prev_wealth = lag(wealth)) %>%\n    ungroup() %>%\n    mutate(change_log_value = log(market_value) - log(prev_market_value),\n           change_log_wealth = log(wealth) - log(prev_wealth)) %>%\n    filter(!is.infinite(change_log_wealth)) %>%\n    arrange(gvkey)\n\n\n\nThe assumption for pay-for-performance and incentives is that we want to measure whether a CEO has taken the correct decisions. In a bigger firm, the impact of a CEOs decisions are larger. If you improve management of employees, then the effects will be bigger for a firm with more employees.\nThe other assumption is that CEOs do not care about dollar increases in dollars but in increases in percentages. Partly\n\n\\[\n\\begin{align}\n\\frac{\\partial W}{W} \\frac{V}{\\partial V}  \\\\\n  &= \\frac{ln(W)}{ln(V)}\n\\end{align}\n\\]\n\n\n\n\n\n\nplot_hypothesis <- ggplot(\n    us_sens,\n    aes(y = change_log_wealth / change_log_value,\n        x = market_value/1000)) +\n  geom_point(alpha = .1) +\n  scale_x_continuous(\n    trans = \"log\", \n    breaks = scales::log_breaks(n = 5, base = 10),\n    labels = function(x) prettyNum(x, dig = 2)) +\n  coord_cartesian(\n    ylim = c(-10, 10)) +\n  xlab(\"market value\") +\n  ylab(\"sensitivity\")\n\n\n\nprint(plot_hypothesis)"
  },
  {
    "objectID": "slides/slides2.html#randomisation-or-permutation-test",
    "href": "slides/slides2.html#randomisation-or-permutation-test",
    "title": "Simulations, Regressions, and Significance",
    "section": "Randomisation or Permutation Test",
    "text": "Randomisation or Permutation Test\n\nRandomisationTest\n\n\n\ndata_hypo <- us_sens %>%\n    mutate(\n      sensitivity = change_log_wealth / change_log_value) %>%\n  select(sensitivity, market_value) %>%\n  filter(complete.cases(.))\n\nobserved_cor <- cor(\n  data_hypo$sensitivity, data_hypo$market_value)\n\nrandom_cor <- cor(\n  data_hypo$sensitivity, sample(data_hypo$market_value))\n\nprint(prettyNum(c(observed_cor, random_cor), dig = 3))\n\n[1] \"-0.00192\" \"0.00512\" \n\n\n\n\n\n\n\nsimulate_cor <- function(data){\n    return(cor(data$sensitivity,\n               sample(data$market_value)))}\nrand_cor <- replicate(1e4,\n                      simulate_cor(data_hypo))\n\n\nhist_sim <- ggplot(\n    mapping = aes(\n        x = rand_cor,\n        fill = abs(rand_cor) < abs(observed_cor))) +\n    geom_histogram(bins = 1000) +\n    xlab(\"Random Correlations\") +\n    scale_fill_manual(values = c(uwa_blue, uwa_gold)) +\n    theme(legend.position = \"none\") +\n    coord_cartesian(\n        xlim = c(-0.1, 0.1))\n\n\n\nplot(hist_sim)"
  },
  {
    "objectID": "slides/slides2.html#bootstrap",
    "href": "slides/slides2.html#bootstrap",
    "title": "Simulations, Regressions, and Significance",
    "section": "Bootstrap",
    "text": "Bootstrap\n\n\n\ncalc_corr <- function(d){\n  n <- nrow(d)\n  id_sample <- sample(1:n, size = n,\n                      replace = TRUE)\n  sample <- d[id_sample, ]\n  corr <- cor(sample$sensitivity,\n              sample$market_value)\n  return(corr)\n}\nboot_corr <- replicate(\n    2000, calc_corr(data_hypo))\n\n\nplot_boot <- ggplot(\n    mapping = aes(x = boot_corr)) +\n  geom_histogram(bins = 100, colour = uwa_blue,\n                 fill = uwa_blue) +\n    geom_vline(aes(xintercept = 0),\n               colour = uwa_gold) +\n    xlab(\"Bootstrapped Correlation\")\n\n\n\nprint(plot_boot)"
  },
  {
    "objectID": "slides/slides2.html#comparison",
    "href": "slides/slides2.html#comparison",
    "title": "Simulations, Regressions, and Significance",
    "section": "Comparison",
    "text": "Comparison\n\n\nPermutation Test\n\nCalculate the observed statistic\nRandomly resample the data by breaking the relation you want to test (= Null Hypothesis)\nCalculate the statistic for each random sample\nIs the observed statistic more extreme than the randomly resampled statistic?\n\nSee also Chapter 4.2 in Cunningham (2021)\n\nBootstrap\n\nRandomly sample observed observations with replacement.\nCalculate the statistic you are interested in.\nIs the distribution of resampled statistics unlikely to be 0 (= Null Hypothesis)?\n\nSee also Chapter 15 in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides2.html#formula-based-p-value",
    "href": "slides/slides2.html#formula-based-p-value",
    "title": "Simulations, Regressions, and Significance",
    "section": "Formula Based P-value",
    "text": "Formula Based P-value\n\ncor <- cor.test(data_hypo$sensitivity, data_hypo$market_value)\npvalue_cor <- cor$p.value\nprint(prettyNum(pvalue_cor, dig = 2))\n\n[1] \"0.81\"\n\n\n\n\nregr_sens <- lm(sensitivity ~ I(market_value/1e3), data = data_hypo)\ncoefficients(summary(regr_sens)) %>% print(dig = 2)\n\n                     Estimate Std. Error t value Pr(>|t|)\n(Intercept)            1.3712      1.024    1.34     0.18\nI(market_value/1000)  -0.0037      0.015   -0.24     0.81"
  },
  {
    "objectID": "slides/slides2.html#models",
    "href": "slides/slides2.html#models",
    "title": "Simulations, Regressions, and Significance",
    "section": "Models",
    "text": "Models\n\n\nSignaling Model\n\n\n\nPeacock’s tail as a signal\n\n\n\nCheap Talk Model\n\n\n\nAssumptions are important"
  },
  {
    "objectID": "slides/slides2.html#answers",
    "href": "slides/slides2.html#answers",
    "title": "Simulations, Regressions, and Significance",
    "section": "Answers",
    "text": "Answers\n\nN <- 1000\nhigh_performance <- rbinom(.x, .y, .z)\ndonation <- ifelse(.x, 1, 0)\nreturn <- ifelse(donation == 1, .y, .z)\nobserved_donation <- ifelse(rbinom(N, 1, .9) == 1, donation, 1 - donation)\nobserved_return <- ... \nsig <- tibble(return = ...,\n              donation = ...) %>%\n    mutate(donated = ...)\nglimpse(sig)\n\n\n\nsig_plot <- ggplot(..., aes(x = .x, y = .y)) +\n    geom_jitter(width = .3)\nplot(sig_plot)\n\n\n\n\nsig_reg <- lm(..., data = sig)\nsummary(...)"
  },
  {
    "objectID": "slides/slides3.html#an-example-of-a-causal-graph",
    "href": "slides/slides3.html#an-example-of-a-causal-graph",
    "title": "Control Variables",
    "section": "An Example of a Causal Graph",
    "text": "An Example of a Causal Graph\n\n\n\n\n\n\n\nbrand_capital\n\n  \n\nInfoEnvironment\n\n InfoEnvironment   \n\nCreditRating\n\n CreditRating   \n\nInfoEnvironment->CreditRating\n\n    \n\nFutureCashFlow\n\n FutureCashFlow   \n\nFutureCashFlow->CreditRating\n\n    \n\nBrandCapital\n\n BrandCapital   \n\nBrandCapital->InfoEnvironment\n\n    \n\nBrandCapital->FutureCashFlow"
  },
  {
    "objectID": "slides/slides3.html#difference-with-equilibrium-models",
    "href": "slides/slides3.html#difference-with-equilibrium-models",
    "title": "Control Variables",
    "section": "Difference with equilibrium models",
    "text": "Difference with equilibrium models\n\n\n\n\n\n\nDifferences\n\n\n\nAll the qualitative information about causal relations is in the graph.\nThe equilibrium model directly gives the relation between the variables of interest.\n\ne.g.: Signaling model\n\n\n\n\n\n\n\n\n\n\ndonations\n\n  \n\nPerformance\n\n Performance   \n\nDonation\n\n Donation   \n\nPerformance->Donation\n\n    \n\nReturn\n\n Return   \n\nDonation->Return\n\n   \n\n\n\n\n\n\n\nIs there a relation between Performance and Return?\nThere is a parallel with voluntary disclosure of information that the company does not have.\nWhat happens if firms are no longer allowed to donate? What happens if firms are all forced to donate?"
  },
  {
    "objectID": "slides/slides3.html#assignment-csr-report",
    "href": "slides/slides3.html#assignment-csr-report",
    "title": "Control Variables",
    "section": "Assignment: CSR report",
    "text": "Assignment: CSR report\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nPerformance\n\n Performance   \n\nCSR_report\n\n CSR_report   \n\nPerformance->CSR_report\n\n    \n\nScandals\n\n Scandals   \n\nPerformance->Scandals\n\n    \n\nReturn\n\n Return   \n\nCSR_report->Return\n\n    \n\nObserved_Report\n\n Observed_Report   \n\nCSR_report->Observed_Report\n\n   \n\n\n\n\n\n\n\nWhat is the effect of an increase in scandals?\nWhat happens if we keep the number of scandals constant?"
  },
  {
    "objectID": "slides/slides3.html#causal-graph",
    "href": "slides/slides3.html#causal-graph",
    "title": "Control Variables",
    "section": "Causal Graph",
    "text": "Causal Graph\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nTechIndustry\n\n TechIndustry   \n\nTechIndustry->Performance\n\n   \n\n\n\n\n\n\nAssume that we are interested in the role of the GC on performance."
  },
  {
    "objectID": "slides/slides3.html#simulation",
    "href": "slides/slides3.html#simulation",
    "title": "Control Variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nset.seed(230383)\nN <- 1000\nds <- tibble(CG = runif(N, 0, 10),\n             TI = rbinom(N, 1, .25)) %>%\n  mutate(Performance =\n           rnorm(N, CG * .15 + TI * 10, 5))\n\n\nlm1 <- lm(Performance ~ CG, data = ds)\nlm2 <- lm(Performance ~ CG + TI, data = ds)\n\n\n\ngof_omit <- \"Adj|IC|Log|Pseudo\"\nstars <- c('*' = .1, '**' = .05, '***' = .01)\nmsummary(list(lm1, lm2), stars = stars,\n         gof_omit = gof_omit, output = \"html\")\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    2.976*** \n    0.036 \n  \n  \n     \n    (0.408) \n    (0.310) \n  \n  \n    CG \n    0.081 \n    0.130** \n  \n  \n     \n    (0.073) \n    (0.052) \n  \n  \n    TI \n     \n    10.433*** \n  \n  \n     \n     \n    (0.344) \n  \n  \n    Num.Obs. \n    1000 \n    1000 \n  \n  \n    R2 \n    0.001 \n    0.480 \n  \n  \n    RMSE \n    6.60 \n    4.76 \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\n\n\nMeasurement error typically decreases the effect and this is also what happened in the assignment. For instance, the difference between the donation and no donation should be 3 but it is less."
  },
  {
    "objectID": "slides/slides3.html#causal-graph-1",
    "href": "slides/slides3.html#causal-graph-1",
    "title": "Control Variables",
    "section": "Causal Graph",
    "text": "Causal Graph\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nTechIndustry\n\n TechIndustry   \n\nTechIndustry->CorporateGovernance\n\n    \n\nTechIndustry->Performance\n\n   \n\n\n\n\n\n\nThere is only 1 difference between this causal graph and the previous one."
  },
  {
    "objectID": "slides/slides3.html#simulation-1",
    "href": "slides/slides3.html#simulation-1",
    "title": "Control Variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nN <- 1000\nds <- tibble(TI = rbinom(N, 1, .25)) %>%\n  mutate(CG = rnorm(N, .5 - TI, .2),\n         Performance = rnorm(N, TI + 0 * CG, 1))\n\n\nlm1 <- lm(Performance ~ CG, data = ds)\nlm2 <- lm(Performance ~ CG + TI, data = ds)\n\n\n\nmsummary(list(lm1, lm2), stars = stars,\n         gof_omit = gof_omit, output = \"html\")\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    0.473*** \n    0.002 \n  \n  \n     \n    (0.036) \n    (0.087) \n  \n  \n    CG \n    −0.945*** \n    −0.090 \n  \n  \n     \n    (0.067) \n    (0.159) \n  \n  \n    TI \n     \n    1.018*** \n  \n  \n     \n     \n    (0.172) \n  \n  \n    Num.Obs. \n    1000 \n    1000 \n  \n  \n    R2 \n    0.165 \n    0.193 \n  \n  \n    RMSE \n    1.01 \n    0.99 \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\n\n\nIn the simulation, we set the effect of GC equal to 0 i.e. there is not effect. The reason to do that is to show why it’s necessary to adjust for TI."
  },
  {
    "objectID": "slides/slides3.html#fixed-effects-as-a-special-case",
    "href": "slides/slides3.html#fixed-effects-as-a-special-case",
    "title": "Control Variables",
    "section": "Fixed effects as a special case",
    "text": "Fixed effects as a special case\n\n\n\n\n\n\nDefinition\n\n\nEffects that are the same for every industry, year, firm, or individual can be adjusted for by using fixed effects.\n\n\n\n\n\n\n\n\n\nBenefits\n\n\nWe do not need to measure the specific variables and can just use indicators variables for each category (e.g. for each different industry).\n\n\n\nSee more in chapter 16 of Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides3.html#fixed-effects-for-industry",
    "href": "slides/slides3.html#fixed-effects-for-industry",
    "title": "Control Variables",
    "section": "Fixed effects (for industry)",
    "text": "Fixed effects (for industry)\n\nCausal DiagramSimulationRegressionsSimulation with correlated fixed effectsRegressions with correlated fixed effects\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nIndustry\n\n Industry   \n\nIndustry->CorporateGovernance\n\n    \n\nIndustry->Performance\n\n   \n\n\n\n\n\n\n\n\nNind <- 20\nN <- 5000\ndi <- tibble(\n  ind_number = 1:Nind,\n  ind_CG = rnorm(Nind, 0, 1),\n  ind_performance = rnorm(Nind, 0, 1)\n)\nds <- tibble(\n    ind_number = sample(1:Nind, N, replace = TRUE)) %>%\n  left_join(\n    di, by = \"ind_number\") %>%\n  mutate(\n    CG = rnorm(N, .5 + ind_CG, .2),\n    Performance = rnorm(N, 0 * CG + ind_performance, 1)\n  )\n\n\n\n\nglimpse(di, width = 50)\n\nRows: 20\nColumns: 3\n$ ind_number      <int> 1, 2, 3, 4, 5, 6, 7, 8, …\n$ ind_CG          <dbl> 0.23567083, -0.34180999,…\n$ ind_performance <dbl> 1.1335103, 1.2873377, 0.…\n\n\n\n\nglimpse(ds, width = 50)\n\nRows: 5,000\nColumns: 5\n$ ind_number      <int> 9, 12, 5, 7, 6, 8, 15, 1…\n$ ind_CG          <dbl> 1.91243572, 0.16031769, …\n$ ind_performance <dbl> 0.1773941, -0.1250858, 0…\n$ CG              <dbl> 2.3076069, 0.6604549, 1.…\n$ Performance     <dbl> 0.09219279, -0.37244401,…\n\n\n\n\n\n\n\nlm1 <- lm(Performance ~ CG, data = ds)\nlm2 <- lm(Performance ~ CG + factor(ind_number), data = ds)\nlibrary(fixest)\nfe <- feols(Performance ~ CG | ind_number, data = ds)\n\n\nmsummary(list(lm1, lm2, fe), gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    0.490*** \n    1.054*** \n     \n  \n  \n     \n    (0.019) \n    (0.080) \n     \n  \n  \n    CG \n    −0.076*** \n    0.030 \n    0.030 \n  \n  \n     \n    (0.018) \n    (0.071) \n    (0.061) \n  \n  \n    factor(ind_number)2 \n     \n    0.207** \n     \n  \n  \n     \n     \n    (0.097) \n     \n  \n  \n    factor(ind_number)3 \n     \n    −0.490*** \n     \n  \n  \n     \n     \n    (0.090) \n     \n  \n  \n    factor(ind_number)4 \n     \n    −0.372** \n     \n  \n  \n     \n     \n    (0.161) \n     \n  \n  \n    factor(ind_number)5 \n     \n    −0.410*** \n     \n  \n  \n     \n     \n    (0.099) \n     \n  \n  \n    factor(ind_number)6 \n     \n    −1.907*** \n     \n  \n  \n     \n     \n    (0.169) \n     \n  \n  \n    factor(ind_number)7 \n     \n    −0.083 \n     \n  \n  \n     \n     \n    (0.134) \n     \n  \n  \n    factor(ind_number)8 \n     \n    −1.191*** \n     \n  \n  \n     \n     \n    (0.178) \n     \n  \n  \n    factor(ind_number)9 \n     \n    −0.935*** \n     \n  \n  \n     \n     \n    (0.148) \n     \n  \n  \n    factor(ind_number)10 \n     \n    −1.712*** \n     \n  \n  \n     \n     \n    (0.087) \n     \n  \n  \n    factor(ind_number)11 \n     \n    −1.162*** \n     \n  \n  \n     \n     \n    (0.122) \n     \n  \n  \n    factor(ind_number)12 \n     \n    −1.118*** \n     \n  \n  \n     \n     \n    (0.090) \n     \n  \n  \n    factor(ind_number)13 \n     \n    0.765*** \n     \n  \n  \n     \n     \n    (0.091) \n     \n  \n  \n    factor(ind_number)14 \n     \n    −0.564*** \n     \n  \n  \n     \n     \n    (0.104) \n     \n  \n  \n    factor(ind_number)15 \n     \n    0.730*** \n     \n  \n  \n     \n     \n    (0.129) \n     \n  \n  \n    factor(ind_number)16 \n     \n    0.689*** \n     \n  \n  \n     \n     \n    (0.158) \n     \n  \n  \n    factor(ind_number)17 \n     \n    −2.056*** \n     \n  \n  \n     \n     \n    (0.098) \n     \n  \n  \n    factor(ind_number)18 \n     \n    −0.503*** \n     \n  \n  \n     \n     \n    (0.091) \n     \n  \n  \n    factor(ind_number)19 \n     \n    −1.907*** \n     \n  \n  \n     \n     \n    (0.093) \n     \n  \n  \n    factor(ind_number)20 \n     \n    0.608*** \n     \n  \n  \n     \n     \n    (0.086) \n     \n  \n  \n    Num.Obs. \n    5000 \n    5000 \n    5000 \n  \n  \n    R2 \n    0.003 \n    0.443 \n    0.443 \n  \n  \n    R2 Within \n     \n     \n    0.000 \n  \n  \n    RMSE \n    1.34 \n    1.00 \n    1.00 \n  \n  \n    Std.Errors \n     \n     \n    by: ind_number \n  \n  \n    FE: ind_number \n     \n     \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\n\nWhy do we need the factor(ind_number) formulation?\n\n\n\n\n\nNind <- 20\nN <- 5000\ncorrel <- -0.5\ndi <- tibble(\n    ind_number = 1:Nind,\n    ind_CG = rnorm(Nind, 0, 1)) %>%\n  mutate(\n    ind_performance = sqrt(1 - correl^2) * rnorm(Nind, 0, 1) + correl * ind_CG)\nds <- tibble(\n    ind_number = sample(1:Nind, N, replace = TRUE)) %>%\n  left_join(\n    di, by = \"ind_number\") %>%\n  mutate(\n    CG = rnorm(N, .5 + ind_CG, .2),\n    Performance = rnorm(N, 0 * CG + ind_performance, 1)\n  )\n\n\n\n\nglimpse(di, width = 50)\n\nRows: 20\nColumns: 3\n$ ind_number      <int> 1, 2, 3, 4, 5, 6, 7, 8, …\n$ ind_CG          <dbl> -0.82999044, 0.44908313,…\n$ ind_performance <dbl> -1.12284290, -0.57326559…\n\n\n\n\nglimpse(ds, width = 50)\n\nRows: 5,000\nColumns: 5\n$ ind_number      <int> 20, 17, 2, 1, 9, 20, 5, …\n$ ind_CG          <dbl> -1.1358960, 0.4833522, 0…\n$ ind_performance <dbl> 1.0252155, -0.6131181, -…\n$ CG              <dbl> -0.79273388, 1.44367931,…\n$ Performance     <dbl> 1.75927497, -1.39745179,…\n\n\n\n\n\n\n\nlm1 <- lm(Performance ~ CG, data = ds)\nfe <- feols(Performance ~ CG | ind_number, data = ds)\n\n\nmsummary(list(lm1, fe), gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    −0.033 \n     \n  \n  \n     \n    (0.022) \n     \n  \n  \n    CG \n    −0.279*** \n    −0.012 \n  \n  \n     \n    (0.015) \n    (0.046) \n  \n  \n    Num.Obs. \n    5000 \n    5000 \n  \n  \n    R2 \n    0.067 \n    0.321 \n  \n  \n    R2 Within \n     \n    0.000 \n  \n  \n    RMSE \n    1.17 \n    1.00 \n  \n  \n    Std.Errors \n     \n    by: ind_number \n  \n  \n    FE: ind_number \n     \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\n\nWhy do we need the factor(ind_number) formulation?\nThe trick with the correlated ind_CG and ind_performance"
  },
  {
    "objectID": "slides/slides3.html#what-do-fixed-effects-do",
    "href": "slides/slides3.html#what-do-fixed-effects-do",
    "title": "Control Variables",
    "section": "What do fixed effects do?",
    "text": "What do fixed effects do?\n\nFull SampleHighlight IndustryRemove Industry Effects\n\n\n\n\nCode\nfe_plot <-\n  ggplot(ds, aes(y = Performance, x = CG)) +\n  geom_point()\nplot(fe_plot)\n\n\n\n\n\n\n\n\n\nCode\nfe_colour <-\n  ggplot(ds, aes(y = Performance, x = CG,\n                colour = factor(ind_number))) +\n  geom_point() + theme(legend.position=\"none\") \nplot(fe_colour)\n\n\n\n\n\n\n\n\n\nCode\nfe_demean <- group_by(ds, ind_number) %>%\n  mutate(Performance2 = Performance - mean(Performance),\n         CG2 = CG - mean(CG)) %>%\n  ggplot(aes(y = Performance2, x = CG2,\n             colour = factor(ind_number))) +\n  geom_point() + theme(legend.position=\"none\") \nplot(fe_demean)"
  },
  {
    "objectID": "slides/slides3.html#speedboat-racing-example-booth2017",
    "href": "slides/slides3.html#speedboat-racing-example-booth2017",
    "title": "Control Variables",
    "section": "Speedboat Racing Example (Booth and Yamamura 2017)",
    "text": "Speedboat Racing Example (Booth and Yamamura 2017)\n\n\n\nMixed-sex and single-sex races determined by lottery (Randomisation)\n7 race courses\nMultiple races in the same month and location\n\n\n\n\n\n\n\n\n\nspeedboat\n\n  \n\nave_ability\n\n ave_ability   \n\nltime\n\n ltime   \n\nave_ability->ltime\n\n    \n\nmixed_race\n\n mixed_race   \n\nmixed_race->ltime\n\n    \n\nfemale\n\n female   \n\nfemale->ave_ability\n\n    \n\nfemale->ltime\n\n    \n\ncourse\n\n course   \n\ncircumstances\n\n circumstances   \n\ncourse->circumstances\n\n    \n\nmonth_location\n\n month_location   \n\nmonth_location->circumstances\n\n    \n\ncircumstances->ltime\n\n    \n\ncircumstances->female"
  },
  {
    "objectID": "slides/slides3.html#results-of-speedboat-races",
    "href": "slides/slides3.html#results-of-speedboat-races",
    "title": "Control Variables",
    "section": "Results of Speedboat Races",
    "text": "Results of Speedboat Races\n\n\nCode\nload(here(\"data\", \"booth_yamamura.Rdata\"))\ntable <- as_tibble(table) %>%\n  select(p_id, women_dat, time, ltime, mix_ra, course,\n         race_id, yrmt_locid)\ntable_clean <- filter(table, complete.cases(table)) %>%\n  select(ltime, women_dat, mix_ra, course, p_id, race_id,\n         yrmt_locid)\nltime_reg <- feols(ltime ~ women_dat : mix_ra + mix_ra\n                   | course + p_id + yrmt_locid,\n                   cluster = \"race_id\",\n                   data = table_clean)\nmsummary(ltime_reg, gof_omit = gof_omit, stars = stars)\n\n\n\n\n \n  \n      \n     (1) \n  \n \n\n  \n    mix_ra \n    −0.002*** \n  \n  \n     \n    (0.000) \n  \n  \n    women_dat × mix_ra \n    0.007*** \n  \n  \n     \n    (0.001) \n  \n  \n    Num.Obs. \n    142346 \n  \n  \n    R2 \n    0.361 \n  \n  \n    R2 Within \n    0.001 \n  \n  \n    RMSE \n    0.02 \n  \n  \n    Std.Errors \n    by: race_id \n  \n  \n    FE: course \n    X \n  \n  \n    FE: p_id \n    X \n  \n  \n    FE: yrmt_locid \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\nThis requires an explanation of interactions. Luckily, it’s relatively simple with two discrete variables.\n| ltime | man    | woman  |\n|-------|:------:|:------:|\n| same  | 0      | 0      |\n| mixed | -0.002 | 0.005  |"
  },
  {
    "objectID": "slides/slides3.html#bad-controls-survival-bias-selection-bias-self-selection-bias",
    "href": "slides/slides3.html#bad-controls-survival-bias-selection-bias-self-selection-bias",
    "title": "Control Variables",
    "section": "Bad Controls, Survival Bias, Selection Bias, Self-Selection Bias",
    "text": "Bad Controls, Survival Bias, Selection Bias, Self-Selection Bias\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nStockPrice\n\n StockPrice   \n\nPerformance->StockPrice\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nSurvival\n\n Survival   \n\nPerformance->Survival\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nSP500\n\n SP500   \n\nPerformance->SP500\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nIPO\n\n IPO   \n\nPerformance->IPO"
  },
  {
    "objectID": "slides/slides3.html#example-in-the-assignment",
    "href": "slides/slides3.html#example-in-the-assignment",
    "title": "Control Variables",
    "section": "Example in the assignment",
    "text": "Example in the assignment\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nPerformance\n\n Performance   \n\nCSR_report\n\n CSR_report   \n\nPerformance->CSR_report\n\n    \n\nScandals\n\n Scandals   \n\nPerformance->Scandals\n\n    \n\nReturn\n\n Return   \n\nCSR_report->Return\n\n    \n\nObserved_Report\n\n Observed_Report   \n\nCSR_report->Observed_Report"
  },
  {
    "objectID": "slides/slides3.html#simulation-bad-control",
    "href": "slides/slides3.html#simulation-bad-control",
    "title": "Control Variables",
    "section": "Simulation Bad Control",
    "text": "Simulation Bad Control\n\nCausal GraphSimulateResults\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance->Performance\n\n    \n\nMarketReturn\n\n MarketReturn   \n\nCorporateGovernance->MarketReturn\n\n    \n\nPerformance->MarketReturn\n\n   \n\n\n\n\n\n\n\n\nd <- tibble(corp_gov = rnorm(N, 0, 1)) %>%\n  mutate(acc_profit = rnorm(N, corp_gov, sd = 3),\n         market_return = rnorm(N, 2 * corp_gov + acc_profit,\n                               sd = 3))\nlm1 <- lm(acc_profit ~ corp_gov, data = d)\nlm2 <- lm(acc_profit ~ corp_gov + market_return, data = d)\n\n\n\n\nmsummary(list(lm1, lm2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    0.039 \n    0.012 \n  \n  \n     \n    (0.042) \n    (0.030) \n  \n  \n    corp_gov \n    1.000*** \n    −0.489*** \n  \n  \n     \n    (0.042) \n    (0.037) \n  \n  \n    market_return \n     \n    0.498*** \n  \n  \n     \n     \n    (0.007) \n  \n  \n    Num.Obs. \n    5000 \n    5000 \n  \n  \n    R2 \n    0.101 \n    0.551 \n  \n  \n    RMSE \n    2.98 \n    2.11 \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides3.html#survival-bias",
    "href": "slides/slides3.html#survival-bias",
    "title": "Control Variables",
    "section": "Survival Bias",
    "text": "Survival Bias\n\nSimulateResults\n\n\n\nd <- mutate(d, survival = if_else(market_return > 5, 1, 0))\n\n\n\n\nlm1 <- lm(acc_profit ~ corp_gov, data = filter(d, survival == 1))\nlm2 <- lm(acc_profit ~ corp_gov * survival, data = d)\nmsummary(list(lm1, lm2), gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n     (1) \n      (2) \n  \n \n\n  \n    (Intercept) \n    3.518*** \n    −0.549*** \n  \n  \n     \n    (0.115) \n    (0.043) \n  \n  \n    corp_gov \n    −0.137 \n    0.606*** \n  \n  \n     \n    (0.095) \n    (0.045) \n  \n  \n    survival \n     \n    4.067*** \n  \n  \n     \n     \n    (0.135) \n  \n  \n    corp_gov × survival \n     \n    −0.743*** \n  \n  \n     \n     \n    (0.115) \n  \n  \n    Num.Obs. \n    853 \n    5000 \n  \n  \n    R2 \n    0.002 \n    0.262 \n  \n  \n    RMSE \n    2.43 \n    2.70 \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\n\n\n\nOne interpretation is that a firm can survive by being lucky (and having high returns) and by having good corporate governance (which translates in high returns). The survivors all are more likely to be having good corporate governance and there is little that can be explained further."
  },
  {
    "objectID": "slides/slides3.html#visualisation-of-colliders-and-interactions",
    "href": "slides/slides3.html#visualisation-of-colliders-and-interactions",
    "title": "Control Variables",
    "section": "Visualisation of Colliders (and Interactions)",
    "text": "Visualisation of Colliders (and Interactions)\n\n\n\n\nFull SampleSurvival HighlightedSurvival Only"
  },
  {
    "objectID": "slides/slides3.html#pitching-format",
    "href": "slides/slides3.html#pitching-format",
    "title": "Control Variables",
    "section": "Pitching Format",
    "text": "Pitching Format\n\n\n\nDescription (Important)\n\nTitle\nResearch Question\nKey Paper\nMotivation\n\nTHREE (IDioT) (Important)\n\nIdea\nData\nTools"
  },
  {
    "objectID": "slides/slides3.html#pitching-format-1",
    "href": "slides/slides3.html#pitching-format-1",
    "title": "Control Variables",
    "section": "Pitching Format",
    "text": "Pitching Format\n\n\n\nDescription (Important)\n\nTitle\nResearch Question\nKey Paper\nMotivation\n\nTHREE (IDioT) (Important)\n\nIdea\nData\nTools\n\n\n\n\nTWO\n\nWhat’s new?\nSo what?\n\nONE contribution\nOther considerations."
  },
  {
    "objectID": "slides/slides4.html#did-we-not-cover-that-already",
    "href": "slides/slides4.html#did-we-not-cover-that-already",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Did we not cover that already?",
    "text": "Did we not cover that already?\n\nYes, but briefly\nYes, but starting from the perspective of a regression (and the code)\n\n\nThe regression perspective is not bad. It means that we can see that more advanced regression techniques can be implemented in our linear regression framework. It’s also how most researchers in accounting and finance have been thought to think about research methods. However, there is a shift coming from economics where the focus is more on the research design."
  },
  {
    "objectID": "slides/slides4.html#the-focus-is-on-research-design",
    "href": "slides/slides4.html#the-focus-is-on-research-design",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The focus is on Research Design",
    "text": "The focus is on Research Design\n\n\n\n\n\n\nImportant\n\n\n\nWhich data should we use?\nWhich comparison identifies the effect that we are interested in?\n\n\n\n\n\n\nIs there sufficient variation that can identify the effect. - See also the pitching document - A specific example is the identification of performance effects\n\n\n\nFor instance Alcohol and Mortality, Chapter 5 in Huntington-Klein (2021).\nIs there sufficient variation in the treatment and the outcome?\nAre we reasonably sure that there are no confounders or only a few and we can measure them?"
  },
  {
    "objectID": "slides/slides4.html#prevously-we-used-models-and-assumptions-to-identify-effects",
    "href": "slides/slides4.html#prevously-we-used-models-and-assumptions-to-identify-effects",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Prevously, we used models and assumptions to identify effects",
    "text": "Prevously, we used models and assumptions to identify effects\n\n\nMathematical models\n\\[\nV = T^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n                 \\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\] \\[\n\\alpha_T + \\alpha_K + \\alpha_L = 1\n\\]\n\n\\(V =\\) The value of the firm\n\\(K =\\) Capital of the firm\n\\(L =\\) Labour of the firm\n\\(T =\\) CEO talent/skills/ability/experience\n\n\n\n\nDAGs\n\n\n\n\n\n\n\nspeedboat\n\n  \n\nave_ability\n\n ave_ability   \n\nltime\n\n ltime   \n\nave_ability->ltime\n\n    \n\nmixed_race\n\n mixed_race   \n\nmixed_race->ltime\n\n    \n\nfemale\n\n female   \n\nfemale->ave_ability\n\n    \n\nfemale->ltime\n\n    \n\ncourse\n\n course   \n\ncircumstances\n\n circumstances   \n\ncourse->circumstances\n\n    \n\nmonth_location\n\n month_location   \n\nmonth_location->circumstances\n\n    \n\ncircumstances->ltime\n\n    \n\ncircumstances->female"
  },
  {
    "objectID": "slides/slides4.html#just-focus-on-a-setting-where-we-are-confident-in-the-assumptions",
    "href": "slides/slides4.html#just-focus-on-a-setting-where-we-are-confident-in-the-assumptions",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Just focus on a setting where we are confident in the assumptions",
    "text": "Just focus on a setting where we are confident in the assumptions\n\n\nActual random assignment\nSpeedboat racing, game shows, Vietnam draft\nNatural experiments\nSee Gippel, Smith, and Zhu (2015), Chapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nPolicy Changes\nChapter 18, Difference-in-Difference in Huntington-Klein (2021)\nDiscrete cutoffs\ne.g. WAM > 75, Chapter 20 Regression Continuity Design in Huntington-Klein (2021)\nUnexpected news\nChapter 17 Event Studies in Huntington-Klein (2021)\n\n\n\nNatural experiments is not the best terminology because most of these instances are not natural nor real experiments. Nevertheless, I still prefer the name over an instrumental variable approach. In too many proposals, I read an off hand comment that the student proposes to use a robustness test where they are going to use an instrumental variable approach. My answer to that is (1) if you have a natural experiment where you can exploit an instrumental variable, this should be the main analysis and (2) instrumental variables need to be defended as a research design based on your understanding of the setting. Calling the design a natural experiment forces you to think more about the experiment (i.e. the research design)."
  },
  {
    "objectID": "slides/slides4.html#look-for-these-designs",
    "href": "slides/slides4.html#look-for-these-designs",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Look for these designs!",
    "text": "Look for these designs!\n\n\nBased on your understanding of the industry and setting or the Data Generating Process\nWhen you read good papers for this unit and other units.\n\n\n\nThis is one of the main reasons that I want you to read broadly. It is unlikely that you will find a paper with a good research design exactly for the research question that you are interested in. However, you might find inspiration in similar or related fields that help you to design a better study for the research question that you are interested in."
  },
  {
    "objectID": "slides/slides4.html#what-effect-can-we-identify",
    "href": "slides/slides4.html#what-effect-can-we-identify",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "What effect can we identify?",
    "text": "What effect can we identify?\n\nAverage Treatment Effect\nAverage Treatment on the Treated\nAverage Treatment on the Untreated\nLocal Average Treatment Effect\nWeigthed Average Treatment Effect\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)\n\n\nDo you have an example of an effect that we might be interested in in Accounting and Finance?\nAverage implies that not all firms will respond the same to the treatment. This is the source of a lot trouble.\nAverage over which population?\nHow would you put these different effects in your own words?\nWATE is evil and I am going to largely ignore it."
  },
  {
    "objectID": "slides/slides4.html#it-all-depends-on-where-the-variation-is-coming-from.",
    "href": "slides/slides4.html#it-all-depends-on-where-the-variation-is-coming-from.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "It all depends on where the variation is coming from.",
    "text": "It all depends on where the variation is coming from.\n\n\n\n\n\n\nWarning\n\n\nDifferent firms react differently and are differently represented in the control group and the treatment group.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith actual random assignment, you probably have an ATE for the population that received the assignment.\nIf you can use a control group because that is what the treated group would look like if they were not treated, you probably have an ATT.\nIf you use a natural experiment to identify part of the variation, you probably have a LATE.\n\n\n\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#why-do-we-care",
    "href": "slides/slides4.html#why-do-we-care",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Why do we care?",
    "text": "Why do we care?\n\n\n\n\n\n\nResearch Design\n\n\nThere is a deep connection between the variation in your research design and the effect you can identify.\n\n\n\n\n\n\n\n\n\n\nPolicy Implications\n\n\nWhether your study has implications for “regulators and investors” depends heavily on the type of effect you can identify.\n\n\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)\n\nThat is the setting of your data determines which research design you can use. The research design determines which effect you can identify. The effect you can identify determines which conclusions you can draw."
  },
  {
    "objectID": "slides/slides4.html#generate-the-data",
    "href": "slides/slides4.html#generate-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Generate the Data",
    "text": "Generate the Data\n\nN <- 1000\nrd1 <- tibble(\n  firm = 1:N,\n  high_performance = rbinom(N, 1, 0.5),\n  noise = rnorm(N, 0, 3)\n) %>%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_donation = 4 - 8 / performance + noise,\n    payoff_no_donation = 1 + noise\n  )\nglimpse(rd1) \n\nRows: 1,000\nColumns: 7\n$ firm               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …\n$ high_performance   <int> 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1…\n$ noise              <dbl> 0.8634427, 3.9991062, -2.208085…\n$ donation           <int> 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1…\n$ performance        <dbl> 4, 1, 4, 1, 1, 1, 1, 4, 4, 4, 4…\n$ payoff_donation    <dbl> 2.863443e+00, -8.938289e-04, -2…\n$ payoff_no_donation <dbl> 1.8634427, 4.9991062, -1.208085…\n\n\n\n\nWhat is the effect that we are we interested in?\nWhat are the policy implications?"
  },
  {
    "objectID": "slides/slides4.html#have-a-look-at-the-data",
    "href": "slides/slides4.html#have-a-look-at-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Have a look at the data",
    "text": "Have a look at the data\n\n\nWe will talk more about the pivot_wider and pivot_longer functions in week 7."
  },
  {
    "objectID": "slides/slides4.html#have-a-second-look-at-the-data",
    "href": "slides/slides4.html#have-a-second-look-at-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Have a second look at the data",
    "text": "Have a second look at the data"
  },
  {
    "objectID": "slides/slides4.html#real-data-does-not-have-the-counterfactuals.-we-only-observe-blue",
    "href": "slides/slides4.html#real-data-does-not-have-the-counterfactuals.-we-only-observe-blue",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Real data does not have the counterfactuals. We only observe blue!",
    "text": "Real data does not have the counterfactuals. We only observe blue!\n\n\n\n\n\n\n\nNote\n\n\nThe actual sample determines which comparisons we can make.\n\n\n\n\nWhy does this work? What effect are we identifying and how."
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-simulated-example-with-averages",
    "href": "slides/slides4.html#lets-redo-the-simulated-example-with-averages",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the simulated example with averages",
    "text": "Let’s redo the simulated example with averages\n\nrd1 %>%\n  mutate(causal_effect = payoff_donation - payoff_no_donation) %>%\n  summarise(M_causal = mean(causal_effect),\n            sd_causal = sd(causal_effect),\n            N = n()) %>%\n  knitr::kable(format = \"markdown\", digits = 2)\n\n\n\n\nM_causal\nsd_causal\nN\n\n\n\n\n-2.14\n3\n1000\n\n\n\n\n\n\n\nThe causal effect of donating for each firm is difference in payoff between donating and not donating.\nWhat effect are we estimating here?"
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-simulated-example-with-averages-1",
    "href": "slides/slides4.html#lets-redo-the-simulated-example-with-averages-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the simulated example with averages",
    "text": "Let’s redo the simulated example with averages\n\n\n\n\n\nM_causal\nsd_causal\nN\n\n\n\n\n-2.14\n3\n1000\n\n\n\n\n\n\nrd1 %>%\n  mutate(causal_effect = payoff_donation - payoff_no_donation) %>%\n  group_by(donation) %>%\n  summarise(M_causal = mean(causal_effect),\n            sd_causal = sd(causal_effect),\n            N = n()) %>%\n  knitr::kable(format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_causal\nsd_causal\nN\n\n\n\n\n0\n-5\n0\n523\n\n\n1\n1\n0\n477"
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-regression-with-averages",
    "href": "slides/slides4.html#lets-redo-the-regression-with-averages",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the regression with averages",
    "text": "Let’s redo the regression with averages\n\nsummary_data  <- rd1 %>%\n  group_by(donation) %>%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-4.17\n0.83\n\n\n1\n1.76\n0.76\n\n\n\n\ncausal_effect_true <-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 1]\ncausal_effect_reg <-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 0]\n\n\n\n\n\n\n\nNote\n\n\n\nThe true ATT is 1\nThe effect estimated by the regression is 0.926"
  },
  {
    "objectID": "slides/slides4.html#if-you-do-not-believe-me-here-is-the-regression",
    "href": "slides/slides4.html#if-you-do-not-believe-me-here-is-the-regression",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "If you do not believe me, here is the regression",
    "text": "If you do not believe me, here is the regression\n\n\n\nrd1 <- mutate(rd1, actual_payoff =\n       ifelse(donation, payoff_donation, payoff_no_donation))\nols <- feols(actual_payoff ~ donation, data = rd1)\n\n\n\n\n\n\n \n  \n      \n     (1) \n  \n \n\n  \n    (Intercept) \n    0.833*** \n  \n  \n     \n    (0.131) \n  \n  \n    donation \n    0.926*** \n  \n  \n     \n    (0.190) \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides4.html#what-could-possibly-go-wrong",
    "href": "slides/slides4.html#what-could-possibly-go-wrong",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "What could possibly go wrong?",
    "text": "What could possibly go wrong?\n\nrd2 <- tibble(\n  high_performance = rbinom(N, 1, 0.5),\n  noise = rnorm(N, 0, 3)) %>%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_donation = 4 - 8 / performance + noise,\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + noise\n  )"
  },
  {
    "objectID": "slides/slides4.html#causal-effect-estimates-with-a-confounder",
    "href": "slides/slides4.html#causal-effect-estimates-with-a-confounder",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Effect Estimates with a Confounder",
    "text": "Causal Effect Estimates with a Confounder\n\nsummary_data  <- rd2 %>%\n  group_by(donation) %>%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-4.01\n1.99\n\n\n1\n1.97\n0.97\n\n\n\n\ncausal_effect_true <-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 1]\ncausal_effect_reg <-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 0]\n\n\nThe true ATT is 1\nThe effect estimated by the regression is -0.024"
  },
  {
    "objectID": "slides/slides4.html#where-is-the-variation-coming-from",
    "href": "slides/slides4.html#where-is-the-variation-coming-from",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Where is the variation coming from?",
    "text": "Where is the variation coming from?\n\n\n\n\n\n\nWe need firms that make mistakes\n\n\n\nFirms that should donate but do not always do it.\nFirms that should not donate but sometimes donate."
  },
  {
    "objectID": "slides/slides4.html#panel-data-simulation-100-firms",
    "href": "slides/slides4.html#panel-data-simulation-100-firms",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Panel Data Simulation (100 firms)",
    "text": "Panel Data Simulation (100 firms)\n\nN <- 100\nrd_firm <- tibble(\n  firm = 1:N,\n  high_performance = rbinom(N, 1, 0.5),\n  other_payoff = rnorm(N, 0, 3)) %>%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + other_payoff,\n    payoff_donation = 4 - 8/performance + other_payoff\n  )\nsummary_data  <- rd_firm %>%\n  group_by(donation) %>%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, digits = 1)\n\n\n\n \n  \n    donation \n    M_payoff_donation \n    M_payoff_no_donation \n  \n \n\n  \n    0 \n    -3.9 \n    2.1 \n  \n  \n    1 \n    1.8 \n    0.8"
  },
  {
    "objectID": "slides/slides4.html#panel-data-simulation-10-time-periods",
    "href": "slides/slides4.html#panel-data-simulation-10-time-periods",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Panel Data Simulation (10 time periods)",
    "text": "Panel Data Simulation (10 time periods)\nThe variation comes from high performers not donating some years\n\nT <- 10\nrd_panel_forget <- tibble(\n  firm = rep(1:N, each = T),\n  year = rep(1:T, times = N)) %>%\n  left_join(rd_firm, by = \"firm\") %>%\n  mutate(forget_donation = rbinom(N * T, 1, plogis(-other_payoff)),\n         actual_donation = (1 - forget_donation) * donation,\n         actual_payoff = ifelse(actual_donation == 1,\n                                payoff_donation, payoff_no_donation))\n\n\n\nThe way we simulate the data reflects the firm fixed effects and the time varying effects.\nWhich effect are we identifying with this sample?"
  },
  {
    "objectID": "slides/slides4.html#the-new-assignment",
    "href": "slides/slides4.html#the-new-assignment",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The New Assignment",
    "text": "The New Assignment\n\nRun a fixed effect model and interpret the result\nCreate a new dataset where all firms make mistakes\nRun a fixed effect model and interpret the result"
  },
  {
    "objectID": "slides/slides4.html#causal-diagram",
    "href": "slides/slides4.html#causal-diagram",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Diagram",
    "text": "Causal Diagram\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx->y\n\n    \n\ncollider\n\n collider   \n\nx->collider\n\n    \n\ny->collider\n\n    \n\nconfounder\n\n confounder   \n\nconfounder->x\n\n    \n\nconfounder->y"
  },
  {
    "objectID": "slides/slides4.html#causal-diagram-1",
    "href": "slides/slides4.html#causal-diagram-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Diagram",
    "text": "Causal Diagram\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx->y\n\n    \n\ncollider\n\n collider   \n\nx->collider\n\n    \n\ny->collider\n\n    \n\nconfounder\n\n confounder   \n\nconfounder->x\n\n    \n\nconfounder->y\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx->y\n\n    \n\ncollider\n\n collider   \n\nx->collider\n\n    \n\ny->collider\n\n    \n\niv\n\n iv   \n\niv->x\n\n    \n\nrandom\n\n random   \n\nrandom->iv\n\n    \n\nconfounder\n\n confounder   \n\nconfounder->x\n\n    \n\nconfounder->y\n\n   \n\n\n\n\n\n\n\nSee Instrumental Variables, Chapter 19 in Huntington-Klein (2021).\n\nMechanically, there are two regressions. (2-stage-least-squares) 1. Use the IV to estimate the randomly generated variation in X -> fitted(X) 2. Use fitted(X) to estimate the effect of random variation in X on Y"
  },
  {
    "objectID": "slides/slides4.html#simulation-and-implementation-with-fixest",
    "href": "slides/slides4.html#simulation-and-implementation-with-fixest",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation and Implementation with fixest",
    "text": "Simulation and Implementation with fixest\n\n#|label: simulation-iv\nd <- tibble(\n  iv = rnorm(N, 0, 1),\n  confounder = rnorm(N, 0, 1)) %>%\n  mutate(\n    x = rnorm(N, .6 * iv - .6 * confounder, .6),\n    y = rnorm(N, .6 * x + .6 * confounder, .6),\n    survival = if_else(y > 0, 1, 0)\n  )\nsurv <- filter(d, survival == 1)\nlm1 <- lm(y ~ x, d)\nlm2 <- lm(y ~ x + confounder, d)\nlm3 <- lm(y ~ x, surv)\nlm4 <- lm(y ~ x + confounder, surv)\niv1 <- feols(y ~ 1 | 0 | x ~ iv, data = d)\niv2 <- feols(y ~ 1 | 0 | x ~ iv, data = surv)\n\n\nAll the exogenous variable are in the tibble statement, all the endogenous variables are in the mutate statement. That is not a coincidence. It also highlights the value and tight link between being able to simulate your theory and understanding it.\nNote, the collider bias is the biggest problem if the selection bias is on both x and y because then the collider bias effects the first stage regressions."
  },
  {
    "objectID": "slides/slides4.html#simulation-results-with-a-real-effect-of-0.6",
    "href": "slides/slides4.html#simulation-results-with-a-real-effect-of-0.6",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation results with a real effect of 0.6",
    "text": "Simulation results with a real effect of 0.6\n\nmsummary(list(\"confounded\" = lm1, \"with control\" = lm2, \"collider\" = lm3, \"collider\" = lm4,\n              \"iv no collider\" = iv1, \"iv with collider\" = iv2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n    confounded \n    with control \n    collider \n    collider  \n    iv no collider \n    iv with collider \n  \n \n\n  \n    (Intercept) \n    −0.082 \n    −0.082 \n    0.591*** \n    0.442*** \n    −0.095 \n    0.546*** \n  \n  \n     \n    (0.082) \n    (0.061) \n    (0.072) \n    (0.089) \n    (0.104) \n    (0.082) \n  \n  \n    x \n    0.257*** \n    0.595*** \n    0.103 \n    0.293*** \n     \n     \n  \n  \n     \n    (0.081) \n    (0.070) \n    (0.067) \n    (0.097) \n     \n     \n  \n  \n    confounder \n     \n    0.644*** \n     \n    0.248** \n     \n     \n  \n  \n     \n     \n    (0.070) \n     \n    (0.095) \n     \n     \n  \n  \n    fit_x \n     \n     \n     \n     \n    0.874*** \n    0.271** \n  \n  \n     \n     \n     \n     \n     \n    (0.195) \n    (0.129) \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\nThis is not strictly a collider because there is no effect of x on survival. However, it already shows that there are problems with “simple” selection bias."
  },
  {
    "objectID": "slides/slides4.html#simulation-without-an-effect",
    "href": "slides/slides4.html#simulation-without-an-effect",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation without an effect",
    "text": "Simulation without an effect\n\nd <- tibble(\n  iv = rnorm(N, 0, 1),\n  confounder = rnorm(N, 0, 1)) %>%\n  mutate(\n    x = rnorm(N, .6 * iv - .6 * confounder, .6),\n    y = rnorm(N, .6 * confounder, .6),\n    survival = if_else(y > 0, 1, 0)\n  )\nsurv <- filter(d, survival == 1)\nlm1 <- lm(y ~ x, d)\nlm2 <- lm(y ~ x + confounder, d)\nlm3 <- lm(y ~ x, surv)\nlm4 <- lm(y ~ x + confounder, surv)\niv1 <- feols(y ~ 1 | 0 | x ~ iv, data = d)\niv2 <- feols(y ~ 1 | 0 | x ~ iv, data = surv)"
  },
  {
    "objectID": "slides/slides4.html#simulation-without-an-effect-1",
    "href": "slides/slides4.html#simulation-without-an-effect-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation without an effect",
    "text": "Simulation without an effect\n\nmsummary(list(\"confounded\" = lm1, \"with control\" = lm2, \"collider\" = lm3, \"collider\" = lm4,\n              \"iv no collider\" = iv1, \"iv with collider\" = iv2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n    confounded \n    with control \n    collider \n    collider  \n    iv no collider \n    iv with collider \n  \n \n\n  \n    (Intercept) \n    0.017 \n    0.005 \n    0.696*** \n    0.483*** \n    0.057 \n    0.714*** \n  \n  \n     \n    (0.075) \n    (0.058) \n    (0.083) \n    (0.077) \n    (0.083) \n    (0.087) \n  \n  \n    x \n    −0.125* \n    0.151** \n    −0.020 \n    0.118* \n     \n     \n  \n  \n     \n    (0.069) \n    (0.064) \n    (0.080) \n    (0.068) \n     \n     \n  \n  \n    confounder \n     \n    0.619*** \n     \n    0.484*** \n     \n     \n  \n  \n     \n     \n    (0.077) \n     \n    (0.093) \n     \n     \n  \n  \n    fit_x \n     \n     \n     \n     \n    0.181 \n    0.035 \n  \n  \n     \n     \n     \n     \n     \n    (0.112) \n    (0.113) \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides4.html#examplesitting-duck-governors-by-falk2018",
    "href": "slides/slides4.html#examplesitting-duck-governors-by-falk2018",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Example:Sitting Duck Governors by Falk and Shelton (2018)",
    "text": "Example:Sitting Duck Governors by Falk and Shelton (2018)\n\n\n\n\n\n\nNote\n\n\n\nResearch Question: Does political uncertainty effect investment?\nMore uncertainty in a state when governor does not come up for reelection.\nState level laws with term limits (~ Random)\n\n\n\n\n\nAn exercise to be run in class"
  },
  {
    "objectID": "slides/slides4.html#data",
    "href": "slides/slides4.html#data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Data",
    "text": "Data\n\nlibrary(readit)\nduck <- readit(here(\"data\", \"LameDuckData.dta\")) %>%\n  select(-starts_with(\"nstate\"), -starts_with(\"stdum\"),\n         -starts_with(\"yd_alt\")) %>%\n  group_by(statename) %>%\n  arrange(year) %>%\n  mutate(log_I_1 = lag(log_I), log_I_2 = lag(log_I, 2),\n         log_Y_1 = lag(log_Y), log_Y_2 = lag(log_Y, 2),\n         log_real_GDP_1 = lag(log_real_GDP),\n         log_real_GDP_2 = lag(log_real_GDP, 2)) %>%\n  ungroup() %>%\n  arrange(statename) %>%\n  filter(year >= 1967, year <= 2004)"
  },
  {
    "objectID": "slides/slides4.html#reduced-form",
    "href": "slides/slides4.html#reduced-form",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Reduced Form",
    "text": "Reduced Form\n\nform_red <- formula(\n  log_I ~ gov_exogenous_middling + log_I_1 + log_I_2 +\n  log_Y + log_Y_1 + log_Y_2 + log_real_GDP + log_real_GDP_1 +\n  log_real_GDP_2 | statename\n  )\nred_reg <- feols(form_red, data = duck)"
  },
  {
    "objectID": "slides/slides4.html#stage-least-squares-2sls",
    "href": "slides/slides4.html#stage-least-squares-2sls",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "2 Stage Least Squares (2SLS)",
    "text": "2 Stage Least Squares (2SLS)\n\nform_iv <- formula(log_I ~ log_I_1 + log_I_2 +\n  log_Y + log_Y_1 + log_Y_2 + log_real_GDP + log_real_GDP_1 +\n  log_real_GDP_2\n  # fixed effects\n  | statename\n  # 1st regression\n  | uncertainty_continuous ~ gov_exogenous_middling\n  )\niv_reg <- feols(form_iv, data = duck)"
  },
  {
    "objectID": "slides/slides4.html#results",
    "href": "slides/slides4.html#results",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Results",
    "text": "Results\n\ncoef_map = c(\"gov_exogenous_middling\" = \"lame duck governor\",\n             \"fit_uncertainty_continuous\" = \"uncertainty\")\nmsummary(list(\"reduced\" = red_reg,\n              \"first stage iv\" = summary(iv_reg, stage = 1),\n              \"second stage iv\" = iv_reg),\n         gof_omit = gof_omit, stars = stars,\n         coef_map = coef_map)\n\n\n\n \n  \n      \n    reduced \n    first stage iv \n    second stage iv \n  \n \n\n  \n    lame duck governor \n    −0.049** \n    1.801*** \n     \n  \n  \n     \n    (0.021) \n    (0.112) \n     \n  \n  \n    uncertainty \n     \n     \n    −0.027** \n  \n  \n     \n     \n     \n    (0.012) \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides4.html#diagnostics-test-for-endogeneity-durbin-wu-hausmann",
    "href": "slides/slides4.html#diagnostics-test-for-endogeneity-durbin-wu-hausmann",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Diagnostics: Test for endogeneity (Durbin-Wu-Hausmann)",
    "text": "Diagnostics: Test for endogeneity (Durbin-Wu-Hausmann)\n\n\n\n\n\n\nNote\n\n\nIs the IV result different from the OLS result?\n\n\n\n\nsumm_iv <- summary(iv_reg)\nsumm_1st <- summary(iv_reg, stage = 1)\nsumm_iv$iv_wh$stat  # iv wu hausmann\n\n[1] 3.829033\n\nsumm_iv$iv_wh$p     # iv wu hausmann\n\n[1] 0.05054677\n\n\nInstrumental Variables, Chapter 19 in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#diagnostics-test-for-weak-instrument",
    "href": "slides/slides4.html#diagnostics-test-for-weak-instrument",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Diagnostics: Test for weak instrument",
    "text": "Diagnostics: Test for weak instrument\n\n\n\n\n\n\nNote\n\n\nIs the instrument predicting the variable we want it to predict?\n\n\n\n\nfitstat(iv_reg, type = \"ivf\")\n\nF-test (1st stage), uncertainty_continuous: stat = 331.0, p < 2.2e-16, on 1 and 1,640 DoF."
  },
  {
    "objectID": "slides/slides4.html#new-assignment",
    "href": "slides/slides4.html#new-assignment",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "New Assignment",
    "text": "New Assignment\nLet’s assume that firms are less likely to donate when there is a local election\n\nN <- 5000\nrd_iv_el <- tibble(\n  high_performance = rbinom(N, 1, .5),\n  extra_payoff = rnorm(N, 0, 3),\n  local_election = rbinom(N, 1, .33)) %>%\n  mutate(\n    actual_donation = ifelse(high_performance == 1, 1 - local_election, 0),\n    payoff_donation = ifelse(high_performance == 1, 2, - 4) + extra_payoff,\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + extra_payoff,\n    actual_payoff = ifelse(actual_donation == 1,\n                           payoff_donation, payoff_no_donation))\n\n\n\nWhich effect can we identify with this data?\nRun the instrumental variable analyses and interpret the results."
  },
  {
    "objectID": "slides/slides4.html#this-paper-is-a-finished-product-your-pitch-proposal-or-dissertation-is-not.",
    "href": "slides/slides4.html#this-paper-is-a-finished-product-your-pitch-proposal-or-dissertation-is-not.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "This paper is a finished product, your pitch, proposal, or dissertation is not.",
    "text": "This paper is a finished product, your pitch, proposal, or dissertation is not.\nWe are grateful to Michael Roberts (the Editor), the Associate Editor, two anonymous referees, Marianne Bertrand, Ing-Haw Cheng, Ken French, Ed Glaeser, Todd Gormley, Ben Iverson (discus- sant), Steve Kaplan, Borja Larrain (discussant), Jonathan Lewellen, Katharina Lewellen, David Matsa (discussant), David Metzger (discussant), Toby Moskowitz, Candice Prendergast, Enrichetta Ravina (discussant), Amit Seru, and Wei Wang (discussant) for helpful suggestions. We thank seminar participants at AFA, BYU, CICF Conference, Depaul, Duke, Gerzensee ESSFM, Harvard, HKUST Finance Symposium, McGill Todai Conference, Finance UC Chile, Helsinki, IDC Herzliya Finance Conference, NBER Corporate Finance and Personnel Meetings, SEC, Simon Fraser Uni- versity, Stanford, Stockholm School of Economics, University of Amsterdam, UC Berkeley, UCLA, and Wharton for helpful comments. We thank David Yermack for his generosity in sharing data. We thank Matt Turner at Pearl Meyer, Don Delves at the Delves Group, and Stephen O’Byrne at Shareholder Value Advisors for helping us understand the intricacies of executive stock option plans. Menaka Hampole provided excellent research assistance. We acknowledge financial support from the Initiative on Global Markets.\n\nOn the one hand, we do not expect you to come up with a design like this. On the other hand, why not use these hard won insights."
  },
  {
    "objectID": "slides/slides4.html#this-paper-has-1-one-research-question.-this-is-a-good-thing",
    "href": "slides/slides4.html#this-paper-has-1-one-research-question.-this-is-a-good-thing",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "This paper has 1 (one!) research question. This is a good thing!",
    "text": "This paper has 1 (one!) research question. This is a good thing!\n\nIt’s not necessarily advantageous to have too many hypotheses. You want to answer one question well."
  },
  {
    "objectID": "slides/slides4.html#do-increases-in-option-grants-increase-risk-taking",
    "href": "slides/slides4.html#do-increases-in-option-grants-increase-risk-taking",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Do increases in option grants increase risk taking?",
    "text": "Do increases in option grants increase risk taking?\n\n\n\n\n\n\n\noptions\n\n  \n\nOption Grants\n\n Option Grants   \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants->Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances->Option Grants\n\n    \n\nAnnoyances->Risk Taking\n\n   \n\n\n\n\n\n\nExample of annoyances: Risk averse CEOs might take less risks and therefore receive more option grants."
  },
  {
    "objectID": "slides/slides4.html#iv-1-scheduled-discrete-increases-in-fixed-value-option-grants",
    "href": "slides/slides4.html#iv-1-scheduled-discrete-increases-in-fixed-value-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "IV 1: Scheduled Discrete Increases in Fixed-Value Option Grants",
    "text": "IV 1: Scheduled Discrete Increases in Fixed-Value Option Grants\n\n\n\n\n\n\n\noptions\n\n  \n\nPredicted New Grant Cycle\n\n Predicted New Grant Cycle   \n\nOption Grants\n\n Option Grants   \n\nPredicted New Grant Cycle->Option Grants\n\n    \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants->Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances->Option Grants\n\n    \n\nAnnoyances->Risk Taking\n\n   \n\n\n\n\n\n\nFor our first instrument, we use fixed-value firms, for which option grants can increase only at regularly prescheduled intervals (i.e., when new cycles start). For example, consider a fixed-value firm on regular three-year cycles. Other time-varying factors may drive trends in risk for this firm. However, these trends are unlikely to coincide exactly with the timing of when new cycles are scheduled to start.\n\n\nBasically saying the beginning of a cycle effect on option grants is not affected by the annoyances."
  },
  {
    "objectID": "slides/slides4.html#iv-2-within-cycle-grant-increases-due-to-industry-shocks-in-fixed-number-option-grants",
    "href": "slides/slides4.html#iv-2-within-cycle-grant-increases-due-to-industry-shocks-in-fixed-number-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "IV 2: Within Cycle Grant Increases due to Industry Shocks in Fixed-Number Option Grants",
    "text": "IV 2: Within Cycle Grant Increases due to Industry Shocks in Fixed-Number Option Grants\n\n\n\n\n\n\n\noptions\n\n  \n\nIndustry Shocks (Fixed Number)\n\n Industry Shocks (Fixed Number)   \n\nOption Grants\n\n Option Grants   \n\nIndustry Shocks (Fixed Number)->Option Grants\n\n    \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants->Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances->Option Grants\n\n    \n\nAnnoyances->Risk Taking\n\n   \n\n\n\n\n\n\nFor our second instrument, we focus on fixed-number firms. The value of options granted in any particular year varies with aggregate returns within a fixed-number cycle. This means that the timing of increases in option pay within a cycle will be random in the sense that the increases are driven in part by industry shocks that are beyond the control of the firm and are largely unpredictable. To account for the possibility that aggregate returns can directly affect risk, we use fixed-value firms as a control group because their option compensation must remain fixed despite changes in aggregate returns.\n\n\nThe identifying assumption is that fixed-number vs fixed-value might be a part of the annoyances. So might the industry shocks. However, the IV assumes that the industry shocks are not different except in how they effect the option grant value."
  },
  {
    "objectID": "slides/slides4.html#the-authors-know-their-setting",
    "href": "slides/slides4.html#the-authors-know-their-setting",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The authors know their setting!",
    "text": "The authors know their setting!\n\nOur identification strategy builds on Hall’s (1999)) observation that firms often award options according to multiyear plans. Two types of plans are commonly used: fixed-number and fixed-value. Under a fixed-number plan, an executive receives the same number of options each year within a cycle. Under a fixed-value plan, an executive receives the same value of options each year within a cycle.\n\n\n\nOur conversations with leading compensation consultants suggest that multiyear plans are used to minimize contracting costs, as option compensation only has to be set once every few years. Hall (1999, p. 97) argues that firms sort into the two types of plans somewhat arbitrarily, observing that “Boards seem to substitute one plan for another without much analysis or understanding of their differences.”\n\n\n\nRead qualitative studies and descriptions of actual practice!\nWe are looking at “slightly suboptimal” decision making to get variation."
  },
  {
    "objectID": "slides/slides4.html#key-assumption-1---relevance-iv-is-related-to-option-grants",
    "href": "slides/slides4.html#key-assumption-1---relevance-iv-is-related-to-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Key Assumption 1 - Relevance: IV is related to Option Grants",
    "text": "Key Assumption 1 - Relevance: IV is related to Option Grants\n\nWe find that the first-year indicator corresponds to a 15% larger increase in the Black-Scholes value of new option grants than in other years.\n\n\nAll estimates are highly significant, with F-statistics greatly exceeding 10, the rule of thumb threshold for concerns related to weak instruments (Staiger and Stock (1997). (III A.)\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#key-assumption-2---exclusion-or-validity-only-path-from-iv-to-risk-taking-is-through-option-grants.",
    "href": "slides/slides4.html#key-assumption-2---exclusion-or-validity-only-path-from-iv-to-risk-taking-is-through-option-grants.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Key Assumption 2 - Exclusion (or validity): Only path from IV to Risk Taking is through Option Grants.",
    "text": "Key Assumption 2 - Exclusion (or validity): Only path from IV to Risk Taking is through Option Grants.\n\nOne might be concerned that predicted first years provide exogenously timed but potentially anticipated increases in option compensation. However, this is not an issue for our empirical strategy. […] He would have no incentive to increase risk prior to an anticipated increase in the value of his option compensation next period.\n\n\nIn addition, we directly examine whether fixed-value cycles appear to be correlated with other firm cycles […]\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nThe key for the exclusion assumption is that anticipation would have an impact on the risk taking prior to the new cycle. This than would have an impact on the actual measure, i.e. the change in risk."
  },
  {
    "objectID": "slides/slides4.html#one-criticism",
    "href": "slides/slides4.html#one-criticism",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "One Criticism",
    "text": "One Criticism\n\nFirst, option compensation tends to follow an increasing step function for executives on fixed-value plans. This is because compensation tends to drift upward over time, yet executives on fixed-value plans cannot experience an upward drift within a cycle.\n\n\nWhile these two stylized facts do not hold in all cases—as can also be seen in Figure 1—our identification strategy only requires that they hold on average.\n\n\n\n\n\n\n\n\nSome more terminology\n\n\n\nCompliers\nAlways-takers/never-takers\nDefiers\n\n\n\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nThe LATE is identified for the compliers. IV assumes that there are no defiers because now our estimated effect becomes an average of the defiers and compliers. One solution is to just remove the defiers if you can (which they do in the paper as a robustness check)."
  },
  {
    "objectID": "slides/slides5.html#a-basic-before---after-comparison",
    "href": "slides/slides5.html#a-basic-before---after-comparison",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A basic before - after comparison",
    "text": "A basic before - after comparison\n\n\n\n\n\n\n\nevent\n\n  \n\nEvent Happened\n\n Event Happened   \n\nTreatment\n\n Treatment   \n\nEvent Happened->Treatment\n\n    \n\nOutcome\n\n Outcome   \n\nTreatment->Outcome\n\n    \n\nTime\n\n Time   \n\nTime->Event Happened\n\n    \n\nTime->Outcome\n\n   \n\n\n\n\n\nChapter 17 Event Studies in Huntington-Klein (2021)\n\nWhere Time could be standing in for a lot of other annoying things that might happen."
  },
  {
    "objectID": "slides/slides5.html#use-data-before-the-event-to-infer-the-counterfactual-outcome-in-yellow",
    "href": "slides/slides5.html#use-data-before-the-event-to-infer-the-counterfactual-outcome-in-yellow",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)",
    "text": "Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not observe the yellow/gold returns. We have to estimate them or convince the reader that there are no trends to be expected for theoretical/institutional reasons. I will not go into the details of the estimation here but I will in week 7."
  },
  {
    "objectID": "slides/slides5.html#front-running-information-leaking-and-anticipation-are-all-annoying.",
    "href": "slides/slides5.html#front-running-information-leaking-and-anticipation-are-all-annoying.",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Front-running, information leaking, and anticipation are all annoying.",
    "text": "Front-running, information leaking, and anticipation are all annoying.\n\n\n\n\n\n\n\nevent\n\n  \n\nBefore Block Trade\n\n Before Block Trade   \n\nLarge Block Trade\n\n Large Block Trade   \n\nBefore Block Trade->Large Block Trade\n\n    \n\nPrice Impact\n\n Price Impact   \n\nLarge Block Trade->Price Impact\n\n    \n\nTime\n\n Time   \n\nTime->Before Block Trade\n\n    \n\nTime->Price Impact\n\n   \n\n\n\n\n\n\n\n\nGo also back to Assignment 2 where we modeled the same problem when investors anticipate a donation.\n\n\n\nTo gauge demand from buyers and potentially gin up interest from sellers, bankers send out lists of shares with upcoming lockup expirations, according to market participants. (Money Stuff, Matt Levine)\nSometimes, bankers also engage in hypothetical conversations with buyers before they have a mandate. Asking prospective buyers whether they might be interested in certain stocks is one thing. But if there are indeed plans afoot for block sales, such conversations, even phrased hypothetically, can tip off savvy money managers. (Money Stuff, Matt Levine)"
  },
  {
    "objectID": "slides/slides5.html#what-if-we-had-an-additional-control-group-to-estimate-the-counterfactual",
    "href": "slides/slides5.html#what-if-we-had-an-additional-control-group-to-estimate-the-counterfactual",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "What if we had an additional control group to estimate the counterfactual?",
    "text": "What if we had an additional control group to estimate the counterfactual?"
  },
  {
    "objectID": "slides/slides5.html#a-simulated-cheap-talk-example-voluntary-disclosure-in-time-2",
    "href": "slides/slides5.html#a-simulated-cheap-talk-example-voluntary-disclosure-in-time-2",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2",
    "text": "A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2\n\nN <- 500\nT <- 2\ntime_effect <- c(3.5, 0)\nrd_did_firm <- tibble(\n  firm = 1:N,\n  performance = runif(N, 1, 10),\n  firm_effect = rnorm(N, 0, 2) + ifelse(performance < 3, 3, 0)\n)\nrd_did_panel <- tibble(\n  firm = rep(1:N, each = T),\n  time = rep(1:T, times = N)) %>%\n  left_join(rd_did_firm, by = \"firm\") %>%\n  mutate(\n    report = ifelse(time == 2, ifelse(performance > 3, 1, 0), 0),\n    noise = rnorm(N*T, 0, 3),\n    profit_report = 6.5 + time_effect[time] + firm_effect + noise,\n    profit_no_report = 1.5 + time_effect[time] + firm_effect + noise,\n    actual_profit = ifelse(report == 1, profit_report, profit_no_report))\n\n\nThe idea is that we have firms who perform well (performance > 3) and firms that perform bad (performance < 3). The firms that perform well will voluntarily disclose a report in time 2. We can see the effect as the difference between time 1 and time 2 for disclosers and non-disclosers.\nImportant: the cost of misreporting is not in calculated in the profit. The reasoning would be that this might be a litigation cost that would only emerge later on."
  },
  {
    "objectID": "slides/slides5.html#the-causal-effects-in-our-simulation",
    "href": "slides/slides5.html#the-causal-effects-in-our-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Causal Effects in Our Simulation",
    "text": "The Causal Effects in Our Simulation\n\nrd_did_panel %>%\n  mutate(causal_effect = profit_report - profit_no_report) %>%\n  group_by(time, report2 = performance > 3) %>%\n  summarise(profit_report = mean(profit_report),\n            profit_no_report = mean(profit_no_report),\n            causal_effect = mean(causal_effect)) %>%\n  kable(digits = 1)\n\n\n\n \n  \n    time \n    report2 \n    profit_report \n    profit_no_report \n    causal_effect \n  \n \n\n  \n    1 \n    FALSE \n    13.1 \n    8.1 \n    5 \n  \n  \n    1 \n    TRUE \n    10.2 \n    5.2 \n    5 \n  \n  \n    2 \n    FALSE \n    9.4 \n    4.4 \n    5 \n  \n  \n    2 \n    TRUE \n    6.6 \n    1.6 \n    5"
  },
  {
    "objectID": "slides/slides5.html#a-summary-of-the-actual-profits",
    "href": "slides/slides5.html#a-summary-of-the-actual-profits",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A Summary of The Actual Profits",
    "text": "A Summary of The Actual Profits\n\nrd_did_panel %>%\n  group_by(time, report2 = performance > 3) %>%\n  summarise(actual_profit = mean(actual_profit)) %>%\n  pivot_wider(names_from = time, values_from = actual_profit) %>%\n  kable(digits = 1)\n\n\n\n \n  \n    report2 \n    1 \n    2 \n  \n \n\n  \n    FALSE \n    8.1 \n    4.4 \n  \n  \n    TRUE \n    5.2 \n    6.6"
  },
  {
    "objectID": "slides/slides5.html#regressions",
    "href": "slides/slides5.html#regressions",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Regressions",
    "text": "Regressions\n\ndid_lm <- feols(actual_profit ~ report, data = rd_did_panel)\ndid_sub <- feols(actual_profit ~ report, data = filter(rd_did_panel, time == 2))\ndid_fixed <- feols(actual_profit ~ report | firm, data = rd_did_panel)\ndid_did <- feols(actual_profit ~ report | firm + time, data = rd_did_panel)\nmsummary(list(simple = did_lm, \"time 2\" = did_sub, \"firm FE\" = did_fixed, \"two-way FE\" = did_did),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n    simple \n     time 2 \n    firm FE \n    two-way FE \n  \n \n\n  \n    (Intercept) \n    5.580*** \n    4.380*** \n     \n     \n  \n  \n     \n    (0.144) \n    (0.308) \n     \n     \n  \n  \n    report \n    1.005*** \n    2.206*** \n    1.403*** \n    5.091*** \n  \n  \n     \n    (0.233) \n    (0.352) \n    (0.208) \n    (0.428) \n  \n  \n    Num.Obs. \n    1000 \n    500 \n    1000 \n    1000 \n  \n  \n    R2 \n    0.018 \n    0.073 \n    0.624 \n    0.685 \n  \n  \n    R2 Within \n     \n     \n    0.071 \n    0.222 \n  \n  \n    RMSE \n    3.58 \n    3.34 \n    2.22 \n    2.03 \n  \n  \n    Std.Errors \n    IID \n    IID \n    by: firm \n    by: firm \n  \n  \n    FE: firm \n     \n     \n    X \n    X \n  \n  \n    FE: time \n     \n     \n     \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides5.html#what-if-we-have-three-periods",
    "href": "slides/slides5.html#what-if-we-have-three-periods",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "What if we have three periods?",
    "text": "What if we have three periods?\n\n\n\n\n\n\nNote\n\n\nWe assume that over time investors and regulators get better at detecting when firms exaggerate in their report.\n\n\n\n\nTime 1: Reports are not believable, nobody reports\nTime 2: The biggest exaggerations will be caught, only well performing firms will report and communicate that they are doing excellent.\nTime 3: More subtle exaggerations will be caught. The worst performers will not report at all, the moderate performers will report and say that they will do well, the good performers will report that they are doing excellent.\n\n\n\nSee the Appendix of the assignment for the derivation of the exact parameters."
  },
  {
    "objectID": "slides/slides5.html#setup-of-three-period-simulation",
    "href": "slides/slides5.html#setup-of-three-period-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Setup of three period simulation",
    "text": "Setup of three period simulation\n\nN <- 1000\nT <- 3\ncutoff2 <- 3 # performance cutoff to report for time 1\ncutoff3 <- c(4/3, 4 + 2/3) # performance cutoff to report for time 2\nprofit1 <- 5\nprofit2 <- c(1.5, 6.5) #Profits for time 2 depending on report\nprofit3 <- c(2/3, 3, 7 + 1/3) #Profits for time 2 depending on report\nrd_did3_firm <- tibble(\n  firm = 1:N,\n  performance = runif(N, 0, 10),\n  firm_effect = rnorm(N, 0, 2) + ifelse(performance < cutoff2, 3, 0)\n)"
  },
  {
    "objectID": "slides/slides5.html#three-period-simulation",
    "href": "slides/slides5.html#three-period-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Three period simulation",
    "text": "Three period simulation\n\nrd_did3_panel <- tibble(\n  firm = rep(1:N, each = T),\n  time = rep(1:T, times = N)) %>%\n  left_join(rd_did3_firm, by = \"firm\") %>%\n  mutate(\n    # When will firms report?\n    report = case_when(\n      time == 1 ~ 0,\n      time == 2 & performance < cutoff2 ~ 0,\n      time == 3 & performance < cutoff3[1] ~ 0,\n      TRUE ~ 1),\n    noise = rnorm(T*N, 0, 5),\n    profit_no_report = firm_effect + noise +\n      case_when(\n        time == 1 ~ profit1,\n        time == 2 ~ profit2[1],\n        time == 3 ~ profit3[1]\n    ),\n    profit_report = firm_effect + noise +\n      case_when(\n        time == 1 ~ profit1,\n        time == 2 ~ profit2[2],\n        time == 3 & performance < cutoff3[2] ~ profit3[2],\n        TRUE ~ profit3[3]\n      ),\n    actual_profit = ifelse(report == 1, profit_report, profit_no_report)\n  )"
  },
  {
    "objectID": "slides/slides5.html#overview-of-4-groups-of-firms",
    "href": "slides/slides5.html#overview-of-4-groups-of-firms",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Overview of 4 groups of firms",
    "text": "Overview of 4 groups of firms\n\nNever reporters\nReporters in year 3\nReporters in year 2 and 3 (Medium)\nReporters in year 2 and 3 (High)\n\n\ncausal_effects <- rd_did3_panel %>%\n  mutate(causal_effect = profit_report - profit_no_report,\n         group = case_when(\n           performance < cutoff3[1] ~ 1,\n           performance < cutoff2 ~ 2,\n           performance < cutoff3[2] ~ 3,\n           TRUE ~ 4\n         )) %>%\n  group_by(time, group) %>%\n  summarise(report = mean(report),\n            N = n(),\n            M_report = mean(profit_report),\n            M_no_report = mean(profit_no_report),\n            M_causal_effect = mean(causal_effect))"
  },
  {
    "objectID": "slides/slides5.html#overview-of-4-groups-of-firms-1",
    "href": "slides/slides5.html#overview-of-4-groups-of-firms-1",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Overview of 4 groups of firms",
    "text": "Overview of 4 groups of firms\n\n\n\n\n \n  \n    time \n    group \n    report \n    N \n    M_report \n    M_no_report \n    M_causal_effect \n  \n \n\n  \n    1 \n    1 \n    0 \n    141 \n    7.4 \n    7.4 \n    0.0 \n  \n  \n    1 \n    2 \n    0 \n    159 \n    7.5 \n    7.5 \n    0.0 \n  \n  \n    1 \n    3 \n    0 \n    168 \n    4.9 \n    4.9 \n    0.0 \n  \n  \n    1 \n    4 \n    0 \n    532 \n    5.0 \n    5.0 \n    0.0 \n  \n  \n    2 \n    1 \n    0 \n    141 \n    9.3 \n    4.3 \n    5.0 \n  \n  \n    2 \n    2 \n    0 \n    159 \n    9.3 \n    4.3 \n    5.0 \n  \n  \n    2 \n    3 \n    1 \n    168 \n    6.7 \n    1.7 \n    5.0 \n  \n  \n    2 \n    4 \n    1 \n    532 \n    6.7 \n    1.7 \n    5.0 \n  \n  \n    3 \n    1 \n    0 \n    141 \n    4.7 \n    2.3 \n    2.3 \n  \n  \n    3 \n    2 \n    1 \n    159 \n    5.4 \n    3.1 \n    2.3 \n  \n  \n    3 \n    3 \n    1 \n    168 \n    2.9 \n    0.6 \n    2.3 \n  \n  \n    3 \n    4 \n    1 \n    532 \n    7.2 \n    0.5 \n    6.7"
  },
  {
    "objectID": "slides/slides5.html#two-way-fixed-effects",
    "href": "slides/slides5.html#two-way-fixed-effects",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Two-way Fixed Effects",
    "text": "Two-way Fixed Effects\n\ntwoway12 <- feols(actual_profit ~ report | firm + time,\n                  data = filter(rd_did3_panel, time != 3))\ntwoway13 <- feols(actual_profit ~ report | firm + time,\n                  data = filter(rd_did3_panel, time != 2))\ntwoway123 <- feols(actual_profit ~ report | firm + time,\n                  data = rd_did3_panel)"
  },
  {
    "objectID": "slides/slides5.html#separate-2-by-2-effects-are-larger-than-the-total-sample-effect",
    "href": "slides/slides5.html#separate-2-by-2-effects-are-larger-than-the-total-sample-effect",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Separate 2 by 2 effects are larger than the total sample effect",
    "text": "Separate 2 by 2 effects are larger than the total sample effect\n\nmsummary(list(\"time 1 and 2\" = twoway12, \"time 1 and 3\" = twoway13,\n              \"time 1, 2 and 3\" = twoway123), gof_omit = gof_omit,\n         stars = c(\"*\" = .1, \"**\" = .05, \"***\" = .01))\n\n\n\n \n  \n      \n     time 1 and 2 \n     time 1 and 3 \n     time 1, 2 and 3 \n  \n \n\n  \n    report \n    4.882*** \n    5.671*** \n    4.219*** \n  \n  \n     \n    (0.488) \n    (0.647) \n    (0.409) \n  \n  \n    Num.Obs. \n    2000 \n    2000 \n    3000 \n  \n  \n    R2 \n    0.578 \n    0.565 \n    0.437 \n  \n  \n    R2 Within \n    0.093 \n    0.066 \n    0.051 \n  \n  \n    RMSE \n    3.49 \n    3.71 \n    4.15 \n  \n  \n    Std.Errors \n    by: firm \n    by: firm \n    by: firm \n  \n  \n    FE: firm \n    X \n    X \n    X \n  \n  \n    FE: time \n    X \n    X \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01\n\n\n\n\n\nThis is not what I would expect. Why would the full sample lead to a smaller effect than all the subsamples?"
  },
  {
    "objectID": "slides/slides5.html#problem-statement",
    "href": "slides/slides5.html#problem-statement",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nFinally, when research settings combine staggered timing of treatment effects and treatment effect heterogeneity across firms or over time, staggered DiD estimates are likely to be biased. In fact, these estimates can produce the wrong sign altogether compared to the true average treatment effects."
  },
  {
    "objectID": "slides/slides5.html#solution",
    "href": "slides/slides5.html#solution",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Solution",
    "text": "Solution\n\nWhile the literature has not settled on a standard, the proposed solutions all deal with the biases arising from the “bad comparisons” problem inherent in TWFE DiD regressions by modifying the set of effective comparison units in the treatment effect estimation process. For example, each alternative estimator ensures that firms receiving treatment are not compared to those that previously received it.\n\n\nAgain, the solution to all our problems is to make sure that we make the right comparison."
  },
  {
    "objectID": "slides/slides5.html#simulation-setup---the-true-average-treatment-effect-of-three-groups",
    "href": "slides/slides5.html#simulation-setup---the-true-average-treatment-effect-of-three-groups",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Simulation Setup - The True Average Treatment Effect of Three Groups",
    "text": "Simulation Setup - The True Average Treatment Effect of Three Groups\n\n\nIt’s clear that the average treatment effect should be positive. It’s positive for every group."
  },
  {
    "objectID": "slides/slides5.html#the-estimated-effect-by-twoway-fixed-effects-of-500-simulations",
    "href": "slides/slides5.html#the-estimated-effect-by-twoway-fixed-effects-of-500-simulations",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Estimated Effect by Twoway Fixed Effects of 500 Simulations",
    "text": "The Estimated Effect by Twoway Fixed Effects of 500 Simulations"
  },
  {
    "objectID": "slides/slides5.html#the-sun2021-solution---restrict-the-sample",
    "href": "slides/slides5.html#the-sun2021-solution---restrict-the-sample",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Sun and Abraham (2021) Solution - Restrict The Sample",
    "text": "The Sun and Abraham (2021) Solution - Restrict The Sample"
  },
  {
    "objectID": "slides/slides5.html#the-estimated-effect-with-the-sun-and-abraham-solution",
    "href": "slides/slides5.html#the-estimated-effect-with-the-sun-and-abraham-solution",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Estimated Effect with the Sun and Abraham Solution",
    "text": "The Estimated Effect with the Sun and Abraham Solution"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham-in-practice",
    "href": "slides/slides5.html#sun-and-abraham-in-practice",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham in Practice",
    "text": "Sun and Abraham in Practice\n\nsa_new <- readRDS(here(\"data\", \"sa_new.RDS\"))\nsa_fe <- feols(roa ~ 1 + sunab(treatment_group, year) | firm + year,\n               cluster = \"state\", data = sa_new)\nsa_fe_att <- summary(sa_fe, agg = \"ATT\")\nsa_fe_group <- summary(sa_fe, agg = \"cohort\")\n\n\ntreatment_group: first year of treatment\nyear: calendar year\n\n\nnames(sa_fe$coefficients)\n\n [1] \"year::-18:cohort::1998\" \"year::-17:cohort::1998\" \"year::-16:cohort::1998\"\n [4] \"year::-15:cohort::1998\" \"year::-14:cohort::1998\" \"year::-13:cohort::1998\"\n [7] \"year::-12:cohort::1998\" \"year::-11:cohort::1998\" \"year::-10:cohort::1998\"\n[10] \"year::-9:cohort::1989\"  \"year::-9:cohort::1998\"  \"year::-8:cohort::1989\" \n[13] \"year::-8:cohort::1998\"  \"year::-7:cohort::1989\"  \"year::-7:cohort::1998\" \n[16] \"year::-6:cohort::1989\"  \"year::-6:cohort::1998\"  \"year::-5:cohort::1989\" \n[19] \"year::-5:cohort::1998\"  \"year::-4:cohort::1989\"  \"year::-4:cohort::1998\" \n[22] \"year::-3:cohort::1989\"  \"year::-3:cohort::1998\"  \"year::-2:cohort::1989\" \n[25] \"year::-2:cohort::1998\"  \"year::0:cohort::1989\"   \"year::0:cohort::1998\"  \n[28] \"year::1:cohort::1989\"   \"year::1:cohort::1998\"   \"year::2:cohort::1989\"  \n[31] \"year::2:cohort::1998\"   \"year::3:cohort::1989\"   \"year::3:cohort::1998\"  \n[34] \"year::4:cohort::1989\"   \"year::4:cohort::1998\"   \"year::5:cohort::1989\"  \n[37] \"year::5:cohort::1998\""
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---relative-year",
    "href": "slides/slides5.html#sun-and-abraham---relative-year",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Relative Year",
    "text": "Sun and Abraham - Relative Year\n\nmsummary(sa_fe, gof_omit = gof_omit, stars = stars, statistic = NULL,\n         estimate = \"{estimate} ({std.error}) {stars}\", coef_omit = \"-1\")\n\n\n\n \n  \n      \n     (1) \n  \n \n\n  \n    year = -9 \n    −0.003 (0.006) \n  \n  \n    year = -8 \n    0.001 (0.005) \n  \n  \n    year = -7 \n    −0.001 (0.006) \n  \n  \n    year = -6 \n    −0.002 (0.005) \n  \n  \n    year = -5 \n    0.005 (0.005) \n  \n  \n    year = -4 \n    0.003 (0.005) \n  \n  \n    year = -3 \n    0.004 (0.004) \n  \n  \n    year = -2 \n    0.010 (0.006) \n  \n  \n    year = 0 \n    0.011 (0.005) * \n  \n  \n    year = 1 \n    0.025 (0.006) *** \n  \n  \n    year = 2 \n    0.042 (0.006) *** \n  \n  \n    year = 3 \n    0.055 (0.005) *** \n  \n  \n    year = 4 \n    0.062 (0.005) *** \n  \n  \n    year = 5 \n    0.082 (0.006) *** \n  \n  \n    Num.Obs. \n    119996 \n  \n  \n    R2 \n    0.727 \n  \n  \n    R2 Within \n    0.005 \n  \n  \n    RMSE \n    0.17 \n  \n  \n    Std.Errors \n    by: state \n  \n  \n    FE: firm \n    X \n  \n  \n    FE: year \n    X"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---relative-year-1",
    "href": "slides/slides5.html#sun-and-abraham---relative-year-1",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Relative Year",
    "text": "Sun and Abraham - Relative Year\n\niplot(sa_fe)"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---att",
    "href": "slides/slides5.html#sun-and-abraham---att",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - ATT",
    "text": "Sun and Abraham - ATT\n\nmsummary(sa_fe_att, gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n     (1) \n  \n \n\n  \n    ATT \n    0.046*** \n  \n  \n     \n    (0.009) \n  \n  \n    Num.Obs. \n    119996 \n  \n  \n    R2 \n    0.727 \n  \n  \n    R2 Within \n    0.005 \n  \n  \n    RMSE \n    0.17 \n  \n  \n    Std.Errors \n    by: state \n  \n  \n    FE: firm \n    X \n  \n  \n    FE: year \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---cohort-effects",
    "href": "slides/slides5.html#sun-and-abraham---cohort-effects",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Cohort Effects",
    "text": "Sun and Abraham - Cohort Effects\n\nmsummary(sa_fe_group, gof_omit = gof_omit, stars = stars)\n\n\n\n \n  \n      \n     (1) \n  \n \n\n  \n    cohort = 1989 \n    0.058*** \n  \n  \n     \n    (0.006) \n  \n  \n    cohort = 1998 \n    0.034*** \n  \n  \n     \n    (0.005) \n  \n  \n    Num.Obs. \n    119996 \n  \n  \n    R2 \n    0.727 \n  \n  \n    R2 Within \n    0.005 \n  \n  \n    RMSE \n    0.17 \n  \n  \n    Std.Errors \n    by: state \n  \n  \n    FE: firm \n    X \n  \n  \n    FE: year \n    X \n  \n\n\n * p < 0.1, ** p < 0.05, *** p < 0.01"
  },
  {
    "objectID": "slides/slides5.html#take-away-lessons",
    "href": "slides/slides5.html#take-away-lessons",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Take-away Lessons",
    "text": "Take-away Lessons\n\n\n\n\n\n\nNote\n\n\n\nSimulations are good!\nEverything is a regression (Ok, not really)\nNot all the data should go in the regression"
  },
  {
    "objectID": "slides/slides5.html#abadie2017",
    "href": "slides/slides5.html#abadie2017",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Abadie et al. (2017)",
    "text": "Abadie et al. (2017)\n\n\n\n\n\n\nNote\n\n\nWhat is the level of the treatment variable? What is the comparison?\n\n\n\n\nMixed race or same-sex race\nState legislation\nCountry legislation\nFirm corporate governance changes"
  },
  {
    "objectID": "freaky_friday/index.html",
    "href": "freaky_friday/index.html",
    "title": "Research Design",
    "section": "",
    "text": "This website is an attempt to replicate the main results in Dellavigna and Pollet (2009) from scratch. The paper is a good example because (1) it has an explicit theoretical model, (2) provides excellent descriptions on how the different measures are constructed, (3) uses the canonical finance design, an event study. I will focus most of my attention on (2) and (3) but (1) is important because it provides guidance to readers of the paper why the measures and the design is important.\nThe basic argument of the paper is that firms will bury earnings announcements on Fridays if the earnings are bad because the market pays less attention to news on Fridays."
  },
  {
    "objectID": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "href": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "title": "Research Design",
    "section": "The Unexpected Component of Earnings",
    "text": "The Unexpected Component of Earnings\n\\[ s_{t,k} = \\frac{e_{t,k} - \\hat{e}_{t,k}}{P_{t,k}} \\]\nIn this equation, \\(s_{t,k}\\) is the surprise (i.e. the unexpected component) in earnings of company \\(k\\) at time \\(t\\). It is calculated by the actual earnings per share, \\(e_{t,k}\\), minus the median expected earnings by analysts, \\(\\hat{e}_{t,k}\\), divided by the price of the stock 5 days before the earnings release, \\(P_{t,k}\\). You will see over and over that empirical researchers are wary that the day(s) just before an announcement might be special, i.e. the news might have already leaked out, for good and less good reasons. So, instead of using the price the day before the earnings release, the paper picks a couple of days earlier 2.\nThe most important part is that we try to filter out all the information in the earnings announcement that is already known to the market by subtracting the earnings estimates of analysts. The implicit assumption is that these earnings estimates are a good measure of the market’s information on the company just before the earnings are announced."
  },
  {
    "objectID": "freaky_friday/index.html#the-market-reaction",
    "href": "freaky_friday/index.html#the-market-reaction",
    "title": "Research Design",
    "section": "The Market Reaction",
    "text": "The Market Reaction\nThe market reaction is calculated as the abnormal return from day \\(h\\) to day \\(H\\). 3\n\\[ R_{t,k}^{(h, H)} = [\\Pi_{j=h}^H (1 + R_{j,k})] - 1\n- \\hat{\\beta}_{t,k} [\\Pi_{j=h}^H (1 + R_{j,m}) - 1]\\]\nThis looks complicated but it is quite simple. The first part is the raw return of the stock over the period that we are interested in 4. The second part is the return of the market times the sensitivity of the stock to the market. The latter, \\(\\hat{\\beta}_{t,k}\\) is estimated with data from before the announcement. The goal of this approach is to filter out other reasons that the stock price might go up or down because of general economic or financial events that affect the stock market as a whole.\nSpecifically, we estimate the following regression model with data from days, \\(u\\), with \\(u\\) between 46 and 300 days before the earnings announcements. This is a regression of the market return on the firm return.\n\\[ R_{u,k} = \\alpha_{t,k} + \\beta_{t,k} R_{u,m}\\]\nAgain, we are using data from long before the earnings announcement so that our estimate is not contaminated by the earnings announcement 5.\nThere are lot of different approaches in the literature to estimate these abnormal returns but they all have the same flavour of trying to filter out other reasons why the stock price might be moving. In a pure regression framework, we would include additional variables as control variables. Constructing variables like the abnormal returns and the earnings surprise like this serves exactly the same function. There are good reasons to use the approach of first constructing the measures as precise as possible in an event study design like this but there are some problem with applying this same logic in different research designs Chen, Hribar, and Melessa (n.d.)."
  },
  {
    "objectID": "freaky_friday/download_linking.html",
    "href": "freaky_friday/download_linking.html",
    "title": "WRDS linking data",
    "section": "",
    "text": "I use three packages on this page and two of them require some more explanation. The here package helps with managing the different files in this larger project. I can refer to different files relative to the root folder all the files are in. The only thing that I need to do is to say where this file is compared to the root folder with the i_am function. The second package is the RPostgres package that helps make a connection with the WRDS data sources."
  },
  {
    "objectID": "freaky_friday/download_linking.html#ibes",
    "href": "freaky_friday/download_linking.html#ibes",
    "title": "WRDS linking data",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nThe SQL code instructs the WRDS data base to get the variables ticker, cusip (another identifier), cname (company name), and sdates (the start date for this ticker) from the ibes.idsum (IBES ID summary) database of WRDS. In this paper, we only want U.S. firms.\n\nSELECT ticker, cusip, cname, sdates\nFROM ibes.idsum\nWHERE usfirm = 1\n\nWith some R code, we clean the data and save it as a file in the data > wrds folder in our main folder.\n\nibes_id <- as_tibble(ibes_query) %>%\n  rename_all(tolower)\nsaveRDS(ibes_id, here(\"data\", \"wrds\", \"ibes_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#crsp",
    "href": "freaky_friday/download_linking.html#crsp",
    "title": "WRDS linking data",
    "section": "CRSP",
    "text": "CRSP\nFrom the CRSP data, we get the permno and ncusip identifier where ncusip stands for the same cusip identifier as mentioned above. We also have the company name, start date, and end date.\n\nSELECT permno, ncusip, comnam, st_date, end_date\nFROM crsp.stocknames\n\n\ncrsp_id <- as_tibble(crsp_query) %>%\n  rename_all(tolower)\nsaveRDS(crsp_id, here(\"data\", \"wrds\", \"crsp_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-ibes",
    "href": "freaky_friday/download_linking.html#compustat-with-ibes",
    "title": "WRDS linking data",
    "section": "Compustat with I/B/E/S",
    "text": "Compustat with I/B/E/S\nFrom Compustat we use the security file which has all the financial securities (and their identifiers) that are linked to the firms in Compustat. We select all the variables from that dataset. We only select the ones where the ibes ticker is available so that we can match via the ticker in the I/B/E/S files.\n\nSELECT *\nFROM comp.security\nWHERE ibtic IS NOT NULL\n\n\ncompu_security <- as_tibble(compu_security_query) %>%\n  rename_all(tolower)\nsaveRDS(compu_security, here(\"data\", \"wrds\", \"compu_security.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-crsp",
    "href": "freaky_friday/download_linking.html#compustat-with-crsp",
    "title": "WRDS linking data",
    "section": "Compustat with CRSP",
    "text": "Compustat with CRSP\nFinally, we get the linking file in compustat. According to the documentation, not all the links are reliable and they advice to use the linktype variable and the usedflag variable to filter only the links that are most reliable. I have implemented the rules that follow best practice according to this tutorial (https://wrds-www.wharton.upenn.edu/pages/wrds-research/applications/linking-databases/linking-crsp-and-compustat/)\n\nSELECT gvkey, linktype, usedflag, liid, lpermno, linkdt, linkenddt\nFROM crsp.Ccmxpf_linktable\n\n\ncrsp_compu <- as_tibble(compu_query) %>%\n  rename_all(tolower) %>%\n  select(gvkey, linktype, usedflag, iid = liid, permno = lpermno, stdt = linkdt, enddt = linkenddt) %>%\n  filter(!is.na(permno), linktype %in% c(\"LU\", \"LC\"), usedflag == 1) %>%\n  select(gvkey, permno, stdt, enddt) %>%\n  distinct()\nsaveRDS(crsp_compu, here(\"data\", \"wrds\", \"crsp_compu.RDS\"))"
  },
  {
    "objectID": "freaky_friday/linking.html",
    "href": "freaky_friday/linking.html",
    "title": "Combining databases",
    "section": "",
    "text": "The packages are the same as before."
  },
  {
    "objectID": "freaky_friday/linking.html#linking-ibes",
    "href": "freaky_friday/linking.html#linking-ibes",
    "title": "Combining databases",
    "section": "Linking I/B/E/S",
    "text": "Linking I/B/E/S\n\nlinking_table %>%\n  select(gvkey, ticker) %>%\n  distinct() %>%\n  summarise(N = n(), .by = ticker) %>%\n  filter(N > 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: ticker <chr>, N <int>\n\n\nThere are no tickers linked with multiple gvkeys. This means that left_join from I/B/E/S is the way to start the joining process. That way, there will be no duplicate matches from Compustat."
  },
  {
    "objectID": "freaky_friday/download_data.html",
    "href": "freaky_friday/download_data.html",
    "title": "Earnings announcements",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/download_data.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\nlibrary(dtplyr)\nlinking_table <- readRDS(here(\"data\", \"freaky_friday\", \"linking_table.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_data.html#ibes",
    "href": "freaky_friday/download_data.html#ibes",
    "title": "Earnings announcements",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nWe start with the earnings announcement data from I/B/E/S with the analyst estimates. According to the method section in Dellavigna and Pollet (2009), we need the data from the start of 1995 to the middle of 2006. We will want the analyst estimates for all the firms with a ticker in the master linking_table.\nI am going to use parameters that we can calculate or set in R and then pass them on to the SQL query. The details are explained in this blogpost by Irene Steves.\n\nbegin_date <- \"'1995-01-01'\"\nend_date <- \"'2006-07-01'\"\ntickers <- unique(linking_table$ticker)\ntickers_sql <- glue::glue_sql(\"{tickers*}\", .con = wrds)\n\nThe dates of the estimate and the actual earnings announcement will be critical to construct unexpected component of the earnings and to determine the exact event data, i.e. the date that (the unexpected component of) the earnings are announced. Thankfully, WRDS provides a description of the date variables. anndats is the first day that an analyst set their estimate for the earnings per share and the revdats is the last day that the analyst confirmed their estimate. We will use revdats as the defacto date that the analyst provided the estimate. anndats_act is the earnings announcement date. value is the estimated EPS by the analyst and actual is the actual EPS as announced by the firm. pdf is flag whether the EPS if for the primary share class or on a diluted basis. I included both and that is probably appropriate for this paper. fpi is the forecast period indicator if we set this to “6”, we get the earnings estimates that are done in the quarter before the earnings announcements. All these variables can be verified in the data descriptions on WRDS. As you can see, it’s quite important if you work with data that you have not collected yourself to read the data descriptions.\nIn the actual sql query you can see that the I use the parameters that I constructed before in R by adding an ? infront of the parameter name when using them.\n\nSELECT ticker, cusip, fpi, anndats, revdats, pdf, value, anndats_act, actual, analys\nFROM ibes.det_epsus\nWHERE anndats_act BETWEEN ?begin_date AND ?end_date\nAND actual IS NOT NULL\nAND fpi = '6'\nAND ticker IN (?tickers_sql)\n\nWe save the data with R. See the previous page for how to get the output of an sql query into R.\n\nann_ibes <- as_tibble(ibes_query) %>%\n  rename_all(tolower)\nsaveRDS(ann_ibes, here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\nglimpse(ann_ibes)\n\nRows: 1,046,677\nColumns: 10\n$ ticker      <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\"…\n$ cusip       <chr> \"0039241X\", \"0039241X\", \"0039241X\", \"0039241X\", \"0039241X\"…\n$ fpi         <chr> \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\"…\n$ anndats     <date> 2005-01-03, 2005-01-20, 2005-01-26, 2005-01-26, 2005-01-2…\n$ revdats     <date> 2005-01-10, 2005-01-26, 2005-01-27, 2005-01-27, 2005-04-1…\n$ pdf         <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\"…\n$ value       <dbl> -0.0654, -0.1139, -0.1000, -0.0900, -0.0900, -0.0323, -0.0…\n$ anndats_act <date> 2005-01-26, 2005-01-26, 2005-04-27, 2005-04-27, 2005-04-2…\n$ actual      <dbl> -0.1100, -0.1100, -0.1100, -0.1100, -0.1100, -0.1100, -0.1…\n$ analys      <dbl> 44775, 43594, 84303, 478, 87125, 43594, 5469, 44775, 43594…"
  },
  {
    "objectID": "freaky_friday/download_data.html#compustat",
    "href": "freaky_friday/download_data.html#compustat",
    "title": "Earnings announcements",
    "section": "Compustat",
    "text": "Compustat\nFollowing the paper, we will verify the earnings announcement date in I/B/E/S with the earnings announcement date in Compustat. Given the importance of finding the exact date for an event study, it is not surprising that Dellavigna and Pollet (2009) spent a lot of effort to make sure that they have the date right.\n\ngvkeys <- unique(linking_table$gvkey)\ngvkeys_sql <- glue::glue_sql(\"{gvkeys*}\", .con = wrds)\n\nrdq is the earnings announcement data in Compustat.\n\nSELECT cusip, rdq, gvkey\nFROM comp.fundq\nWHERE rdq BETWEEN ?begin_date AND ?end_date\nAND gvkey IN (?gvkeys_sql)\n\n\nann_compu <- as_tibble(compu_query) %>%\n  rename_all(tolower) %>%\n  mutate(cusip = str_sub(cusip, 1, 8))\nsaveRDS(ann_compu, here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nglimpse(ann_compu)\n\nRows: 327,964\nColumns: 3\n$ cusip <chr> \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"000…\n$ rdq   <date> 1995-03-15, 1995-07-06, 1995-09-13, 1995-12-12, 1996-03-14, 199…\n$ gvkey <chr> \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001…"
  },
  {
    "objectID": "freaky_friday/download_data.html#combine-announcements",
    "href": "freaky_friday/download_data.html#combine-announcements",
    "title": "Earnings announcements",
    "section": "Combine Announcements",
    "text": "Combine Announcements\nTo combine the two datasets, we will link them through a simplified version of the larger linking table. I will also enforce that the first 6 characters of cusip are the same. I don’t think it is strictly necessary to do that but it does gives us more confidence that the links are of higher quality. We need to match the I/B/E/S data and the Compustat data based on the firm and its earnings announcement date. However, if you read the paper (Dellavigna and Pollet 2009), you will notice that the reason why want to combine is because the date in both datasets does not always match. The paper gets around that by matching earnings announcements if the date is not more than 5 days apart in the two data sources. This is why I create anndat_begin and anndat_end to define the interval in which we want to match the data. Finally, we can calculate the actual event date as the minimum of the date in the I/B/E/S data and the Compustat data (Dellavigna and Pollet 2009) 1.\nYou can also see that I have two lines of commented code. In these lines, I read in the datasets again. This is not strictly necessary to make this file fully reproducible but it does make debugging the code easier. If I want to make some changes to the code I do not have to download the data again from WRDS. I can just use the one in the data folder.\n\n# ann_ibes <- readRDS(here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\n# ann_compu <- readRDS(here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nsimple_link <- linking_table %>%\n  select(ticker, gvkey, permno, cusip) %>%\n  mutate(cusip = str_sub(cusip, end = 6)) %>%\n  distinct()\nearn_ann <- ann_ibes %>%\n  distinct(ticker, actual, pdf, cusip, anndats_act) %>%\n  mutate(cusip = str_sub(cusip, end = 6)) %>%\n  left_join(simple_link, by = join_by(ticker)) %>%\n  filter(!is.na(gvkey), !(cusip.x != cusip.y)) %>%\n  select(-starts_with(\"cusip\")) %>%\n  mutate(anndat_begin = anndats_act - 5, anndat_end = anndats_act + 5) %>%\n  left_join(ann_compu, by = join_by(gvkey == gvkey,\n                                    anndat_begin <= rdq,  anndat_end >= rdq)) %>% \n  filter(!is.na(rdq)) %>%\n  mutate(anndat = pmin(anndats_act, rdq)) %>%\n  select(-anndat_begin, -anndat_end)\n\nWarning in left_join(., simple_link, by = join_by(ticker)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 68 of `x` matches multiple rows in `y`.\nℹ Row 34 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nsaveRDS(earn_ann, here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nglimpse(earn_ann)\n\nRows: 154,469\nColumns: 9\n$ ticker      <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H\"…\n$ actual      <dbl> -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.0…\n$ pdf         <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D\"…\n$ anndats_act <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-0…\n$ gvkey       <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081\"…\n$ permno      <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 10…\n$ cusip       <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410\"…\n$ rdq         <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-0…\n$ anndat      <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-0…\n\n\nearn_ann serves as a linking table to match different earnings announcements (as opposed to firms and their securities in linking_table)\n\n\n\nvariable\ndata source\ndescription\n\n\n\n\nticker\nI/B/E/S\nIdentifier\n\n\nanndats_act\nI/B/E/S\nActual Announcement Date\n\n\ngvkey\nCompustat\nIdentifier\n\n\npermno\nCRSP\nIdentifier\n\n\ncusip\n\nIdentifier of length 6,8 or 9\n\n\nrdq\nCompustat\nActual Announcement Date\n\n\nanndat\n\npmin(rdq, anndats_act)\n\n\n\nanndat is the validated way of calculating the announcement date. However, we need to keep the other dates around because we will need them to link back to the original databases.\nThe paper states that they have 154,051 earnings announcements (Dellavigna and Pollet 2009). We have 154469 earnings announcements."
  },
  {
    "objectID": "freaky_friday/download_stocks.html",
    "href": "freaky_friday/download_stocks.html",
    "title": "Stock price data",
    "section": "",
    "text": "On this page, we download the stock price data so that we can later calculate the abnormal return after the earnings announcements.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\ni_am(\"freaky_friday/download_stocks.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nearn_ann <- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\n\n\nwrds <- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  user='stimas',\n                  sslmode='require')\n\nThis section sets the sql parameters. The beginning date is 300 days before the first earnings announcement and the end date is 75 days after the last earnings announcement. Finally, I keep the permno identifiers because these are the only stocks we want the data from.\n\ncrsp_input <- earn_ann %>%\n  summarise(begin = min(anndat) - 300, end = max(anndat) + 75, .by = permno) %>%\n  glimpse()\n\nRows: 8,755\nColumns: 3\n$ permno <dbl> 10560, 88784, 10574, 80585, 84606, 88836, 88790, 87771, 82726, …\n$ begin  <date> 2003-12-26, 2001-01-17, 2001-07-04, 2003-05-06, 2001-07-03, 20…\n$ end    <date> 2006-07-10, 2006-07-18, 2006-07-24, 2006-04-11, 2006-07-09, 20…\n\nbegin_date <- min(crsp_input$begin)\nend_date <- max(crsp_input$end)\npermno_sql <- glue::glue_sql(\"{crsp_input$permno*}\", .con = wrds)\n\nI use the same syntax as before to call the WRDS databases as before with sql interspersed with the R parameters created in the previous code block. We get the daily volume, return, price, shares outstanding, cumulative factor to adjust price, and cumulative factor to adjust shares. The latter two are adjustment factors for stock splits and dividends which we probably will not need but if we do we have them.\nThis is by far the largest download from WRDS and this is why it has it’s own page. We do not want to rerun this more than strictly necessary.\n\nSELECT permno, date, vol, ret, prc, shrout, cfacpr, cfacshr\nFROM crsp_a_stock.dsf\nWHERE permno IN (?permno_sql)\nAND date BETWEEN ?begin_date AND ?end_date\n\n\nall_stocks <- as_tibble(crsp_query) %>%\n  rename_all(tolower)\nprint(all_stocks)\n\n# A tibble: 14,967,830 × 8\n   permno date         vol      ret   prc shrout cfacpr cfacshr\n    <dbl> <date>     <dbl>    <dbl> <dbl>  <dbl>  <dbl>   <dbl>\n 1  10002 1994-03-10   700 -0.00952  13     2999    1.5     1.5\n 2  10002 1994-03-11   200  0.0577   13.8   2999    1.5     1.5\n 3  10002 1994-03-14     0 -0.0455  -13.1   2999    1.5     1.5\n 4  10002 1994-03-15     0  0       -13.1   2999    1.5     1.5\n 5  10002 1994-03-16  1700  0.00952  13.2   2999    1.5     1.5\n 6  10002 1994-03-17     0 -0.00943 -13.1   2999    1.5     1.5\n 7  10002 1994-03-18     0  0       -13.1   2999    1.5     1.5\n 8  10002 1994-03-21     0  0.00457 -13.1   2999    1.5     1.5\n 9  10002 1994-03-22  2000  0.0190   13.4   2999    1.5     1.5\n10  10002 1994-03-23     0 -0.00935 -13.2   2999    1.5     1.5\n# ℹ 14,967,820 more rows\n\nsaveRDS(all_stocks, here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\n\nOne important footnote is that the price is negative on days where there were no trades. This might be important going forward."
  },
  {
    "objectID": "freaky_friday/abnormal_returns.html",
    "href": "freaky_friday/abnormal_returns.html",
    "title": "Abnormal returns",
    "section": "",
    "text": "Setup\nThe lubridate package is the tidyverse package that helps with time related data. Dates are a specific class of variables and the package helps with managing date variables.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/abnormal_returns.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nIn order to predict the expected market return reaction to earnings, we will want to take into account the general market reaction. Dellavigna and Pollet (2009) only adjust for the overall market return. There are other adjustments possible for specific risk factors. The original factor model is the Fama-French 3 Factors model and the data is available via the Kenneth French data library.\nI have downloaded the data as a .csv file. The code reads the data in skipping 5 lines and reading the variables as numbers (double precision). We then need to transform the date variable to a date type with a function from the lubridate package. Finally, we need to scale the returns by 100 because they are expressed in percentages. For replicating the Dellavigna and Pollet (2009) paper, we only need the market return minus the risk free rate (mkt_rf) and the risk free rate (rf). If I read this paper correctly, we need to use the raw market return which is calculated as mkt.\n\nearn_ann <- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nanalyst <- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\"))\nall_stocks <- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench <- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %>%\n  mutate(date = ymd(date)) %>%\n  mutate_if(is.numeric, ~ . / 100) %>%\n  mutate(mkt = mkt_rf + rf) %>% \n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   <date>       <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\nshrout is shares outstanding in 1000\nmarket_value is in million USD\n\n\n\nAbnormal returns\nRemember that we are interested in predicting the counterfactual of what the returns would have been if there were no earnings announcement. We use the following prediction model:\n\\[ R_{u,k} = \\alpha_{t,k} + \\beta_{t,k} R_{u,m}\\]\nThat means that we need to estimate \\(\\alpha\\) and \\(\\beta\\) for each announcement based on the market return and firm return from 300 days before the earnings announcement to 46 days before the earnings announcements (Dellavigna and Pollet 2009). This is a lot of computations!\nI played around with a lot of different implementations and finally settled on a completely tidyverse style implementation as the fastest on my machine. I did not include the other implementations in this document but I will show how you can test the timing of your code with the microbenchmark package.\nThe first thing we need to do is to write the code as a function that we can call as many times as we need. 1 I call this function create_coefs. The input of the function is n, the number of earnings announcements we want to run the code for. The output is a tibble with the coefficients \\(\\alpha\\) and \\(\\beta\\) for each permno and anndat combination.\nThe actual function takes the earnings announcement data and keeps the unique permno and anndat combinations. Next, the start and end data for the return data is calculated based on the earnings announcement date. Next, we merge the return data and the Fama-French data and delete the missing or infinite observations for the return.\n\n\n\n\n\n\nNote\n\n\n\nAt this point, you can see the flexibility of my approach to keep the data sets separate until we need to join them. The earnings announcement data is one row per earnings announcement date per firm. The stock data has one observation per firm per day. The Fama-French data has one observation per day. Because these data have a different level of analysis, I have kept them separate until the point that we need to analyse the data. If I have to update one of the datasets, I do not necessarily have to update all the other ones. I would strongly encourage you to do the same thing where you keep different parts of the data cleaning separate for as long as possible. This will make your code flexible and easy to adapt.\n\n\nThe next two steps are the most advanced ones. First, remark that we have multiple rows for each earnings announcement. We use the summarise function to put all the returns in a vector y and we make a matrix X with all 1’s in the first column and the mkt. Remark that we are wrapping y and X in a list so that we effectively make y and X list columns in our tibble. The tidyverse lets us put almost anything in a dataset as long as we wrap it in a list. Finally, we summarise the y and X not just for the whole dataset but for each combination of permno and anndat. Thus, we end up with a dataset with a row for each earnings announcement with the returns we want to predict (y) from before the earnings announcement and the market returns from the same time period (X).\nIn the second step, we will run the regression for each row to estimate \\(\\alpha\\) and \\(\\beta\\). We do use a lightweight version of lm called lm.fit to speed up the estimation. lm.fit calculates less statistics like the standard errors that we need to calculate the p-values. Because ultimately, we need to this for 150,000 rows lm.fit will save considerable time compared to lm. lm.fit works slightly different in that now regression equation is specified. You need to give the predictors as the first argument and the outcome variable as the second argument. This is the reason why we formed the y and X variable in the previous step. We just need the coefficients form the fit object and we can use the coef function to extract them.\nFinally, we want to apply this lm.fit function to every row (i.e. for every earnings announcement). We use the map-family to apply a function to every element of a column. The pmap function specifically let us apply a function to multiple columns if we wrap them in a list. So, pmap takes the list of columns X and y and applies (~) the function lm.fit to the first (..1) and second (..2) element of that list. At the end, we extract the coefficients.\nThe microbenchmark function allows us to test the time it takes to run this function for 100, 1000, and 10000 earnings announcements. The computations are done 10 times for each call to get an average/median estimate 2. The surprising thing to me was that the function scales very well. The average time per announcement goes down with more announcements.\n\ncreate_coefs <- function(n = 6){\n  earn_ann %>% head(n = n) %>%\n    distinct(permno, anndat) %>%\n    mutate(start = anndat - 300, end = anndat - 46) %>%\n    left_join(select(all_stocks, permno, date, ret),\n              by = join_by(permno == permno, start <= date, end >= date)) %>%\n    left_join(select(famafrench, mkt, rf, date),\n              by = join_by(date == date)) %>%\n    filter(!is.na(ret), !is.infinite(ret)) %>%\n    summarise(y = list(cbind(ret)), X = list(cbind(alpha = 1, beta = mkt)),\n               .by = c(permno, anndat)) %>%\n    mutate(coefs = pmap(list(X, y), ~ lm.fit(..1, ..2) %>% coef()),\n           .by = c(permno, anndat)) %>%\n    select(-y, -X)\n}\n\nmicrobenchmark::microbenchmark(\n                  create_coefs(100),\n                  create_coefs(1000),\n                  create_coefs(10000),\n                  times = 10)\n\nUnit: seconds\n                expr      min       lq     mean   median       uq      max\n   create_coefs(100) 1.471252 1.533914 2.127709 1.638708 1.756135 6.682866\n  create_coefs(1000) 1.730352 1.834642 1.882995 1.862035 1.883467 2.143123\n create_coefs(10000) 3.171283 3.265883 3.427505 3.370048 3.443241 4.122894\n neval\n    10\n    10\n    10\n\n\nNext, we calculate the coefficients for all announcements and save them in the results object. Next, we need to calculate the abnormal returns based on the following formula in Dellavigna and Pollet (2009).\n\\[ R_{t,k}^{(h, H)} = [\\Pi_{j=h}^H (1 + R_{j,k})] - 1\n- \\hat{\\beta}_{t,k} [\\Pi_{j=h}^H (1 + R_{j,m}) - 1]\\]\nIn the results, the coef column is a list column where each row has two elements: alpha and beta. The unnest_wider function splits the list columns in two columns alpha and beta. Next we create the date 75 days after the announcement because that is the longest time frame the paper is interested (Dellavigna and Pollet 2009). Just like before, we add the relevant stock price data and Fama-French data. Our dataset now has a row for each day 0 to 75 days after the announcement date. Dellavigna and Pollet (2009) are interested in the short term abnormal return on the day of the announcement and the day after (0-1 days) and the long term abnormal return (2-75 days). I create the time_frame variable to denote whether a day should be counted in the short or long cumulative abnormal return. As an intermediate step, I calculate the products of the (market) returns + 1 (see the equation) by announcement and time frame (and beta because we need it for further calculations).\nFinally, I use the pivot_wider function to transform the dataset from the long format to a wider format. Specifically, each announcement has two rows one for the short term car and one for the long term car. The transformation will create a new dataset where every row has a different announcement with a car_long and a car_short. We do that by taking the values_from the car variable and the name for the column from the time_frame variable.\n\nN <- nrow(earn_ann)\nresults <- create_coefs()\nresults <- create_coefs(N)\n\nabnormal <- results %>%\n  unnest_wider(coefs) %>%\n  mutate(date75 = anndat + 75) %>%\n  left_join(select(all_stocks, permno, date, ret),\n            by = join_by(permno == permno,\n                         date75 >= date, anndat <= date)) %>%\n  left_join(select(famafrench, mkt, rf, date),\n            by = join_by(date == date)) %>%\n  mutate(time_frame = if_else(date - anndat <= 1, \"short\", \"long\")) %>%\n  summarise(raw = prod(1 + ret), mkt = prod(1 + mkt),\n            .by = c(permno, anndat, time_frame, beta)) %>% \n  mutate(car = raw - 1 - beta * (mkt - 1)) %>%\n  select(-beta, -raw, -mkt) %>%\n  filter(!is.na(car)) %>%\n  pivot_wider(values_from = car, names_from = time_frame,\n              names_prefix = \"car_\")\nglimpse(abnormal)\n\nRows: 128,066\nColumns: 4\n$ permno    <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1057…\n$ anndat    <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-01,…\n$ car_short <dbl> 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0.01…\n$ car_long  <dbl> -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569961…\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pivot_wider and pivot_longer functions are really, really useful when you are working with poorly structured data. As a rule of thumb when you clean and summarise the data, you will want the data to be in a long format where each row is its own distinct unit of analysis (Wickham 2014). For instance, in this code we had a row for each return day after the announcement for each announcement. The big advantage is that we can easily, with code, change the groups of data that we want to summarise, mutate, or filter. If we want to change what the short time frame means, we can just change that one line of code (e.g. to data - anndat <= 5). The wider format is more useful to present the data. That is why I use it more often at the end of an analysis.\n\n\n\n\nPutting it all together\nFinally, we want to put all the data together. You can see that we only merge all the data at the end. This is good practice. I would advise to not even try to build up a complete dataset by starting with for instance the I/B/E/S data and than to gradually add the variables that you need. This is a very error prone process and it is hard to make changes without redoing the whole data cleaning process.\nFirst, we need more stock price data and the market value from the CRSP data. We collect this in the clean_prices dataset. Remark that I remove the data where the price is negative because that means that there were no trades that day. Dellavigna and Pollet (2009) require the price and market value 5 days before the earnings announcement. My guess is that they are trying to avoid any effects of information leaking out into the market before the earnings announcement. I then merge the earnings announcement data with the analyst estimates data, the abnormal return data and the price and market value data. Finally, I calculate the earnings surprise as the difference between the actual and the median predicted earnings per share divided by the price per share to get the earnings surprise per dollar of market value.\n\nclean_prices <- all_stocks %>%\n  filter(prc > 0) %>%\n  select(permno, date, prc, shrout) %>%\n  mutate(market_value = prc * shrout) %>%\n  select(-shrout) %>%\n  print()\n\n# A tibble: 14,333,348 × 4\n   permno date         prc market_value\n    <dbl> <date>     <dbl>        <dbl>\n 1  10002 1994-03-10  13         38987 \n 2  10002 1994-03-11  13.8       41236.\n 3  10002 1994-03-16  13.2       39737.\n 4  10002 1994-03-22  13.4       40112.\n 5  10002 1994-03-24  13.1       39362.\n 6  10002 1994-04-07  12.5       37488.\n 7  10002 1994-04-15  14         41986 \n 8  10002 1994-04-18  14         41986 \n 9  10002 1994-04-22  12.5       37488.\n10  10002 1994-04-28  14         41986 \n# ℹ 14,333,338 more rows\n\nsurprise <- earn_ann %>%\n  left_join(analyst,\n            by = join_by(ticker, anndat, actual, pdf)) %>%\n  left_join(abnormal,\n            by = join_by(permno, anndat)) %>%\n  mutate(date_minus5 = anndat - 5) %>%\n  left_join(clean_prices,\n            by = join_by(permno, closest(date_minus5 >= date))) %>%\n  mutate(surprise = (actual - median) / prc) \nglimpse(surprise)\n\nRows: 154,469\nColumns: 20\n$ ticker       <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       <dbl> -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            <int> 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, NA, 5, 3, 2, 1, 4,…\n$ median       <dbl> -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         <dbl> -0.08965, -0.06800, -0.03700, -0.10055, -0.07658, -0.0900…\n$ mean_days    <dbl> 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    <dbl> 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     <dbl> -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  <date> 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         <date> 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          <dbl> 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value <dbl> 2592630.69, 1993992.84, 2200875.00, 1434970.50, 1769503.4…\n$ surprise     <dbl> -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n\n\n\n\nData Cleaning\nFinally, I apply the data cleaning rules from Dellavigna and Pollet (2009). I also include the a print statement so that we can track how many observations are removed after each data cleaning step. The following rules are applied:\n\nRemove the observations where surprise is missing.\nRemove the observations where the median or actual estimate is larger than the price.\nRemove penny stocks which I arbitrarily and after some googling decide to be stocks with a price lower than $2 per stock.\nRemove the observations with earnings announcement on Saturday and Sunday.\nI remove the observations with a car in the top and bottom .05% of observations.\n\n\nwinsorise <- 5/10000\nmain <- surprise %>%\n  filter(!is.na(surprise)) %>% print(n = 0) %>%\n  filter(abs(median) < prc, abs(actual) < prc) %>% print(n = 0) %>%\n  filter(prc > 2) %>% print(n = 0) %>%\n  mutate(weekday = wday(anndat, label = TRUE)) %>%\n  filter(! weekday %in% c(\"Sat\", \"Sun\")) %>% print(n = 0) %>%\n  filter(percent_rank(car_long) >= winsorise,\n         percent_rank(car_long) <= 1 - winsorise,\n         percent_rank(car_short) >= winsorise,\n         percent_rank(car_short) <= 1 - winsorise) %>%\n  print()\n\n# A tibble: 137,333 × 20\n# ℹ 137,333 more rows\n# ℹ 20 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>\n# A tibble: 135,959 × 20\n# ℹ 135,959 more rows\n# ℹ 20 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>\n# A tibble: 133,633 × 20\n# ℹ 133,633 more rows\n# ℹ 20 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>\n# A tibble: 133,541 × 21\n# ℹ 133,541 more rows\n# ℹ 21 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>, weekday <ord>\n# A tibble: 130,759 × 21\n   ticker actual pdf   anndats_act gvkey  permno cusip    rdq        anndat    \n   <chr>   <dbl> <chr> <date>      <chr>   <dbl> <chr>    <date>     <date>    \n 1 A2      -0.11 D     2005-01-26  001081  10560 00392410 2005-01-26 2005-01-26\n 2 A2      -0.11 D     2005-04-27  001081  10560 00392410 2005-04-27 2005-04-27\n 3 A2      -0.05 D     2005-07-27  001081  10560 00392410 2005-07-27 2005-07-27\n 4 A2      -0.07 D     2005-10-26  001081  10560 00392410 2005-10-26 2005-10-26\n 5 A2      -0.1  D     2006-02-01  001081  10560 00392410 2006-02-01 2006-02-01\n 6 A2      -0.04 D     2004-10-21  001081  10560 00392410 2004-10-21 2004-10-21\n 7 AA0G    -0.45 P     2001-11-13  133724  88784 00724X10 2001-11-13 2001-11-13\n 8 AA0H     0.01 D     2002-04-30  014285  10574 08265710 2002-04-30 2002-04-30\n 9 AA0H     0.03 D     2002-08-06  014285  10574 08265710 2002-08-06 2002-08-06\n10 AA0H     0.01 D     2002-10-30  014285  10574 08265710 2002-10-30 2002-10-30\n# ℹ 130,749 more rows\n# ℹ 12 more variables: N <int>, median <dbl>, mean <dbl>, mean_days <dbl>,\n#   car_short <dbl>, car_long <dbl>, date_minus5 <date>, date <date>,\n#   prc <dbl>, market_value <dbl>, surprise <dbl>, weekday <ord>\n\nsaveRDS(main, here(\"data\", \"freaky_friday\", \"main.RDS\"))\n\n\n\nOverview of the datasets\n\n\n\nFile\nDescription\n\n\n\n\nearn_ann.RDS\nThe information on the earnings announcements\n\n\nanalyst.RDS\nThe analyst estimates data\n\n\nmain.RDS\nThe main dataset to replicate the paper\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\nFootnotes\n\n\nR is fundamentally a programming language that is build around functions. The tidyverse partially works around that by making tibbles the primary object. Nevertheless, when you need to do some more advanced programming, creating functions is quite natural in R.↩︎\nFor comparison on my 2020 Mac Mini, it takes about 1.5 seconds for 100 announcements and 4.0 seconds for 10000 announcements. On my 2014 Macbook pro, that is respectively 4.3 seconds and 11.2 seconds.↩︎"
  },
  {
    "objectID": "freaky_friday/descriptive.html",
    "href": "freaky_friday/descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"freaky_friday/descriptive.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nmain <- readRDS(here(\"data\", \"freaky_friday\", \"main.RDS\")) %>%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\nglimpse(main)\n\nRows: 130,759\nColumns: 23\n$ ticker       <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       <dbl> -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            <int> 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 4, 2, …\n$ median       <dbl> -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         <dbl> -0.0896500, -0.0680000, -0.0370000, -0.1005500, -0.076580…\n$ mean_days    <dbl> 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    <dbl> 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     <dbl> -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  <date> 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         <date> 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          <dbl> 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value <dbl> 2592630.7, 1993992.8, 2200875.0, 1434970.5, 1769503.5, 24…\n$ surprise     <dbl> -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n$ weekday      <ord> Wed, Wed, Wed, Wed, Wed, Thu, Tue, Tue, Tue, Wed, Tue, We…\n$ group        <chr> \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         <dbl> 2005, 2005, 2005, 2005, 2006, 2004, 2001, 2002, 2002, 200…"
  },
  {
    "objectID": "freaky_friday/descriptive.html#quantiles",
    "href": "freaky_friday/descriptive.html#quantiles",
    "title": "Descriptive statistics",
    "section": "Quantiles",
    "text": "Quantiles\n\nquantiles <- main %>%\n  mutate(sign = case_when(surprise > 0 ~ \"positive\",\n                          surprise < 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %>%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %>%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %>%\n  glimpse()\n\nRows: 130,759\nColumns: 26\n$ ticker       <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       <dbl> -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       <date> 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            <int> 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 4, 2, …\n$ median       <dbl> -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         <dbl> -0.0896500, -0.0680000, -0.0370000, -0.1005500, -0.076580…\n$ mean_days    <dbl> 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    <dbl> 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     <dbl> -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  <date> 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         <date> 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          <dbl> 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value <dbl> 2592630.7, 1993992.8, 2200875.0, 1434970.5, 1769503.5, 24…\n$ surprise     <dbl> -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n$ weekday      <ord> Wed, Wed, Wed, Wed, Wed, Thu, Tue, Tue, Tue, Wed, Tue, We…\n$ group        <chr> \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         <dbl> 2005, 2005, 2005, 2005, 2006, 2004, 2001, 2002, 2002, 200…\n$ sign         <chr> \"negative\", \"negative\", \"negative\", \"positive\", \"negative…\n$ quintile     <int> 2, 1, 3, 5, 2, 5, 2, 4, 4, 4, 1, 3, 3, 2, 1, 2, 4, 3, 1, …\n$ quantile     <dbl> 2, 1, 3, 11, 2, 11, 2, 10, 10, 4, 6, 9, 9, 2, 6, 8, 4, 9,…\n\n\nThis is a quick version of Figure 1a. It can be further cleaned up with a better axis labels. It shows the main results from Dellavigna and Pollet (2009) that the market reaction is subdued on Fridays.\n\nggplot(quantiles,\n       aes(y = car_short, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun.y = mean, geom = \"line\") +\n  scale_color_grey()\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nThis is how I would program Figure 1a and b together. It’s a good example of how using pivot_longer can make your life easier. In this case, if we need to plot multiple similar variables.\n\nquantiles %>%\n  pivot_longer(c(car_short, car_long), values_to = \"car\", names_to = \"window\") %>%\n  mutate(fig_name = if_else(window == \"car_short\", \"Figure 1a\", \"Figure 1b\")) %>%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ fig_name, nrow = 2)"
  },
  {
    "objectID": "freaky_friday/regressions.html",
    "href": "freaky_friday/regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(modelsummary)\ngof_omit <- \"Adj|RMS|IC\"\ni_am(\"freaky_friday/regressions.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-2",
    "href": "freaky_friday/regressions.html#table-2",
    "title": "Regressions",
    "section": "Table 2",
    "text": "Table 2\nThe tables do not really replicate which is interesting to me. For a number of reasons.\n\nThe results are more consistent. I wonder whether I got rid of more outliers earlier. Remember I did end up with less observations. One interpretation is that I have cleaned the data better, the other is that I got rid of important, influential observations by being too strict when cleaning the data.\nThe results for the short term CAR are consistent with the figure. Friday market reactions to bottom quantile surprises are more positive than non-friday market reactions and the sign flips for top quantile surprises.\nI also lose substantially more observations due to the inclusion of the volatility measures. I do not know exactly why that is the case.\n\n\nPanel A: Short Term CAR\n\nsubset <- readRDS(here(\"data\", \"freaky_friday\", \"subset.RDS\"))\n#| label: table2a\nmodel1a <- feols(car_short ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2a <- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3a <- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,839 observations removed because of NA values (Fixed-effects: 6,839).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1a, model2a, model3a), gof_omit = gof_omit, stars = TRUE)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    −0.036*** \n     \n     \n  \n  \n     \n    (0.001) \n     \n     \n  \n  \n    friday \n    0.014*** \n    0.012*** \n    0.013** \n  \n  \n     \n    (0.003) \n    (0.003) \n    (0.004) \n  \n  \n    top \n    0.061*** \n     \n     \n  \n  \n     \n    (0.002) \n     \n     \n  \n  \n    friday × top \n    −0.023*** \n    −0.020*** \n    −0.021*** \n  \n  \n     \n    (0.004) \n    (0.004) \n    (0.005) \n  \n  \n    Num.Obs. \n    22486 \n    22486 \n    15647 \n  \n  \n    R2 \n    0.086 \n    0.095 \n    0.110 \n  \n  \n    R2 Within \n     \n    0.001 \n    0.001 \n  \n  \n    Std.Errors \n    by: anndat \n    by: anndat \n    by: anndat \n  \n  \n    FE: size_decile \n     \n    X \n    X \n  \n  \n    FE: year \n     \n    X \n    X \n  \n  \n    FE: month \n     \n    X \n    X \n  \n  \n    FE: vol_decile \n     \n     \n    X \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n\nPanel B: Long Term CAR\n\nmodel1b <- feols(car_long ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2b <- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3b <- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,839 observations removed because of NA values (Fixed-effects: 6,839).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1b, model2b, model3b), gof_omit = gof_omit, stars = TRUE)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    −0.022*** \n     \n     \n  \n  \n     \n    (0.005) \n     \n     \n  \n  \n    friday \n    −0.012 \n    −0.012 \n    −0.022 \n  \n  \n     \n    (0.013) \n    (0.013) \n    (0.015) \n  \n  \n    top \n    0.037*** \n     \n     \n  \n  \n     \n    (0.004) \n     \n     \n  \n  \n    friday × top \n    0.041** \n    0.043** \n    0.052** \n  \n  \n     \n    (0.015) \n    (0.014) \n    (0.017) \n  \n  \n    Num.Obs. \n    22486 \n    22486 \n    15647 \n  \n  \n    R2 \n    0.006 \n    0.035 \n    0.041 \n  \n  \n    R2 Within \n     \n    0.001 \n    0.001 \n  \n  \n    Std.Errors \n    by: anndat \n    by: anndat \n    by: anndat \n  \n  \n    FE: size_decile \n     \n    X \n    X \n  \n  \n    FE: year \n     \n    X \n    X \n  \n  \n    FE: month \n     \n    X \n    X \n  \n  \n    FE: vol_decile \n     \n     \n    X \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-3",
    "href": "freaky_friday/regressions.html#table-3",
    "title": "Regressions",
    "section": "Table 3",
    "text": "Table 3\n\nmain_extra <- main %>%\n  mutate(log_size = log(market_value))  %>%\n  mutate(log_size_adj = log_size - mean(log_size, na.rm = T),\n         .by = c(quarter, year)) %>%\n  mutate(size_decile = ntile(log_size_adj, 10))\n\nmodel1 <- feols(car_short ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel2 <- feols(car_short ~ friday + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\nmodel3 <- feols(car_long ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel4 <- feols(car_long ~ friday  + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\n\nmsummary(list(model1, model2, model3, model4), gof_omit = gof_omit, stars = TRUE)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n      (4) \n  \n \n\n  \n    (Intercept) \n    −0.041*** \n     \n    −0.015*** \n     \n  \n  \n     \n    (0.001) \n     \n    (0.002) \n     \n  \n  \n    friday \n    0.016*** \n    0.014*** \n    −0.016* \n    −0.015* \n  \n  \n     \n    (0.002) \n    (0.002) \n    (0.008) \n    (0.007) \n  \n  \n    quantile \n    0.006*** \n     \n    0.003*** \n     \n  \n  \n     \n    (0.000) \n     \n    (0.000) \n     \n  \n  \n    friday × quantile \n    −0.002*** \n    −0.002*** \n    0.003** \n    0.003*** \n  \n  \n     \n    (0.000) \n    (0.000) \n    (0.001) \n    (0.001) \n  \n  \n    Num.Obs. \n    130759 \n    130759 \n    130759 \n    130759 \n  \n  \n    R2 \n    0.054 \n    0.057 \n    0.002 \n    0.015 \n  \n  \n    R2 Within \n     \n    0.000 \n     \n    0.000 \n  \n  \n    Std.Errors \n    by: anndat \n    by: anndat \n    by: anndat \n    by: anndat \n  \n  \n    FE: size_decile \n     \n    X \n     \n    X \n  \n  \n    FE: year \n     \n    X \n     \n    X \n  \n  \n    FE: month \n     \n    X \n     \n    X \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"
  },
  {
    "objectID": "freaky_friday/assignment.html",
    "href": "freaky_friday/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "For the assignment, you can just copy the code in the assignment and make the necessary changes where indicated. Do not forget to copy all the code. Not just the one where you change something.\n\nSetup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/assignment.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nLet’s restrict the data to two years. So that the computations do not take too long.\n\nbegin <- ymd(\"2003-01-01\")\nend <- ymd(\"2005-12-31\")\n\nI keep the full stock price data for now. This code is a slightly adapted version of the Abnormal returns page on the website. There are two changes: (1) we have to filter only the announcements between begin and end and (2) we add more factors from the French data. I will not go into the theoretical details here. The basic idea is that stock return are not only sensitive to one set of economic factors which are captured in the market return, but also to more specific factors such as the size (smb) and whether the company is value or growth company (hml). In what follows, we will treat the returns to these factors exactly the same as the market return, i.e. as predictors for the returns of the company that we are interested in.\n\nearn_ann <- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\")) %>%\n  filter(anndat >= begin, anndat <= end)\nanalyst <- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\")) %>%\n  filter(anndat >= begin, anndat <= end)\nall_stocks <- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench <- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %>%\n  mutate(date = ymd(date)) %>%\n  mutate_if(is.numeric, ~ . / 100) %>%\n  mutate(mkt = mkt_rf + rf) %>%\n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   <date>       <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\n\nThree-Factor model\nThe following code is almost identical to the Abnormal returns page. However, for the three-factor model we need to run the following regression for each announcement.\n\\[ R_{u,k} = \\alpha_{t,k} + beta^{mkt}_{t,k} R_{u,m}\n+ \\beta^{size}_{t,k} smb_{u,m} + \\beta^{value}_{t,k} hlm_{u,m} + \\epsilon_{u,k}\\]\nThat is, we include two more factors (smb, and hlm) in the regression model that we run for each announcement. The three-factor model typically works with the stock return and market return adjusted for the risk free rate (i.e. ret - rf and mkt_rf respectively). The code will give us a column coefs with the 4 estimated parameters alpha, beta_mkt, beta_size, beta_value.\nYou need to change the code below in two parts as indicated by the comments in the code.\nThe code first creates a function creat_coefs_3factor for n earnings announcements where the default value for n is 6. You can test whether your function works by creating the test object which should be a data set with 6 rows. One for each of the 6 earnings announcements at the top of the earn_ann\n\ncreate_coefs_3factor <- function(n = 6){\n  earn_ann %>% head(n = n) %>%\n    distinct(permno, anndat) %>%\n    mutate(start = anndat - 300, end = anndat - 46) %>%\n    left_join(select(all_stocks, permno, date, ret),\n              by = join_by(permno == permno, start <= date, end >= date)) %>%\n    # Changes are necessary in the following lines\n    # ... needs to be changed\n    left_join(select(famafrench, ...),\n              by = join_by(date == date)) %>%\n    filter(!is.na(ret), !is.infinite(ret)) %>%\n    # Changes are necesary in the following lines:\n    # .x, .y., .z need to be changed\n    summarise(y = list(cbind(ret - rf)),\n              X = list(cbind(alpha = 1, beta_mkt = .x,\n                             beta_size = .y, beta_value = .z)),\n              .by = c(permno, anndat)) %>%\n    mutate(coefs = pmap(list(X, y), ~ lm.fit(..1, ..2) %>% coef()),\n           .by = c(permno, anndat)) %>%\n    select(-y, -X)\n}\n\ntest <- create_coefs_3factor()\n\nmicrobenchmark::microbenchmark(\n                  create_coefs_3factor(1000),\n                  times = 10)\n\nIf it takes on average (noticeably) more than 10 seconds to run the three-factor model on 1000 observations, it is ok to limit the sample to 1 year. You can change the start and end date above and rerun the code.\n\n\nAbnormal Returns\nIn the first parts of the next code, we will calculate the coefficients for all announcements in the 2 year dataset. This is computationally the most intensive step and I could take up to two minutes to finish. When finished, we have an object results that contains the alphas and betas, we need to calculate the abnormal returns.\nIn the next step, we need to calculate the abnormal returns for every announcement in our data based on the three factors (mkt, smb, hlm). That is, we have to subtract the actual return on a day, \\(h\\), after the announcement from the expected return based on the three-factor model.\n\\[ R_{t,k} ^ {h} = R_{h, k} - \\alpha_{t,k} - \\beta^{mkt}_{t,k} R_{h,m}\n- \\beta^{size}_{t,k} smb_{h,m} - \\beta^{value}_{t,k} hlm_{h,m}\\]\n\\(R_{t,k}^h\\) is the abnormal return on day \\(h\\) after the announcement, \\(R_{h,m}\\) is the market return on day \\(h\\), \\(smb_{h,m}\\) is the size factor and \\(hlm_{h,m}\\) is the value factor.\nIn the last change, you need to add up the abnormal returns for the days 0 and 1 for the short window after the announcement and the for the days 2-75 after the announcement. That is you need to calculate car_sum\nI also give the accumulation with the product which is more correct but the literature typically works with the sum as far as I could discern. You will also see further in the code that it does not really matter for our results even in the restricted dataset. You can use car_prod as an example for car_sum where car_sum is simpler.\n\nN <- nrow(earn_ann)\nresults <- create_coefs_3factor(N)\n\nabnormal <- results %>%\n  unnest_wider(coefs) %>%\n  mutate(date75 = anndat + 75) %>%\n  left_join(select(all_stocks, permno, date, ret),\n            by = join_by(permno == permno,\n                         date75 >= date, anndat <= date)) %>% print %>%\n  # Changes are necessary in the following lines\n  # ... needs to be changed\n  left_join(select(famafrench, ...),\n            by = join_by(date == date)) %>%\n  # Changes are necessary in the following lines\n  # ... needs to be changed\n  mutate(ar = ...,\n         time_frame = if_else(date - anndat <= 1, \"short\", \"long\")) %>%\n  # Changes are necessary in the following lines\n  # .x needs to be changed\n  summarise(car_sum = ...,\n            car_prod = prod(1 + ar) - 1,\n            .by = c(permno, anndat, time_frame)) %>%\n  filter(!is.na(car_sum), !is.na(car_prod)) %>%\n  pivot_wider(values_from = c(car_sum, car_prod), names_from = time_frame)\nglimpse(abnormal)\n\n\n\nPutting it all together\nThe next two code blocks gather the necessary market price and market value information. Next we combine the earning announcement data with the analyst expectations data, the abnormal returns, and the stock price data. Finally, we calculate the earnings surprise scaled by the stock price.\nYou can just include this code as is.\n\nclean_prices <- all_stocks %>%\n  filter(prc > 0) %>%\n  select(permno, date, prc, shrout) %>%\n  mutate(market_value = prc * shrout) %>%\n  select(-shrout) %>%\n  print()\n\n# A tibble: 14,333,348 × 4\n   permno date         prc market_value\n    <dbl> <date>     <dbl>        <dbl>\n 1  10002 1994-03-10  13         38987 \n 2  10002 1994-03-11  13.8       41236.\n 3  10002 1994-03-16  13.2       39737.\n 4  10002 1994-03-22  13.4       40112.\n 5  10002 1994-03-24  13.1       39362.\n 6  10002 1994-04-07  12.5       37488.\n 7  10002 1994-04-15  14         41986 \n 8  10002 1994-04-18  14         41986 \n 9  10002 1994-04-22  12.5       37488.\n10  10002 1994-04-28  14         41986 \n# ℹ 14,333,338 more rows\n\n\n\nsurprise <- earn_ann %>%\n  left_join(analyst,\n            by = join_by(ticker, anndat, actual, pdf)) %>%\n  left_join(abnormal,\n            by = join_by(permno, anndat)) %>%\n  mutate(date_minus5 = anndat - 5) %>%\n  left_join(clean_prices,\n            by = join_by(permno, closest(date_minus5 >= date))) %>%\n  mutate(surprise = (actual - median) / prc)\nglimpse(surprise)\n\nThe last code block here further cleans the data following the rules set in the paper and as used by me in the Abnormal returns page. You can just include the code as is.\n\nwinsorise <- 5/10000\nmain <- surprise %>%\n  filter(!is.na(surprise)) %>%\n  filter(abs(median) < prc, abs(actual) < prc) %>%\n  filter(prc > 2) %>%\n  mutate(weekday = wday(anndat, label = TRUE)) %>%\n  filter(! weekday %in% c(\"Sat\", \"Sun\")) %>%\n  filter(percent_rank(car_sum_long) >= winsorise,\n         percent_rank(car_sum_long) <= 1 - winsorise,\n         percent_rank(car_sum_short) >= winsorise,\n         percent_rank(car_sum_short) <= 1 - winsorise) %>%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\n\n\n\nDescriptive Plot\nThe next code block creates the quantiles in the new main dataset so that we can recreate one of the figures.\n\nquantiles <- main %>%\n  mutate(sign = case_when(surprise > 0 ~ \"positive\",\n                          surprise < 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %>%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %>%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %>%\n  glimpse()\n\nThe last part you need to do is to recreate the quick version of Figure 1a in the descriptives page\n\nggplot(quantiles,\n       aes(...)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey()\n\nThis is some free code just to demonstrate how easy it is to combine pivot_longer and ggplot to create similar plots for a bunch of similar variables where the variables are originally in different columns.\n\nquantiles %>%\n  pivot_longer(c(car_sum_short, car_prod_short, car_sum_long, car_prod_long),\n               values_to = \"car\", names_to = \"window\") %>%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ window, nrow = 2)"
  },
  {
    "objectID": "machine_learning/application.html",
    "href": "machine_learning/application.html",
    "title": "Peer Firms",
    "section": "",
    "text": "In this section, I will show how you can code a prediction model for the return of a company’s stock price based on the returns of a number of peer companies. I loosely follow the same approach as in Baker and Gelbach (2020). In that paper, peer firms are identified based on the SIC code in the Compustat data which indicates what the industry is of the firm’s primary product.\nWe are using the tidymodel package for the application and I will follow the introduction to tidymodels. The advantage of the tidymodels approach is that you can follow a very similar workflow for other machine learning methods than the one I will be showing here."
  },
  {
    "objectID": "machine_learning/application.html#get-some-data",
    "href": "machine_learning/application.html#get-some-data",
    "title": "Peer Firms",
    "section": "Get Some Data",
    "text": "Get Some Data\nWe are going to use some example data from one earnings announcement and we are going to use data from an earnings announcement where the firm has a lot of peers because in that situation prediction is more likely to suffer from overfitting. The data_ml data excludes all peers with missing observations because most of the algorithms do not deal well with missing values. There are more sophisticated approaches possible to deal with missing data but they are not necessary to illustrate the main workflow.\n\npeers <- readRDS(here(\"data\", \"machine_learning\", \"peers.RDS\")) %>%\n  filter(N == max(N))\nglimpse(peers)\n\nRows: 4\nColumns: 6\n$ gvkey   <chr> \"063799\", \"063799\", \"063799\", \"017208\"\n$ permno  <dbl> 84058, 84058, 84058, 83729\n$ anndat  <date> 2003-01-23, 2003-04-22, 2003-07-24, 2003-04-18\n$ permnos <list> <62770, 65138, 20694, 85073, 22032, 86685, 23916, 35167, 36469…\n$ N       <dbl> 200, 200, 200, 200\n$ data    <list> [<tbl_df[157 x 199]>], [<tbl_df[156 x 200]>], [<tbl_df[155 x 2…\n\ndata_ml <- pull(peers, data)[[1]] %>%\n  select_if(~!any(is.na(.)))\nprint(data_ml)\n\n# A tibble: 157 × 193\n    `10002`  `10304`  `10562`  `10563`  `10588`   `10623`  `10725`  `10825`\n      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>\n 1  0.0508  -0.00128  0.00498  0.00394 -0.00187 -0.0106    0.00609 -0.0147 \n 2  0        0.00511  0.00142  0        0.00900  0         0.00700 -0.00199\n 3  0.0242  -0.00551  0.00566 -0.0196   0.00747 -0.00391  -0.00474  0.0115 \n 4  0.0142   0.00469  0.00141  0.0299   0.0221   0.000357  0.0133   0.00296\n 5  0.0349  -0.00551  0.00351  0        0.0227   0.0250    0.0282  -0.00246\n 6  0.00674  0.0128  -0.00210 -0.0275   0.0113   0.0157    0.0146   0.0173 \n 7  0.0179  -0.0105   0.00421  0.00806  0.0192   0.0257    0.0231   0.0155 \n 8  0.0263   0.0195   0.0216   0.0160   0.0137   0.00836   0.0349   0.00382\n 9  0.0231  -0.0187  -0.0219   0        0.0287  -0.00166  -0.0182   0.00429\n10 -0.0141   0.0187   0.0238   0.0331   0.00952  0.0498    0.0142   0.0491 \n# ℹ 147 more rows\n# ℹ 185 more variables: `10906` <dbl>, `10913` <dbl>, `10916` <dbl>,\n#   `11216` <dbl>, `11348` <dbl>, `11389` <dbl>, `11634` <dbl>, `11808` <dbl>,\n#   `11823` <dbl>, `16644` <dbl>, `20694` <dbl>, `22032` <dbl>, `23326` <dbl>,\n#   `23916` <dbl>, `24628` <dbl>, `27254` <dbl>, `35167` <dbl>, `35503` <dbl>,\n#   `35829` <dbl>, `35917` <dbl>, `36469` <dbl>, `39766` <dbl>, `41807` <dbl>,\n#   `44688` <dbl>, `47159` <dbl>, `52265` <dbl>, `52840` <dbl>, …"
  },
  {
    "objectID": "machine_learning/application.html#linear-model",
    "href": "machine_learning/application.html#linear-model",
    "title": "Peer Firms",
    "section": "Linear model",
    "text": "Linear model\nWe first going to run a traditional linear model within the tidymodels framework. The approach is overkill for just running a linear model but it will help us to build up the full workflow. In the code, I first split the data in a proportion \\(\\frac{N - 20}{N}\\) of training data and a proportion $ frac{20}{N}$ of test data, where \\(N\\) is the number of days of data available. As a result, we will use the training data to determine the parameters and than use that model to predict the last 20 days as test data. Next, we set which type of model we want to use, a linear model. We than fit the model where we specify the return variable as the outcome variable and we use the dot . to indicate that we want to use all the other variables as predictors. Finally, the tidy function gives the estimates and statistics for the estimates in a format that works with the tidyverse.\n\nN <- nrow(data_ml)\ndata_split <- initial_time_split(data_ml, prop = (N - 20)/N)\ntrain <- training(data_split)\ntest <- testing(data_split)\nlm_mod <- linear_reg()\nlm_fit <- lm_mod %>%\n  fit(return ~ ., data = train)\ntidy(lm_fit)\n\n# A tibble: 193 × 5\n   term         estimate std.error statistic p.value\n   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)  -0.00257       NaN       NaN     NaN\n 2 `10002`       2.23          NaN       NaN     NaN\n 3 `10304`     -21.6           NaN       NaN     NaN\n 4 `10562`      -5.43          NaN       NaN     NaN\n 5 `10563`      -2.23          NaN       NaN     NaN\n 6 `10588`      -5.37          NaN       NaN     NaN\n 7 `10623`       1.66          NaN       NaN     NaN\n 8 `10725`       5.77          NaN       NaN     NaN\n 9 `10825`      -5.44          NaN       NaN     NaN\n10 `10906`       9.65          NaN       NaN     NaN\n# ℹ 183 more rows\n\n\nYou can immediately see that the model does not report any standard errors or p-values. This happens because we have more predictors than we have trading days and thus we can perfectly fit the in-sample returns. This type of data is exactly why would want to use a regularised regression approach.\nBefore we do that, we need to fix another issue. In the previous code, we used the day variable as a predictor which was not what we intended. You can see that we can get an estimate for day in the code below. Also notice the advantage of using tidy representation of the estimated coefficients is that we can use filter to focus on some terms. To solve the issue with the day variable, we will use a recipe to set the formula and do some data cleaning. Specifically, we will specify that the day variable is an ID variable which means that it should not be used for prediction but we want to keep it in the data for investigating the fit later.\n\ntidy(lm_fit) %>%\n  filter(term == \"day\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 day         NA        NA        NA      NA\n\nlm_recipe <- recipe(return ~ ., data = train) %>%\n  update_role(day, new_role = \"ID\")\nsummary(lm_recipe)\n\n# A tibble: 193 × 4\n   variable type      role      source  \n   <chr>    <list>    <chr>     <chr>   \n 1 10002    <chr [2]> predictor original\n 2 10304    <chr [2]> predictor original\n 3 10562    <chr [2]> predictor original\n 4 10563    <chr [2]> predictor original\n 5 10588    <chr [2]> predictor original\n 6 10623    <chr [2]> predictor original\n 7 10725    <chr [2]> predictor original\n 8 10825    <chr [2]> predictor original\n 9 10906    <chr [2]> predictor original\n10 10913    <chr [2]> predictor original\n# ℹ 183 more rows\n\n\nLet’s put everything together in a workflow, fit the workflow and make predictions for the test data.\n\nlm_workflow <-\n  workflow() %>%\n  add_model(lm_mod) %>%\n  add_recipe(lm_recipe)\nlm_fit <- lm_workflow %>%\n  fit(data = train)\npreds <- predict(lm_fit, test, type = \"numeric\")\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\nglimpse(preds)\n\nRows: 20\nColumns: 1\n$ .pred <dbl> -1.0977181, -0.8374522, 2.2027743, -1.1720824, -0.4099103, 0.489…\n\n\nThe predictions give us 20 predictions for the days. We can also calculate the RMSE for the training data (in-sample fit) and the test data (out-of-sample fit).\n\naugment(lm_fit, train) %>%\n  rmse(return, .pred)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard    7.48e-16\n\naugment(lm_fit, test) %>%\n  rmse(return, .pred)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.47\n\n\nWhile the RMSE is almost 0 within the sample, it’s 1.47 out-of-sample. That is there is a large difference between the prediction error within the training data and the prediction error in the test data. This is the reason why we want to use a regularised regression to do out-of-sample predictions 2"
  },
  {
    "objectID": "machine_learning/application.html#elastic-net",
    "href": "machine_learning/application.html#elastic-net",
    "title": "Peer Firms",
    "section": "Elastic Net",
    "text": "Elastic Net\nIn this section, we are going to run the regularised regression as described in the theory. We will center and scale the data so that all returns have a mean of 0 and a standard deviation of 1. Remember that the penalty term punishes coefficients that are higher, however the size of the coefficient also depends on the variation in the variable. To put all the coefficients on equal footing, we scale them first.\nWe will set the penalty \\(\\lambda = .4\\) and the mixture proportion \\(\\alpha = 0.5\\) in the elastic net. We will need to install the glmnet package because that is the engine we use in our linear regression model. As before, we can put the updated data cleaning and the linear model in a new workflow and fit the model. Finally, we report the in-sample and out-of-sample RMSE.\n\nnet_recipe <- lm_recipe %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\nnet_model <- linear_reg(penalty = .4, mixture = 0.5) %>%\n  set_engine(\"glmnet\")\nnet_workflow <-\n  workflow() %>%\n  add_model(net_model) %>%\n  add_recipe(net_recipe)\nnet_fit <- net_workflow %>%\n  fit(data = train)\naugment(net_fit, train) %>%\n  rmse(return, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0241\n\naugment(net_fit, test) %>%\n  rmse(return, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0278\n\n\nIn contrast to the ordinary linear model, we see that the regularised model has a similar prediction error in-sample and out-of-sample. Furthermore, the out-of-sample prediction is much better with the regularised regression."
  },
  {
    "objectID": "machine_learning/application.html#tuning-elastic-net",
    "href": "machine_learning/application.html#tuning-elastic-net",
    "title": "Peer Firms",
    "section": "Tuning Elastic Net",
    "text": "Tuning Elastic Net\nThe problem with the previous analysis is that we had to set the penalty and mixture values. We can do better and test multiple values and see which ones give us the best prediction. This process is called tuning of the hyper parameters and it is a standard procedure in many machine learning applications.\nThe above code shows the general principle. We are dividing the training data in 10 randomly selected folds. We use the same workflow as above but we use the part of the training data that is not in a fold to predict the data in the fold. Now, we have 10 out-of-sample RMSEs. With collect_metrics, we get an overview of the mean RMSE and the standard error around the mean.\n\nfolds <- vfold_cv(train, v = 10)\nnet_fit_rf <-\n  net_workflow %>%\n  fit_resamples(folds, metrics = metric_set(rmse))\ncollect_metrics(net_fit_rf)\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0235    10 0.00186 Preprocessor1_Model1\n\n\nWhile the randomly selected folds is often appropriate, in our case it is not. We have a time series and we want to predict future returns based on earlier returns. So, we are going to use a different function to create rolling windows of 75 days of data to build the model to predict the next 5 days. So for instance, we use a day 1-75 to predict day 76-80. Next, we use day 6-80 to predict day 81-85. We repeat the procedure until we run out of data. When we print windows, you can see that we have 10 splits with 75 modeling observations and 5 observations that we want to predict.\n\nwindows = rolling_origin(train, initial = 75, assess = 5,\n                         skip = 5, cumulative = FALSE)\nprint(windows)\n\n# Rolling origin forecast resampling \n# A tibble: 10 × 2\n   splits         id     \n   <list>         <chr>  \n 1 <split [75/5]> Slice01\n 2 <split [75/5]> Slice02\n 3 <split [75/5]> Slice03\n 4 <split [75/5]> Slice04\n 5 <split [75/5]> Slice05\n 6 <split [75/5]> Slice06\n 7 <split [75/5]> Slice07\n 8 <split [75/5]> Slice08\n 9 <split [75/5]> Slice09\n10 <split [75/5]> Slice10\n\nnet_fit_windows <-\n  net_workflow %>%\n  fit_resamples(windows, metrics = metric_set(rmse))\ncollect_metrics(net_fit_windows)\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model1\n\n\nAgain, we find that the regularised version gives better out-of-sample predictions than the non-regularised version. In the next step, we will use different values for the penalty and the mixture. With the same procedure as before, we can estimate the mean out-of-sample RMSE for each combination of the penalty and mixture and then decide to use the combination that leads to the best predictions. This process is also called cross validation.\nFirst, we specify that we want a linear model with two hyper-parameters, the penalty and the mixture, that need to be tuned during the cross validation process. I specify 6 possible values for the penalty and 11 for the mixture. Next we specify the workflow and then fit the model over the grid of all 66 possible values of the penalty and the mixture.\n\nnet_tune_spec <-\n  linear_reg(\n    penalty = tune(),\n    mixture = tune()\n  ) %>%\n  set_engine(\"glmnet\")\ngrid_hyper <- expand.grid(\n  penalty = c(0, 0.1, 0.2, 0.3, 0.4, 0.5),\n  mixture = seq(0, 1, length.out = 11)\n)\nnet_tune_wf <-\n  workflow() %>%\n  add_recipe(net_recipe) %>%\n  add_model(net_tune_spec)\n\nnet_tune <-\n  net_tune_wf %>%\n  tune_grid(\n    resamples = windows,\n    grid = grid_hyper,\n    metrics = metric_set(rmse)\n  )\n\nNext, we collect the RMSE measures for the out-of-sample predictions for each combination of penalty and mixture. We see that a lot of models with some regularisation give the best out-of-sample predictions. In a more realistic, complicated example, I would expect there to be one combination that is the best.\n\ncollect_metrics(net_tune) %>%\n  arrange(mean)\n\n# A tibble: 66 × 8\n   penalty mixture .metric .estimator   mean     n std_err .config              \n     <dbl>   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>                \n 1     0.2     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model09\n 2     0.3     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model10\n 3     0.4     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model11\n 4     0.5     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model12\n 5     0.1     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model14\n 6     0.2     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model15\n 7     0.3     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model16\n 8     0.4     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model17\n 9     0.5     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model18\n10     0.1     0.3 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model20\n# ℹ 56 more rows\n\n\nNext, we select the best combination of penalty and mixture and use the final model on all the training data. We can then evaluate the prediction quality based on the RMSE metric.\n\nbest_model <- select_best(net_tune, \"rmse\")\nfinal_wf <- net_tune_wf %>%\n  finalize_workflow(best_model)\nfinal_fit <-\n  final_wf %>%\n  last_fit(data_split, metrics = metric_set(rmse))\ncollect_metrics(final_fit)\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      0.0278 Preprocessor1_Model1\n\n\nFinally, we can calculate the predictions for the test set which we split off at the beginning and add the day column from the test set. We can than graph the deviations between the actual return and the predicted return for the 25 days in the test data.\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\naug_fit <- bind_cols(collect_predictions(final_fit), select(test, day))\nggplot(aug_fit, aes(x = day, y = return - .pred)) +\n  geom_point()\n\nDon't know how to automatically pick scale for object of type <difftime>.\nDefaulting to continuous.\n\n\n\n\n\nI can see no clear patterns in the deviations between the actual return and the predicted values."
  },
  {
    "objectID": "machine_learning/application.html#the-data",
    "href": "machine_learning/application.html#the-data",
    "title": "Peer Firms",
    "section": "The data",
    "text": "The data\nI rewrote the function to get the returns necessary for the machine learning model. The biggest difficulty is to make sure that we have the correct dates because the training data and the test data are not an uninterrupted time series. So, the extra parameters are mainly to deal with the end and start day relative to the earnings date. At the end of the data cleaning pipe, I also delete all the columns with missing data. Because, we have need to run the data for a lot of earnings announcements, we might end up with some less than ideal cases with missing data, insufficient variables, or missing returns for the firm that we are interested in. There are more elegant solutions to avoid getting errors for these issues but I will just try to avoid them or try to make sure that the code keeps running even when an error occurs for one of the earnings announcements.\nI left some of my test code at the end of the code chunk. It’s a good idea to have these quick tests when you write more extensive functions. Over time, you might need to update the functions and you want to make sure that they still work on examples that worked before.\n\nget_returns_event <- function(anndat, permno_y, permnos,\n                        before_start = 300, before_end = 25,\n                        after_start = 0, after_end = 75\n                        ){\n  before_begin <- anndat - before_start\n  before_last <- anndat - before_end\n  after_begin <- anndat + after_start\n  after_last <- anndat + after_end\n  data <- returns %>%\n    # all dates within the range\n    filter(permno %in% permnos, date >= before_begin, date <= after_last) %>%\n    # exclude the dates after training and before event\n    filter(date < before_last | date >= after_begin) %>%\n    filter(!is.na(ret)) %>%\n    pivot_wider(values_from = ret, id_cols = date, names_from = permno) %>%\n    mutate(day = date - anndat) %>%\n    select(-date) %>%\n    arrange(day) %>%\n    select_if(~!any(is.na(.)))\n\n  vars <- names(data)\n  names(data) <- if_else(vars == as.character(permno_y),\n                         \"return\", vars)\n  return(data)\n}\nget_returns_event(anndat, permnos[3], permnos) %>%\n  arrange(-day)\n\n# A tibble: 244 × 3\n    `50906`   return day    \n      <dbl>    <dbl> <drtn> \n 1  0.0194  -0.0184  74 days\n 2  0.0665   0.0155  73 days\n 3  0.0380   0.00323 72 days\n 4  0.0262  -0.0178  71 days\n 5  0.00620 -0.00316 70 days\n 6  0.0179  -0.00629 67 days\n 7  0.0485   0.0108  66 days\n 8 -0.00110 -0.0193  65 days\n 9  0.0231   0.0926  64 days\n10  0.0114  -0.00339 60 days\n# ℹ 234 more rows\n\nget_returns_event(anndat = peers$anndat[1],\n                  permno_y = peers$permno[1],\n                  permnos = peers$permnos[[1]])\n\n# A tibble: 243 × 192\n    `10002`  `10304`  `10562`  `10563`   `10623`  `10725`  `10825`   `10906`\n      <dbl>    <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1  0.0508  -0.00128  0.00498  0.00394 -0.0106    0.00609 -0.0147  -0.00645 \n 2  0        0.00511  0.00142  0        0         0.00700 -0.00199  0.0138  \n 3  0.0242  -0.00551  0.00566 -0.0196  -0.00391  -0.00474  0.0115  -0.00791 \n 4  0.0142   0.00469  0.00141  0.0299   0.000357  0.0133   0.00296 -0.00532 \n 5  0.0349  -0.00551  0.00351  0        0.0250    0.0282  -0.00246  0.00573 \n 6  0.00674  0.0128  -0.00210 -0.0275   0.0157    0.0146   0.0173   0.0137  \n 7  0.0179  -0.0105   0.00421  0.00806  0.0257    0.0231   0.0155  -0.000749\n 8  0.0263   0.0195   0.0216   0.0160   0.00836   0.0349   0.00382  0.0124  \n 9  0.0231  -0.0187  -0.0219   0       -0.00166  -0.0182   0.00429 -0.0118  \n10 -0.0141   0.0187   0.0238   0.0331   0.0498    0.0142   0.0491   0.00824 \n# ℹ 233 more rows\n# ℹ 184 more variables: `10913` <dbl>, `10916` <dbl>, `11216` <dbl>,\n#   `11348` <dbl>, `11389` <dbl>, `11634` <dbl>, `11808` <dbl>, `11823` <dbl>,\n#   `16644` <dbl>, `20694` <dbl>, `22032` <dbl>, `23326` <dbl>, `23916` <dbl>,\n#   `24628` <dbl>, `27254` <dbl>, `35167` <dbl>, `35503` <dbl>, `35829` <dbl>,\n#   `35917` <dbl>, `36469` <dbl>, `39766` <dbl>, `41807` <dbl>, `44688` <dbl>,\n#   `47159` <dbl>, `52265` <dbl>, `52840` <dbl>, `56232` <dbl>, …\n\n\nThe next functions runs the full elastic net workflow with cross validation for the hyper parameters. As you can see in the test code at the end, you can just use the function above to get data and then pass it on to the function to run the full elastic net. I make sure that the training data is from before the event day and the test data is from after the event day. For the rolling prediction windows in the cross validation step I choose 100 observations to predict the next 15 days which will result in 5 windows in the typical situation. I also limit the number of hyper parameters to test, just to limit the computations. The remainder of the code is very similar to the code in the previous section for the single prediction task.\n\nrun_elastic_net <- function(data, event_day = 0){\n  # split the data on day 0\n  n_total <- nrow(data)\n  n_before <- nrow(filter(data, day < event_day))\n  prop <- n_before/n_total\n  data_split <- initial_time_split(data, prop = prop)\n  train <- training(data_split)\n  test <- testing(data_split)\n  # prepare data and windows \n  net_recipe <-\n    recipe(return ~ ., data = train) %>%\n    update_role(day, new_role = \"ID\") %>%\n    step_center(all_predictors()) %>%\n    step_scale(all_predictors())\n  folds <- rolling_origin(train, initial = 100, assess = 15,\n                          skip = 15, cumulative = FALSE)\n  # set up the hyper parameters\n  net_tune_spec <-\n    linear_reg(\n      penalty = tune(),\n      mixture = tune()\n    ) %>%\n      set_engine(\"glmnet\")\n  grid_hyper <- expand.grid(\n    penalty = c(0, 0.2, 0.4, 0.6, 0.8, 1),\n    mixture = seq(0, 1, length.out = 6))\n  # run model\n  net_tune_wf <-\n    workflow() %>%\n    add_model(net_tune_spec) %>%\n    add_recipe(net_recipe)\n  net_tune <-\n    net_tune_wf %>%\n    tune_grid(\n      resamples = folds,\n      grid = grid_hyper,\n      metrics = metric_set(rmse))\n # Get best model\n  best_model <- select_best(net_tune, \"rmse\")\n  final_wf <- net_tune_wf %>%\n    finalize_workflow(best_model)\n  final_fit <-\n    final_wf %>%\n    last_fit(data_split, metric_set(rmse))\n  return(final_fit)\n}\n\ntest <- get_returns_event(anndat = peers$anndat[1],\n                  permno_y = peers$permno[1],\n                  permnos = peers$permnos[[1]]) %>%\n        run_elastic_net(event_day = 0)\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: ''\n\n\nFinally, we can combine the two functions above in one function and calculate the abnormal returns as the difference between the prediction from the elastic net and actual return. The cumulative return is the the sum of those abnormal returns. Notice that the bulk of the function is wrapped in the tryCatch function. This is an R function that let’s you control what happens if an error occurs in the code it wraps. I use the function to return NA in the case that any of the functions gives an error.\n\ncalculate_car <- function(anndat, permno_y, permnos,\n                         before_start = 300, before_end = 25,\n                         after_start = 0, after_end = 75,\n                         event_day = 0, car_short_last = 1){\n  result <- NA\n  tryCatch(\n    {data <- get_returns_event(anndat, permno_y, permnos,\n                              before_start, before_end,\n                              after_start, after_end)\n    fit <- run_elastic_net(data = data, event_day = event_day)\n    result <- augment(fit) %>%\n      select(return, .pred, day) %>%\n      mutate(ar = return - .pred) %>%\n      mutate(window = if_else(day <= car_short_last, \"short\", \"long\")) %>%\n      summarise(car = sum(ar), .by = window)},\n    error = function(err) {return(NA)})\n  return(result)\n}\n\ncalculate_car(anndat = peers$anndat[1], permno_y = peers$permno[1], permnos = peers$permnos[[1]],\n              event_day = 0)\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: ''\n\n\n# A tibble: 2 × 2\n  window      car\n  <chr>     <dbl>\n1 short  -0.00746\n2 long   -0.0150 \n\n\nIn the last code block, we run the functions for N_announce earnings announcements. The code is very similar to the original abnormal return code. We first construct the data in a way that allows us to use the functions we just created. The last mutate step then runs the function in parallel depending on how many cores that are available in our computer. The reason I chose to only save the CARs is that in this way the function in future_map does not have to pass on huge amount of data as input or output which generally improves performance.\n\nN_announce <- 500\ncores <- parallel::detectCores()\ntictoc::tic()\nplan(multisession, workers = cores - 2)\ncars <- earn_ann %>%\n  head(N_announce) %>%\n  left_join(compu_sic_new, by = join_by(gvkey)) %>%\n  filter(!is.na(sic_new)) %>%\n  left_join(compu_sic_new, by = join_by(sic_new),\n            relationship = \"many-to-many\",\n            suffix = c(\"\", \"_peer\")) %>%\n  left_join(linking_table,\n            by = join_by(gvkey_peer == gvkey,\n                         between(anndat, start_date, end_date)),\n            suffix = c(\"\", \"_peer\")) %>%\n  filter(!is.na(permno_peer)) %>%\n  select(gvkey, permno, anndat, permno_peer) %>%\n  distinct(.) %>%\n  summarise(permnos = list(permno_peer),\n            .by = c(gvkey, permno, anndat)) %>%\n  mutate(N = map_dbl(permnos, ~ length(.x))) %>%\n  filter(N > minimum_peers) %>%\n  mutate(car = future_pmap(list(anndat, permno, permnos),\n                            ~ calculate_car(..1, ..2, ..3, event_day = 0),\n                           .options = furrr_options(seed = 230383),\n                           .progress = TRUE))\ntictoc::toc()\n\n804.243 sec elapsed"
  },
  {
    "objectID": "machine_learning/assignment.html",
    "href": "machine_learning/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Answer the following two questions in no more than 1 A4 page in total. My expectations are 5 to 10 sentences per question depending on your writing style. You can submit your answer on LMS in word, qmd, pdf, or html version."
  },
  {
    "objectID": "machine_learning/assignment.html#questions",
    "href": "machine_learning/assignment.html#questions",
    "title": "Assignment",
    "section": "Questions",
    "text": "Questions\n\nSearch for an article in the same research area as your thesis that uses any machine learning technique. Give a citation to the paper and based on your reading of the abstract of the paper and skimming the methodology section, argue whether the machine learning improves the researchers’ ability to answer their research question. When you look for a paper do not restrict your search too much. I primarily want you to find a paper where you are comfortable with the main research question. For instance, I used google scholar and searched for \"voluntary disclosure\" \"machine learning\" source:accounting to find an article with a machine learning technique on voluntary disclosure in an accounting journal. If you cannot find an article in your research area, I would also accept a paper in any area that you have discussed in FINA4481, FINA4491, or ACCT4471. Do not spend too much time on finding the right paper!\nHow could a machine learning technique improve the method for your thesis? You can ignore the cost of data collection and can assume that you can use all the data sources that have been used in your research field.\n\n\n\n\n\n\n\nNote\n\n\n\nFor answering the questions, you will have to ask yourself the question whether the main research question involves a prediction task 1. If not and the research question involves estimating a causal 2 or descriptive 3 estimate, can better out-of-sample prediction help with this research question. Ask yourself the question whether traditional linear models or time series models are sufficient or whether more advanced machine learning methods are required."
  },
  {
    "objectID": "machine_learning/predictions.html",
    "href": "machine_learning/predictions.html",
    "title": "Predictions",
    "section": "",
    "text": "Introduction\nSo far in the course, I have made the assumption that our research is mainly interested in estimating a certain parameter of interest, i.e. we want to know the effect of a certain variable on a certain outcome variable. For instance, we are interested in the effect of a policy uncertainty on a firm’s investment decisions (Falk and Shelton 2018), or the use of options in CEO compensation on risk taking (Shue and Townsend 2017). In terms of a regression model, \\(y_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i\\), we are interested in \\(\\beta\\), the effect of \\(x\\) on \\(y\\) after controlling for \\(z\\). All the statistical and conceptual tools in the lectures had the aim of making sure that the estimate \\(\\hat{\\beta}\\) is the best possible estimate of the true underlying and unobserved \\(\\beta\\) (Mullainathan and Spiess 2017).\nIn contrast, some research questions focus on building a model to predict the outcomes. In that case, we are not interested in the estimated parameter \\(\\beta\\) (or \\(\\alpha\\) or $). We are interested in the prediction \\(\\hat{y}\\), especially for future or unobserved cases (Mullainathan and Spiess 2017). One of the biggest risks for prediction tasks is that our model excels at predicting the outcomes in the data that we have but it is rubbish at predicting for new data. In that case, our model is overfitting. The strength of a lot of machine learning applications is that they excel at incorporating a lot of variables while at the same time guard against overfitting. If prediction is the research question these are the methods we should turn to.\n\n\nContent\nFor this course, I will give a very gentle introduction to the tidymodels framework from the same team as the tidyverse. The framework provides a unified workflow to work with different machine learning (and traditional regression) models for prediction tasks. I will not use it for a typical prediction task. I will use it for the research question whether (initial) stock price reactions to Friday earnings announcements are muted (Dellavigna and Pollet 2009).\nFor that research question, we want to estimate the abnormal return after a company’s earnings announcement. That means that we need to predict what the counterfactual return would have been in the absence of an announcement. That is a prediction task! We did not really care whether we correctly estimated the market beta or the effect of the factors. We cared whether we predicted the expected returns correctly. When we only have one or three predictors the advantage of the machine learning methods is likely to be minimal but we could use more predictors. The market return and the factors are essentially stand ins for controlling for other factors besides that announcement that could effect the return. A straight forward extension of the factor models would be to include the returns of a number of peer firms as predictors and allow the weight (i.e. the betas) to vary (Baker and Gelbach 2020). In this case, we might have a lot of predictors and only a limited amount of observations to estimate the betas. This is a classical case where we might be at risk of overfitting.\n\n\nOther Applications\nThe inclusion of abnormal returns is a specific application of a more general use case for machine learning techniques in accounting and finance research. Sometimes, we are not interested in some parameters. We just want to include (control) predictors to remove some potential confounding effect. For instance, in my original regression model, if we have a lot of variables \\(z\\) but we don’t care about the parameters \\(\\gamma\\), we can use machine learning techniques to include more variables than a typical regression would allow (Mullainathan and Spiess 2017).\n\\[\ny_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i\n\\]\nOne way to think about this is that we are splitting up the regression in two steps. First, we predict the outcome variable, \\(\\hat{y}\\) based on the many predictors \\(z\\). Second, we estimate the parameter \\(\\beta\\) with \\(y - \\hat{y}\\) as the dependent variable (Mullainathan and Spiess 2017).\nAnother related application is that we want to use the machine learning techniques to predict the causal variable of interest with the predictors z, so that we can use \\(x - \\hat{x}\\) as the new causal variable and estimate it’s effect on \\(y - \\hat{y}\\). For some research questions, it will be appropriate to say that after we removed the influence of a bunch of observable factors, we are more confident that the remaining variation in \\(x\\) and \\(y\\) is the capturing the causal effect we are interested in. However, remember that sometimes the opposite is true. In the (Shue and Townsend 2017) paper, we wanted to predict the changes in the option awards that followed a predictable schedule. The paper essentially used a very simple prediction model to predict the option awards that we could treat as if they are random (or exogenous).\nA final application is when you want to increase the dataset when you need to collect some variable by manually labelling or categorising some data. Sometimes it is possible to collect the data manually for a subset of the data. You can then use the machine learning tools to predict what variable based on the relation between predictors and your measure in the manually collected data.\n\n\n\n\n\nReferences\n\nBaker, Andrew, and Jonah B. Gelbach. 2020. “Machine Learning and Predicted Returns for Event Studies in Securities Litigation.” Journal of Law, Finance, and Accounting 5 (2): 231–72. https://doi.org/10.1561/108.00000047.\n\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x.\n\n\nFalk, Nathan, and Cameron A. Shelton. 2018. “Fleeing a Lame Duck: Policy Uncertainty and Manufacturing Investment in US States.” American Economic Journal: Economic Policy 10 (4): 135–52. https://doi.org/10.1257/pol.20160365.\n\n\nMullainathan, Sendhil, and Jann Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106. https://doi.org/10.1257/jep.31.2.87.\n\n\nShue, Kelly, and Richard R. Townsend. 2017. “How Do Quasi-Random Option Grants Affect CEO Risk-Taking?” The Journal of Finance 72 (6): 2551–88. https://doi.org/10.1111/jofi.12545."
  },
  {
    "objectID": "machine_learning/theory.html",
    "href": "machine_learning/theory.html",
    "title": "Theory",
    "section": "",
    "text": "Theory\nIn this section, I will show how you can code a prediction model for the return of a company’s stock price based on the returns of a number of peer companies. I loosely follow the same approach as in Baker and Gelbach (2020). In that paper, peer firms are identified based on the SIC code in the Compustat data which indicates what the industry is of the firm’s primary product.\nWe are using the tidymodel package for the application and I will follow the introduction to tidymodels. The advantage of the tidymodels approach is that you can follow a very similar workflow for other machine learning methods than the one I will be showing here.\nTo understand the machine learning approach that we are using, I need to start of with some theory. The fundamental idea is that we want to avoid overfitting in the data that we have, so that when we use the model to predict on new data we still have good predictions. This means that when we estimate our model we do not want to have a model that fits the data as good as possible. We want to regularize the parameters in the model so that we do not get perfect fit but better out-of-sample predictions. For instance, if we use 200 trading days and have 200 peer firms, we can perfectly predict within the sample of 200 days 1 but there are no guarantee that we will get good predictions from that model out-of-sample (i.e. after the earnings announcement).\nFor the linear model to predict stock returns based on peers, we will use a the elastic net regularizer to bias the estimates within the sample data to make it more likely that the linear model will give good predictions within the sample. One way to think about the linear model with peers as predictors is that we are creating a bespoke market return for each firm as a weighted average of its peers.\nA regular linear model works estimates the \\(\\beta\\)s by minimising the following equation where we minimise the sum of the squared difference between the outcome (\\(y_i\\)) and prediction (\\(X_i \\beta\\)) for each observation i.\n\\[\n\\sum^N_{i=1} (y_i - X_i \\beta)^2\n\\]\nThat is, we want to find estimates that give the best possible fit in the data. The regulariser puts a penalty on bigger absolute values for the \\(\\beta\\)s to limit overfitting to the in-sample data. The estimates will now be chosen to minimise the following equation.\n\\[\n\\sum^N_{i=1} (y_i - X_i \\beta)^2 + \\lambda \\left ( \\alpha \\sum^p_j \\beta^2_j + (1 - \\alpha) \\sum^p_j |\\beta_j| \\right)\n\\]\nThe size of the penalty is given by the parameters \\(\\lambda > 0\\) and consists of two parts: the sum of the squared \\(\\beta\\)s and the sum of the absolute values of the \\(\\beta\\)s. The first term is the ridge reguliser and the second one is the LASSO regulariser. They both have been shown to have useful properties as regulisers and thus they are often used together with the weight \\(1 \\geq \\alpha \\geq 0\\). With \\(\\alpha = 1\\), we only use ridge regression and with \\(\\alpha = 0\\), we only use the LASSO.\nThe final step is that we need to choose the right values for \\(\\lambda\\) and \\(\\alpha\\). A common approach is to use cross validation where we split the data that we have in roughly equal sized partitions or folds. For instance, you have 10 folds with each 10% of the data. With cross validation, we will use the 90% of the remaining data, use a number of different values for \\(\\lambda\\) and \\(\\alpha\\) to estimate \\(\\beta\\)s and predict the 10% fold that we did not use for estimation. In other words, we use the data that we have to do a prediction task with the advantage that we can evaluate which \\(\\lambda\\) and \\(\\alpha\\) gives us the best predictions.\nWe do need a measure to evaluate the quality of the predictions. A common choice is the Root Mean Squared Error (RMSE) which is defined as the square root of the squared difference between the actual value of the outcome and the predictions for the outcome.\n\\[\n\\sqrt{ \\frac{\\sum^N_{i=1} (y_i - \\hat{y}_i)^2}{N} }\n\\]\nWe will use the RMSE to evaluate the predictions out-of-sample. The RMSE is similar to the first equation where we choose the \\(\\beta\\)s to minimise the squared difference between the outcome and the fitted data in the in-sample data.\n\n\n\n\n\n\nReferences\n\nBaker, Andrew, and Jonah B. Gelbach. 2020. “Machine Learning and Predicted Returns for Event Studies in Securities Litigation.” Journal of Law, Finance, and Accounting 5 (2): 231–72. https://doi.org/10.1561/108.00000047.\n\nFootnotes\n\n\nIt’s a system of linear equations with 200 unknown parameters (the \\(\\beta\\)s) and 200 observations (the trading days).↩︎"
  },
  {
    "objectID": "generalised/interpretation.html",
    "href": "generalised/interpretation.html",
    "title": "Interpretation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(fixest)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nN <- 1001\ngof_map <- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generalised/interpretation.html#interpretation",
    "href": "generalised/interpretation.html#interpretation",
    "title": "Interpretation",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe problem\nThe problem with non-linear, generalised linear models is that the interpretation of the coefficients is more murky than in our linear models. In a linear model, we can interpret the coefficient, \\(\\beta\\), of the variable \\(x\\) easily. When \\(x\\) is a continuous variable, we can interpret the coefficient as the increase in \\(y\\) when \\(x\\) increases by 1. When \\(x\\) indicates whether an observation is in the treatment group (\\(x=1\\)) or in the control group (\\(x=0\\)), the coefficient estimates the difference between the control and treatment group for \\(y\\).\nUnfortunately, the non-linear transformation for the generalised linear models complicates the interpretation. Let’s illustrate this with an example where we know the true model for the probability that a firm has a female CEO (\\(y_1\\)) and the probability that the firm has a female CFO (\\(y_2\\)). The true probabilities are given by the following logistic models.\n\\[\n\\begin{aligned}\ny_1 &= \\frac{e^{2 + 3x}}{1 + e^{2 + 3x}} &&= g(2 + 3x) \\\\\ny_2 &= \\frac{e^{-2 + 3x}}{1 + e^{-2 + 3x}} &&= g(-2 + 3x)\n\\end{aligned}\n\\]\nYou can think of \\(x\\) as a characteristic of the company that increases the likelihood of female executives in the company. A casual glance of the equations would give the impression that the effect of the on the CEOs and CFOs is the same because \\(\\beta = 3\\) is the same in both equations. We can also plot the two probabilities as functions of \\(x\\) and you can see that that one curve is just the other one shifted horizontally.\n\ninterpretation <-\n  tibble(x = rnorm(N, 0, 1)) %>%\n  mutate(y1 = plogis(2 + 3 * x),\n         y2 = plogis(-2 + 3 * x))\n\ninterpretation %>%\n  pivot_longer(cols = c(y1, y2)) %>%\nggplot(aes(y = value, x = x,\n                        colour = name)) +\n  scale_color_viridis_d() +\n  geom_line()\n\n\n\n\nHowever, from the plots you can already see that firms at \\(x = 0\\) will see a larger increase in the probability of having a female CFO (\\(y_2\\)) than a female CEO (\\(y_1\\)) when \\(x\\) increases. Because at \\(x=0\\) most firms already are more likely to have a female CEO and thus the effect of \\(x\\) cannot be very large. In other words, despite the fact that the two probabilities follow a very similar function, the effect of an increase in \\(x\\) can be very different depending on the value of \\(x\\).\nAnother way to look at the problem is to think of \\(x\\) as a policy that is either present (\\(x=1\\)) or not (\\(x=0\\)). We can calculate the causal effect of the policy as follows.\n\ny1_effect <- plogis(2 + 3 * 1) - plogis(2 + 3 * 0)\ny2_effect <- plogis(-2 + 3 * 1) - plogis(-2 + 3 * 0)\nprint(c(y1_effect, y2_effect))\n\n[1] 0.1125101 0.6118557\n\n\nAgain, we see that the policy has a large effect (0.61) for \\(y_2\\) and a small effect for \\(y_1\\) (0.11). y\n\n\nWhy does it matter?\nThere are a number of reasons when this will matter. First of all, the coefficient (\\(\\beta = 3\\)) does not directly map onto the causal effect. The same coefficient can lead to different causal effects depending on the value of \\(x\\) and depending on other parts of the model (e.g. the intercept). That means that we cannot just rely on the coefficient.\nBecause the true effect depends on other parts of the model, generalised linear models make the use of fixed effects and robust standard errors more tricky as well. I am not going to go into the details of these issues but that is the overall problem with generalised linear model: the effect of each term is dependent on the other parts of the model.\nThere are two questions to ask whether this is a problem for your research question.\n\nThe first question is whether you are interested in predicting the variable \\(y\\) or whether you are interested in estimating an effect on the variable \\(y\\). As I discussed in the introduction to machine learning, when we are mainly interested in prediction, the coefficients or effects are less important. In the case of predictions, we definitely want to use a model that does not allow to make impossible predictions (e.g. probabilities that are higher than 1).\nThe second question is whether we are interested in the effect on the linear scale (\\(\\alpha + \\beta x\\)) or on the transformed scale (\\(g(\\alpha + \\beta x)\\)). In the working example I have used so far, you probably would be interested in the transformed scale, i.e. the probability that the firm has a female CEO. In a lot of cases in accounting and finance, we would be interested directly in those probabilities e.g. the probability that a firm goes bankrupt, or discloses certain information. In contrast, studies in consumer finance or behavioural economics are more interested in the utility that consumers derive from certain interventions. While the utility is often unobserved, we can observe the consumers choices (e.g. which mortgage they choose). A typical approach is to model the unobserved utility on the a linear scale (\\(z\\)) and model the probability of a choice as a transformation of the utility (\\(g(z)\\)). In fact, the logistic transformation is the workhorse function in this literature. Finally, there is literature that prefers the linear scale for logistic models because they argue that we can interpret the effects on the log odds scale, \\(\\textrm{log}(\\frac{p}{1-p})\\). That is, the effect on the linear represents an effect on the relative probability.1 Betting markets often present winning probabilities as odds or relative probabilities."
  },
  {
    "objectID": "generalised/interpretation.html#solution-without-controls",
    "href": "generalised/interpretation.html#solution-without-controls",
    "title": "Interpretation",
    "section": "Solution without controls",
    "text": "Solution without controls\nIn the next, section I am going to assume that we are on interested in the effect on the probability scale and we would like to summarise the effect in one number. We will start with the simplest case where the variable of interest is a simple treatment (\\(x = 1\\)) versus control (\\(x = 0\\)). We generate data according to the two functions above. In this simple case, it’s relatively easy to estimate the causal effects by just looking at the difference between the treatment and the control condition for both variables.\n\nsolutions <-\n  tibble(x = rbinom(N, 1, 0.5)) %>%\n  mutate(y1 = rbinom(N, 1, plogis(2 + 3 * x)),\n         y2 = rbinom(N, 1, plogis(-2 + 3 * x)))\n\nsolutions %>%\n  summarise(across(c(y1, y2), mean), .by = x)\n\n# A tibble: 2 × 3\n      x    y1    y2\n  <int> <dbl> <dbl>\n1     1 0.986 0.708\n2     0 0.886 0.136\n\n\nYou can see that the differences are pretty close to the theoretical effects y1_effect = 0.11 and y2_effect = 0.61 that we calculated before.\n\nLinear model\nThe next step is to figure out which regression approach gives us similar estimates. You can see that the OLS model, where we ignore that the outcome variable is restricted, gives us the estimate that we are interested in. To account for the distribution of the outcome variable, it is a good idea to specify se = \"hetero\". This way, feols uses a more robust estimation for the standard errors, accounting for the likely non-normal distribution of the error term.\nThe code also shows who you can run a logit or probit regression in R with the glm function. The main difference with the lm function is that you need to specify the family of models and the transformation link.\n\nols1 <- feols(y1 ~ x, data = solutions, se = \"hetero\")\nlogit1 <- glm(y1 ~ x, data = solutions, family = binomial(link = logit))\nprobit1 <- glm(y1 ~ x, data = solutions, family = binomial(link = probit))\nols2 <- feols(y2 ~ x, data = solutions, se = \"hetero\")\nlogit2 <- glm(y2 ~ x, data = solutions, family = binomial(link = logit))\nprobit2 <- glm(y2 ~ x, data = solutions, family = binomial(link = probit))\nmsummary(list(ols1 = ols1, logit1 = logit1, probit1 = probit1,\n              ols2 = ols2, logit2 = logit2, probit2 = probit2),\n         gof_map = gof_map, fmt = 2)\n\n\n\n \n  \n      \n    ols1 \n    logit1 \n    probit1 \n     ols2 \n     logit2 \n     probit2 \n  \n \n\n  \n    (Intercept) \n    0.89 \n    2.05 \n    1.21 \n    0.14 \n    −1.85 \n    −1.10 \n  \n  \n     \n    (0.01) \n    (0.14) \n    (0.07) \n    (0.02) \n    (0.13) \n    (0.07) \n  \n  \n    x \n    0.10 \n    2.22 \n    1.00 \n    0.57 \n    2.73 \n    1.64 \n  \n  \n     \n    (0.02) \n    (0.41) \n    (0.16) \n    (0.03) \n    (0.16) \n    (0.09) \n  \n  \n    Num.Obs. \n    1001 \n    1001 \n    1001 \n    1001 \n    1001 \n    1001 \n  \n  \n    R2 \n    0.043 \n     \n     \n    0.333 \n     \n     \n  \n\n\n\n\n\n\n\nMarginal Effects\nYou can get the best of both worlds in this simple case. We can use the non-linear models to give predictions on the probability scale with fitted and then look at the average between the treatment and the control group.\n\nsolutions %>%\n  mutate(pred_logit = fitted(logit1),\n         pred_probit = fitted(probit1)) %>%\n  summarise(logit = mean(pred_logit),\n            probit = mean(pred_probit), .by = x)\n\n# A tibble: 2 × 3\n      x logit probit\n  <int> <dbl>  <dbl>\n1     1 0.986  0.986\n2     0 0.886  0.886\n\n\nWith the regression objects, we can use the marginaleffects package to get these estimates directly.\n\nlibrary(marginaleffects)\navg_comparisons(logit1, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)  2.5 % 97.5 %\n    x    1 - 0      0.1     0.0152 6.58   <0.001 0.0705   0.13\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\navg_comparisons(probit1, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)  2.5 % 97.5 %\n    x    1 - 0      0.1     0.0152 6.58   <0.001 0.0705   0.13\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high"
  },
  {
    "objectID": "generalised/interpretation.html#solution-with-controls",
    "href": "generalised/interpretation.html#solution-with-controls",
    "title": "Interpretation",
    "section": "Solution with controls",
    "text": "Solution with controls\nThings become more complicated when we introduce control variables. Remember, the effect of a variable is not constant in a non-linear model and it depends on other parts of the model. That means that the size of effect of x depends on the value of the control variable. Below, I simulate a dataset with a discrete control variable that takes three values (-1, 0, 1) with different probabilities. We can then run our three models and report them as before.\n\nsol_controls <-\n  tibble(x = rbinom(N, 1, 0.5),\n         control = sample(c(-1, 0, 1), N, replace = TRUE,\n                          prob = c(0.5, 0.3, 0.2))) %>%\n  mutate(y = rbinom(N, 1, plogis(-2 + 3 * x + 2 * control)))\n\nols_controls <- feols(y ~ x + control, data = sol_controls,\n                      se = \"hetero\")\nlogit_controls <- glm(y ~ x + control, data = sol_controls,\n                      family = binomial(link = logit))\nprobit_controls <- glm(y ~ x + control, data = sol_controls,\n                       family = binomial(link = probit))\nmsummary(list(ols = ols_controls, logit = logit_controls,\n              probit = probit_controls),\n         gof_map = gof_map, fmt = 2)\n\n\n\n \n  \n      \n    ols \n    logit \n    probit \n  \n \n\n  \n    (Intercept) \n    0.24 \n    −1.99 \n    −1.10 \n  \n  \n     \n    (0.02) \n    (0.16) \n    (0.09) \n  \n  \n    x \n    0.42 \n    3.19 \n    1.74 \n  \n  \n     \n    (0.02) \n    (0.24) \n    (0.12) \n  \n  \n    control \n    0.29 \n    2.06 \n    1.14 \n  \n  \n     \n    (0.02) \n    (0.15) \n    (0.08) \n  \n  \n    Num.Obs. \n    1001 \n    1001 \n    1001 \n  \n  \n    R2 \n    0.418 \n     \n     \n  \n\n\n\n\n\nLet’s now calculate the effect for each value of the control variable 2. We use the same procedure but we just do it by control. We calculate the difference between the predicted probability for the control and treatment group . We also keep track of the number of observations in each cell. You can see in the printed intermediate calculation that the estimated effect differs for the three different values of the control variable. Because we want a one number summary, we can take the weighted (by number of observation) average of the effect to get the Average Marginal Effect.\n\nsol_controls %>%\n  mutate(predictions = fitted(logit_controls)) %>%\n  summarise(prob = mean(predictions), n = n(),\n            .by = c(x, control)) %>%\n  pivot_wider(values_from = c(prob, n), names_from = x) %>%\n  mutate(effect = prob_1 - prob_0, n = n_1 + n_0) %>%\n  print() %>%\n  summarise(AME = sum(effect * n)/sum(n))\n\n# A tibble: 3 × 7\n  control prob_0 prob_1   n_0   n_1 effect     n\n    <dbl>  <dbl>  <dbl> <int> <int>  <dbl> <int>\n1      -1 0.0171  0.296   249   254  0.279   503\n2       1 0.517   0.963    99   100  0.446   199\n3       0 0.120   0.768   146   153  0.648   299\n\n\n# A tibble: 1 × 1\n    AME\n  <dbl>\n1 0.422\n\n\nThere is another way of doing the calculation. We can also predict the probability assuming that an observation is in the treatment group (pred_x1) and assuming that an observation is in the control group (pred_x2). We can then calculate the Average Marginal Effect as the mean effect within the sample and we get a very similar result.\n\npred_new <- function(x = 0, control = 1){\n  plogis(predict(logit_controls, newdata = tibble(x = x, control = control)))\n}\nsol_controls %>%\n  mutate(pred_x1 = map_dbl(control, ~ pred_new(1, ..1)),\n         pred_x0 = map_dbl(control, ~ pred_new(0, ..1)),\n         effect = pred_x1 - pred_x0) %>%\n  print() %>%\n  summarise(AME = mean(effect))\n\n# A tibble: 1,001 × 6\n       x control     y pred_x1 pred_x0 effect\n   <int>   <dbl> <int>   <dbl>   <dbl>  <dbl>\n 1     0      -1     0   0.296  0.0171  0.279\n 2     1       1     1   0.963  0.517   0.446\n 3     1       0     1   0.768  0.120   0.648\n 4     0       1     0   0.963  0.517   0.446\n 5     0      -1     0   0.296  0.0171  0.279\n 6     1       1     1   0.963  0.517   0.446\n 7     0      -1     0   0.296  0.0171  0.279\n 8     0      -1     0   0.296  0.0171  0.279\n 9     1      -1     1   0.296  0.0171  0.279\n10     0      -1     0   0.296  0.0171  0.279\n# ℹ 991 more rows\n\n\n# A tibble: 1 × 1\n    AME\n  <dbl>\n1 0.422\n\n\nThe last approach is what the avg_comparisons package does. One advantage of the last approach is that it scales better if we have multiple control variables and some of them are continuous. The predictions will account for the differences in the effect because of the differences in the control variables.\n\navg_comparisons(logit_controls, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %\n    x    1 - 0    0.422     0.0225 18.8   <0.001 0.378  0.466\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\navg_comparisons(probit_controls, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %\n    x    1 - 0    0.419     0.0229 18.3   <0.001 0.374  0.464\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the linear model ols_controls recovers the average marginal effect directly in the regression. If you are interested in a treatment effect of a treatment compared to a control group, in my opinion, you should just use a linear model with robustly estimated standard errors. The linear model makes it easy to incorporate fixed effects, deal with panel data, and expand the model to two-stage-least-squares or difference-in-differences while giving the estimate that you care about.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe average marginal effect calculation highlights that the effect that we estimate depends on the values of the control variables. As a result, we need to be careful about extrapolating the effect to different settings where the control variables might take different values.\n\n\n\nSolution with continuous variable\nThe case of a continuous variable x is slightly more complicated because we already know that the relation between the outcome variable y and x is not perfectly linear. However, even with a continuous outcome variable, we also have to make the assumption of a linear relation. However, if we can make the assumption that the relation between y and x is roughly linear, the OLS estimate will be a good approximation of the average marginal effect as we can see below. As before, we can easily incorporate all the benefits of using feols while getting the estimate that we are targeting.\n\nsol_continuous <-\n  tibble(x = rnorm(N, 0, 1),\n         control = rnorm(N, 0, 1)) %>%\n  mutate(y = rbinom(N, 1, plogis(-2 + 3 * x + 2 * control)))\n\nols_continuous <- feols(y ~ x + control, data = sol_continuous,\n                        se = \"hetero\")\nlogit_continuous <- glm(y ~ x + control, data = sol_continuous,\n                      family = binomial(link = logit))\nprobit_continuous <- glm(y ~ x + control, data = sol_continuous,\n                       family = binomial(link = probit))\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nmsummary(list(ols = ols_continuous, logit = logit_continuous,\n              probit = probit_continuous),\n         gof_map = gof_map, fmt = 2)\n\n\n\n \n  \n      \n    ols \n    logit \n    probit \n  \n \n\n  \n    (Intercept) \n    0.30 \n    −2.41 \n    −1.35 \n  \n  \n     \n    (0.01) \n    (0.18) \n    (0.09) \n  \n  \n    x \n    0.26 \n    3.46 \n    1.92 \n  \n  \n     \n    (0.01) \n    (0.25) \n    (0.13) \n  \n  \n    control \n    0.19 \n    2.43 \n    1.35 \n  \n  \n     \n    (0.01) \n    (0.19) \n    (0.10) \n  \n  \n    Num.Obs. \n    1001 \n    1001 \n    1001 \n  \n  \n    R2 \n    0.487 \n     \n     \n  \n\n\n\n\n\nTo get the average marginal effect with a continuous x, we use the avg_slopes function from marginaleffects. The function does something similar as avg_comparisons does for the discrete x. It will predict the probability of the outcome y for x and for x + 0.0013 and then take the difference. In other words, the function will approximate the estimate slope between y and x for each observations. The average marginal effect is again the average for the whole sample.\nThe fact that the calculation needs to be done for every observation can be a disadvantage of you have a lot of observations and a lot of effects to estimate.\n\navg_slopes(logit_continuous, variables = \"x\")\n\n\n Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %\n    x    0.265    0.00703 37.6   <0.001 0.251  0.278\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high \n\navg_slopes(probit_continuous, variables = \"x\")\n\n\n Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %\n    x    0.263    0.00703 37.4   <0.001 0.249  0.276\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high \n\n\nAll estimates point to the following effect: if x increases by .01, we expect the probability of y to increase by about .25% in our sample."
  },
  {
    "objectID": "generalised/interpretation.html#further-information",
    "href": "generalised/interpretation.html#further-information",
    "title": "Interpretation",
    "section": "Further information",
    "text": "Further information\nThere is a lot more that can be done to with marginal effects and they can also be used to estimate non-linear effects in linear models. Andrew Heiss has a good overview of the theory and the marginaleffects package has excellent vignettes on how to implement the different possibilities."
  },
  {
    "objectID": "generalised/interpretation.html#the-case-for-a-poisson-model",
    "href": "generalised/interpretation.html#the-case-for-a-poisson-model",
    "title": "Interpretation",
    "section": "The Case for a Poisson Model",
    "text": "The Case for a Poisson Model\nThe code should not be shown\n\nset.seed(830323)\nN <- 5000\nn_firm <- 500\noverdispersion <- 2 # 0.5 is more typical for real data\nhetero <- 1\nbeta <- - 0.2\nfirm <-\n  tibble(\n    firm = 1:100,\n    fixed = rnorm(100, 0, .5))\npanel <-\n  tibble(\n    firm = sample(1:100, N, replace = TRUE),\n    x2 = rnorm(N, 0, 2),\n    x1_noise = rnorm(N, 0, .5),\n    noise = rnorm(N, 0, .5)) %>%\n  left_join(firm) %>%\n  mutate(\n    x1 = fixed + x1_noise,\n    yhetero = exp(beta * x1 + x2 + hetero * x1 * noise),\n    ydiscrete = rnbinom(n = N, mu = exp(beta * x1 + x2),\n                        size = 1/overdispersion),\n    ycontinuous = rchisq(n = N, ydiscrete))\n\nJoining with `by = join_by(firm)`\n\n\n\npoisson_disc <- feglm(ydiscrete ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\npoisson_cont <- feglm(ycontinuous ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\nlog_plus1_disc <- feols(log(ydiscrete + 1) ~ x1 + x2 | firm,\n                        data = panel)\nlog_plus1_cont <- feols(log(ycontinuous + 1) ~ x1 + x2 | firm,\n                        data = panel)\nols_disc <-feols(ydiscrete ~ x1 + x2 | firm, data = panel)\nols_cont <-feols(ycontinuous ~ x1 + x2 | firm, data = panel)\nrate_disc <-feols(I(ydiscrete / x2) ~ x1 | firm, data = panel)\nrate_cont <-feols(I(ycontinuous / x2) ~ x1 | firm, data = panel)\nmsummary(list(poisson_disc, log_plus1_disc, ols_disc, rate_disc))\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n      (4) \n  \n \n\n  \n    x1 \n    −0.283 \n    −0.061 \n    −2.864 \n    6.562 \n  \n  \n     \n    (0.088) \n    (0.023) \n    (1.057) \n    (4.550) \n  \n  \n    x2 \n    1.003 \n    0.371 \n    6.581 \n     \n  \n  \n     \n    (0.041) \n    (0.010) \n    (0.811) \n     \n  \n  \n    Num.Obs. \n    5000 \n    5000 \n    5000 \n    5000 \n  \n  \n    R2 \n    0.741 \n    0.423 \n    0.094 \n    0.020 \n  \n  \n    R2 Adj. \n    0.740 \n    0.411 \n    0.075 \n    0.000 \n  \n  \n    R2 Within \n    0.705 \n    0.407 \n    0.075 \n    0.002 \n  \n  \n    R2 Within Adj. \n    0.705 \n    0.406 \n    0.075 \n    0.002 \n  \n  \n    AIC \n    48585.4 \n    13123.6 \n    52565.7 \n    57594.1 \n  \n  \n    BIC \n    49250.1 \n    13788.3 \n    53230.4 \n    58252.4 \n  \n  \n    RMSE \n    26.10 \n    0.88 \n    45.48 \n    75.21 \n  \n  \n    Std.Errors \n    by: firm \n    by: firm \n    by: firm \n    by: firm \n  \n  \n    FE: firm \n    X \n    X \n    X \n    X \n  \n\n\n\n\n\n- The OLS version are more noisy. log+1 is biased."
  },
  {
    "objectID": "generalised/introduction.html",
    "href": "generalised/introduction.html",
    "title": "Introduction and Theory",
    "section": "",
    "text": "The question in this section is how we should deal with outcome variables that are not continuous or restricted to be positive. I will restrict the applications to the most common examples: (1) binary outcome variables and (2) positive outcome variables. Binary outcome variables would be for instance the decision to disclose information, or merge a company. Positive outcome variables are variables such as employee salaries or the market value of a company.\nThis is a fairly contentious topic where different literature streams have different expectations and norms for what is appropriate. This is by no means a complete overview of the topic. My aim is to give you my perspective and to give you enough tools to be able to decide what is most appropriate for your research and to explain to assessors why you made those choices.\nThe basic problem is the following. In our typical linear model, the outcome variable can take on any value between \\(-\\infty\\) and \\(+\\infty\\).\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nIf we want to make sure that \\(y_i\\) is restricted to a certain to domain such as between 0 and 1, we can use a transformation function on the linear model. These transformed linear models are often called generalised linear models.\n\\[\ny_i = g(\\alpha + \\beta x_i + \\epsilon_i)\n\\]\nYou can also think of the inverse of the transformation function as transforming the outcome so that it can take all positive and negative values. This is implicitly what we have done earlier when we used the logarithmic transformation.\n\\[\ng^{-1}(y_i) = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nThis should remind you of our earlier discussion on what the appropriate measure is for the pay-performance sensitivity. In the second week, we discussed the importance of theory to determine what the right measure is according to our research question. In this week, we are asking the question whether we are interested in \\(y_i\\) or in the transformation \\(g^{-1}(y_i)\\).\nThe discussion should also remind you of the difference between prediction and parameter estimation at the start of the machine learning chapter. The issue of the mismatch between the domain of the linear model and the outcome variable is less of an issue if we are really interested in estimating the parameter \\(\\beta\\). It really is a problem if we are trying to predict the outcome variable \\(y\\) because we know that we are going to get predictions that are impossible. In the reminder of this chapter I will focus on parameter estimation because that is a more common research question in finance and accounting research. That does not mean that prediction of restricted outcome variables is not important. Indeed, regularised versions of generalised linear models for binary outcome variables are popular classification models. They are fairly easy to implement in the tidymodels framework we have used in the machine learning application."
  },
  {
    "objectID": "generalised/introduction.html#setup",
    "href": "generalised/introduction.html#setup",
    "title": "Introduction and Theory",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "generalised/introduction.html#transformation",
    "href": "generalised/introduction.html#transformation",
    "title": "Introduction and Theory",
    "section": "Transformation",
    "text": "Transformation\nIn this section, I will visualise the most common transformation functions in the accounting and finance literature. I start with the logistic and probit transformations that are most commonly used to model binary variables. We can think of those variables as having two values either \\(y_i = 1\\) or \\(y_i = 0\\). In this case, we can model the probability \\(P(y_i = 1) = g(\\alpha + \\beta x_i + \\epsilon_i)\\) and then use a binomial distribution to predict 1s and 0s. The logistic and probit transformation are two possible options for the function \\(g\\).\n\nLogistic Transformation\nThe logistic transformation transforms the linear scale, \\(z_i = \\alpha + \\beta x_i + \\epsilon_i\\), to \\(\\frac{e^{z_i}}{1+ e^{z_i}}\\). R has an inbuilt function plogis that does the transformation for us but you can obviously just write the transformation yourself. The code below shows that you get identical results.\n\nN <- 1001\nlogit <-\n  tibble(\n    linear_scale = seq(from = -10, to = 10, length.out = N)) %>%\n  mutate(\n    logit_probability1 = exp(linear_scale)/(1 + exp(linear_scale)),\n    logit_probability2 = plogis(linear_scale),\n  )\nglimpse(logit)\n\nRows: 1,001\nColumns: 3\n$ linear_scale       <dbl> -10.00, -9.98, -9.96, -9.94, -9.92, -9.90, -9.88, -…\n$ logit_probability1 <dbl> 4.539787e-05, 4.631492e-05, 4.725050e-05, 4.820498e…\n$ logit_probability2 <dbl> 4.539787e-05, 4.631492e-05, 4.725050e-05, 4.820498e…\n\nggplot(logit, aes(y = logit_probability1, x = linear_scale)) +\n  geom_line() +\n  xlab(\"z\") + ylab(\"logistic(z)\")\n\n\n\n\nIn the figure, you can see how the transformed value stays between 0 and 1, exactly as we would want from a probability. Also notice how the impact of an increase of 1 on the linear scale \\(z\\) varies depending on the starting value of \\(z\\). This is because the logistic transformation is an non-linear transformation and it makes the interpretation of the the coefficients in our model more difficult.\n\n\nCumulative Normal or Probit\nThere is another function that is commonly used to transform the linear scale to the probability scale, the cumulative normal or probit function. We need to take a short detour to the normal distribution to explain the cumulative normal distribution to explain this function. You can see a normal distribution below where the lower 90% of the distribution is filled in yellow while the upper 10% is filled in blue. The value for \\(x\\) associated with the 90% is approximately 1.28. That means that we can associate a probability with each value of \\(x\\) which tells us what the probability is that a randomly generated normal value is smaller than \\(x\\). This function is the probit function.\n\ntibble(x = seq(from = -3, to = 3, length.out = N)) %>%\n  mutate(dnorm = dnorm(x),\n         fill = if_else(x < qnorm(0.90), \"below\", \"above\")) %>%\n  ggplot(aes(y = dnorm, x = x, fill = fill)) +\n  geom_area() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Normal Distribution\")\n\n\n\n\nIn the next code, I show the cumulative probability function for a normal distribution with mean 0 and standard deviation 1. You can see that it look quite similar to the logistic transformation. In the next, section we will compare the two transformations directly.\n\ncumul_norm  <- logit %>%\n  mutate(cumul_norm_prob = pnorm(linear_scale))\nggplot(cumul_norm, aes(y = cumul_norm_prob, x = linear_scale)) +\n  geom_line() +\n  xlab(\"z\") + ylab(\"cumulative_normal(z)\")\n\n\n\n\n\n\nComparison\nThe main reason to show the comparison is to highlight that when we divide \\(z\\) by 1.6 the probit transformation is very similar to the logistic transformation. This probit transformation is also equivalent to the cumulative normal probability for a normal distribution with mean 0 and standard deviation 1.6. In other words, the difference between the logistic or probit regression is often not very large.\n\ncombination <- cumul_norm %>%\n  mutate(cumul_norm_adj = pnorm(linear_scale/1.6)) %>%\n  select(-logit_probability2) %>%\n  pivot_longer(c(logit_probability1, cumul_norm_prob, cumul_norm_adj))\nggplot(combination, aes(y = value, x = linear_scale,\n                        colour = name)) +\n  scale_color_viridis_d() +\n  geom_line() +\n  xlab(\"z\") + xlab(\"g(z)\") + ggtitle(\"Binary Distribution\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe choice between a logistic regression or a probit regression is often not consequential. If your research question requires you to model a binary variable, the choice between probit or logistic regression can be based on what the preferred choice is in your research area.\n\n\n\n\nPoisson\nThe poisson distribution (rpois) is originally developed to model the number of independent events with a fixed rate happening in a given amount of time. It is the general workhorse model for any count variable. As, you can see in the figure below, you can think of the poisson distribution as discrete random distribution (the dots) around the the exponentiated linear model (the line). It will be important further on to think of a poisson regression as a model for\n\\[ g^{-1}(E(y_i | x_i)) = \\alpha + \\beta x_i\\]\nwhere \\(g^{-1}(z) = log(z)\\) and \\(g(z) = exp(z)\\). That is the logarithm of the expected value of \\(y_i\\) is given by \\(\\alpha + \\beta x_i\\). Moreover, in contrast the exponent, the poisson distribution can give 0 values1.\n\npoisson <-\n  tibble(linear_scale = seq(from = -3, to = 3, length.out = N)) %>%\n  mutate(exp_scale = exp(linear_scale),\n         poisson = rpois(N, exp_scale))\nggplot(poisson, aes(y = exp_scale, x = linear_scale)) +\n  geom_line() +\n  geom_point(aes(y = poisson)) +\n  xlab(\"z\") + ylab(\"g(z)\") + ggtitle(\"Poisson Distribution\")"
  },
  {
    "objectID": "generalised/poisson.html",
    "href": "generalised/poisson.html",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nset.seed(830323)\nN <- 4000\nn_firm <- 500\ngof_omit <- \"Adj|Lik|IC|RMSE\""
  },
  {
    "objectID": "generalised/poisson.html#introduction",
    "href": "generalised/poisson.html#introduction",
    "title": "Poisson Regression",
    "section": "Introduction",
    "text": "Introduction\nIn the previous section, I made the case for using OLS regressions even when the outcome variable is a discrete variable. This is especially true in the case that we are interested in estimating the effect of an intervention. The coefficient that we are getting can easily be interpreted as the difference in the probability of getting one outcome over the other."
  },
  {
    "objectID": "generalised/poisson.html#type-of-outcome-data.",
    "href": "generalised/poisson.html#type-of-outcome-data.",
    "title": "Poisson Regression",
    "section": "Type of outcome data.",
    "text": "Type of outcome data.\nThe type of outcome variables this argument applies to are variables that are the result of a stable multiplicative process. In earlier lectures, I have made the point that we can think of the contribution of a CEO to the firm as a multiplicative effect. The CEO’s ability has a larger contribution to firm value if they are working in a larger firm. Probably the most basic example in finance is compound interest. If we start with $100 and the yearly interest rate is 5%, we can write our wealth as a function of time (in number of years).\n\\[\nW(T) = 100 (1 + 0.05)^T\n\\]\nNow, imagine that we divide the interest and the number of years by \\(N > 1\\). That is, imagine that we pay an interest of \\(\\frac{0.05}{N}\\) every period with \\(N\\) periods per year.\n\\[\nW(T) = 100 (1 + \\frac{0.05}{N})^{\\frac{T}{N}}\n\\]\nIn the where we have a lot of small periods (\\(N \\to \\infty\\)), we can write our wealth as follows.\n\\[\nW(T) = 100 e^{0.05 T}\n\\]\nIn general, if we have a variable \\(V\\) that is the results of a multiplicative process of small components with a rate of change \\(r\\) and \\(S\\) steps and a starting value \\(V_0\\), we can write \\(V(S)\\) as follows.\n\\[\n\\begin{aligned}\nV(S) &= V_0 e^{rS} \\\\\n\\textrm{log} (V(S)) &= \\textrm{log} (V_0) + rS \\\\\n\\frac{V(S)}{V_0} &= e^{rS} \\\\\n\\textrm{log} \\frac{V(S)}{V_0} &= rS \\\\\n\\end{aligned}\n\\]\nThe Poisson distribution itself is the discrete equivalent of this idea. The distribution models the number of events, \\(V(S)\\), for a population, \\(V_0\\), when the underlying process follows a fixed occurrence rate \\(r\\) per unit of time and per element of the population. The theoretical case for the Poisson regression is that the coefficients on the linear scale targets, \\(r\\), the instantaneous rate of change or the rate of occurrences 1 which has a meaningful economic interpretation for non-negative variables such as number of corporate patents, carbon emissions, or distance between companies (Cohn, Liu, and Wardlaw 2022). They also make sense for variables that naturally grow like firm size, revenues, or CEO wealth and income."
  },
  {
    "objectID": "generalised/poisson.html#the-case-for-a-poisson-regression",
    "href": "generalised/poisson.html#the-case-for-a-poisson-regression",
    "title": "Poisson Regression",
    "section": "The Case for a Poisson regression",
    "text": "The Case for a Poisson regression\n\nIntuition\nThe statistical case for the Poisson regression is extensively documented in Cohn, Liu, and Wardlaw (2022). Here I will just list the main advantages and shortly demonstrate them with a simulated example. There are a number of alternative approaches that we could use to model these type of variables. The obvious alternative is to model the variable \\(\\textrm{log}(V)\\) in a linear regression. However, this does not work if we have a lot of observations where \\(V = 0\\). One proposed solution in the literature is then to use the transformation \\(\\textrm{log}(V + 1)\\). Cohn, Liu, and Wardlaw (2022) show that the coefficients with the log plus 1 approach are hard to interpret and can have a different sign than with a poisson regression. A further strength of the Poisson approach is that it allows for the inclusion of fixed effects in the regression without changing the interpretation of the coefficients. Remember that with generalised regressions the effect depends on other parts of the model if we are interested in the non-transformed scale, \\(V\\). The concession that we have to make is that we are interpreting the effects on the transformed scale of \\(r\\) and not on the scale of \\(V_0\\). As I explained above, this can often be a reasonable assumption to make.\nOne criticism of the Poisson regression is that it assumes that the variation around the mean is proportional to the mean. However, if this assumption does not hold, the estimates of the coefficients will not be biased and (cluster) robust standard errors are robust against violations of this assumption.\nA last point is that, just like in the binomial case, we could just use a linear model on \\(V\\) or \\(\\frac{V}{V_0}\\). However, because with multiplicative effects (or exponential growth) \\(V\\) can vary by multiple orders of magnitude, the estimates of the coefficients can be noisy and have large standard errors.\nCohn, Liu, and Wardlaw (2022) retest six published papers that use a log transformed dependent variable and compare it to a Poisson regression. They find that in all six cases the coefficient is markedly different and in three cases the sign changes. Moreover, the change in the coefficient is larger than removing any control variables. The type of regression matters more than the control variables.\n\n\n\n\n\n\nNote\n\n\n\nIn my view, Cohn, Liu, and Wardlaw (2022) makes a strong case that for a lot of non-negative outcome variables in accounting and finance research designs, the Poisson regression should be the default. This is also my recommendation.\n\n\n\n\nSimulation\nIn the simulation below, I create a dataset for a discrete and a continuous \\(y\\) where the expected value of \\(y\\) is given by\n\\[\nE(y|x_1, x_2) = e^{-0.3 x_1 + x_2}\n\\]\nThis is the data generating process that we associate with a multiplicative process or from a Poisson count process. We will be interested in estimating the effect of \\(x_1\\) on \\(y\\) which in the Poisson regression should give an estimate of \\(-0.3\\).\nThe data generating process also includes fixed effects and additional variation around this expected value which violates the assumptions of the naive Poisson regression. The details of this approach are not important and require knowledge of the negative binomial distribution(rnbinom) to get count data and the chi-squared distribution (rchisq).\n\noverdispersion <- 0.5\nhetero <- 1\nbeta <- - 0.3\nfirm <-\n  tibble(\n    firm = 1:n_firm,\n    fixed = rnorm(n_firm, 0, .5))\npanel <-\n  tibble(\n    firm = sample(1:100, N, replace = TRUE),\n    x1_noise = rnorm(N, 0, .5),\n    noise = rnorm(N, 0, .5)) %>%\n  left_join(firm) %>%\n  mutate(\n    x1 = fixed + x1_noise,\n    x2 = rnorm(N, x1 + x1^2, 2),\n    ydiscrete = rnbinom(n = N, mu = exp(beta * x1 + x2),\n                        size = 1/overdispersion),\n    ycontinuous = rchisq(n = N, ydiscrete))\n\nJoining with `by = join_by(firm)`\n\n\nI plot the data on log + 1 scale and you can see that the figure looks distorted or weird for lower values of ydiscrete or ycontinuous. This is by now means proof but it is indicative of some of the problems with the log or log plus 1 transformation.\n\npanel %>%\n  pivot_longer(c(ydiscrete, ycontinuous), values_to = \"y\") %>%\n  ggplot(aes(y = y + 1, x = x1)) +\n  geom_point() +\n  scale_y_log10() +\n  facet_wrap(~name)\n\n\n\n\nFor both the continuous and count variable, I run four regression models with fixed effects.\n\nThe Poisson regression with \\(y\\) as dependent variable.\nAn OlS regression with \\(\\textrm{log}(y + 1)\\) as dependent variable.\nAn OLS regression with \\(y\\) as dependent variable. Because this regression is not on the rate of change scale, we do not expect a coefficient of -0.3 here. The main purpose is to show how noisy the estimate is.\nAn OLS regression with \\(\\frac{y/x_2\\) as dependent variable. This approach will suffer from the same noisy estimates.\n\n\npoisson_disc <- feglm(ydiscrete ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\npoisson_cont <- feglm(ycontinuous ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\nlog_plus1_disc <- feols(log(ydiscrete + 1) ~ x1 + x2 | firm,\n                        data = panel)\nlog_plus1_cont <- feols(log(ycontinuous + 1) ~ x1 + x2 | firm,\n                        data = panel)\nols_disc <-feols(ydiscrete ~ x1 + x2 | firm, data = panel)\nols_cont <-feols(ycontinuous ~ x1 + x2 | firm, data = panel)\nrate_disc <-feols(I(ydiscrete / x2) ~ x1 | firm, data = panel)\nrate_cont <-feols(I(ycontinuous / x2) ~ x1 | firm, data = panel)\nmsummary(list(poisson = poisson_disc, log1 = log_plus1_disc,\n              ols = ols_disc, rate = rate_disc),\n         gof_omit = gof_omit)\n\n\n\n \n  \n      \n    poisson \n    log1 \n    ols \n    rate \n  \n \n\n  \n    x1 \n    −0.261 \n    −0.081 \n    12.167 \n    5.239 \n  \n  \n     \n    (0.095) \n    (0.030) \n    (9.304) \n    (3.143) \n  \n  \n    x2 \n    0.983 \n    0.541 \n    21.597 \n     \n  \n  \n     \n    (0.021) \n    (0.012) \n    (4.107) \n     \n  \n  \n    Num.Obs. \n    4000 \n    4000 \n    4000 \n    4000 \n  \n  \n    R2 \n    0.923 \n    0.705 \n    0.110 \n    0.024 \n  \n  \n    R2 Within \n    0.902 \n    0.684 \n    0.081 \n    0.000 \n  \n  \n    Std.Errors \n    by: firm \n    by: firm \n    by: firm \n    by: firm \n  \n  \n    FE: firm \n    X \n    X \n    X \n    X \n  \n\n\n\n\nmsummary(list(poisson = poisson_cont, log1 = log_plus1_cont,\n              ols = ols_cont, rate = rate_cont),\n         gof_omit = gof_omit)\n\n\n\n \n  \n      \n    poisson \n    log1 \n    ols \n    rate \n  \n \n\n  \n    x1 \n    −0.259 \n    −0.087 \n    12.037 \n    7.140 \n  \n  \n     \n    (0.095) \n    (0.035) \n    (9.336) \n    (4.219) \n  \n  \n    x2 \n    0.979 \n    0.536 \n    21.686 \n     \n  \n  \n     \n    (0.020) \n    (0.012) \n    (4.076) \n     \n  \n  \n    Num.Obs. \n    4000 \n    4000 \n    4000 \n    4000 \n  \n  \n    R2 \n    0.916 \n    0.653 \n    0.112 \n    0.023 \n  \n  \n    R2 Within \n    0.893 \n    0.630 \n    0.082 \n    0.000 \n  \n  \n    Std.Errors \n    by: firm \n    by: firm \n    by: firm \n    by: firm \n  \n  \n    FE: firm \n    X \n    X \n    X \n    X \n  \n\n\n\n\n\nYou can see that the Poisson regression is pretty close to recover the true estimate of -0.3 both for the count as for the continuous case with small standard errors. The log plus 1 approach gives a considerably different estimate and the OLS estimates are positive instead of negative but with large standard errors."
  },
  {
    "objectID": "generated/introduction.html",
    "href": "generated/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This last section is going to bring everything full circle to an extent. The topic of this section is generated variables which are variables that are the result of a regression. For instance, we could use the residuals or predicted values of one regression and use them as variables in an other regression. This is the idea behind two-stage least squares with an instrumental variable, models of abnormal returns, models of abnormal accruals and many more in the accounting and finance literature.\nThe issue is that these generated variables might violate some of the implicit or explicit assumptions in the second regression model. I will go over some of the issues that are specific to certain models such as two-stage least squares on this page and the use of generated variables on the next page. The main message of this section is that the properties of these models that exists of more than one regression are not always well understood: the estimates could be biased or the standard software packages could give the wrong standard errors. The solutions I propose in this setting is to use simulations of many datasets to understand whether an approach gives biased estimates and use bootstrapped standard errors to check whether the multi-step approach gives reasonable standard errors. I use plenty of coding examples to illustrate these points."
  },
  {
    "objectID": "generated/introduction.html#setup",
    "href": "generated/introduction.html#setup",
    "title": "Introduction",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(modelsummary)\ngof_map <- c(\"nobs\", \"r.squared\")\nlibrary(fixest)\nlibrary(bayesboot)\ni_am(\"generated/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nset.seed(230383)"
  },
  {
    "objectID": "generated/introduction.html#an-example-2sls",
    "href": "generated/introduction.html#an-example-2sls",
    "title": "Introduction",
    "section": "An example: 2SLS",
    "text": "An example: 2SLS\nIn this example, we are using a standard setting where an instrumental variable, iv, is used to identify the effect of a variable x on y in the presence of annoying confounding. Note that I generate the data in two steps. The initial tibble step simulates the variables that are exogenous and not affected by other parts of the model while the second mutate step simulates the endogenous variables that are causally affected by other parts of the model.\n\nN <- 1e4\ndata_2sls <-\n  tibble(\n    iv = rbinom(N, 1, 0.5),\n    annoying = rnorm(N)\n  ) %>%\n  mutate(\n    x = rnorm(N, 2 * iv - 3 * annoying, 5),\n    y = rnorm(N, 3 * annoying + x, 5)\n  )\n\nNext, we run four different models with this data.\n\nlm1 is the confounded regression.\nlm2 controls for the annoying confounding factor and provides the best possible answer to the question. Unfortunately, in reality we often are not sure that we have all the confounding factors.\ntsls is the standard two-stage least squares estimate with iv as the instrumental variable.\ntsls_manual1 and tsls_manual2 run the two regressions of two-stage least squares manually.\n\n\nlm1 <- lm(y ~ x, data = data_2sls)\nlm2 <- lm(y ~ x + annoying, data = data_2sls)\ntsls <- feols(y ~ 1 | 0 | x ~ iv, data = data_2sls)\ntsls_manual1 <- lm(x ~ iv, data = data_2sls)\ndata_2sls <- data_2sls %>%\n  mutate(fit_x = fitted(tsls_manual1))\ntsls_manual2 <- lm(y ~ fit_x, data = data_2sls)\ncoef_map <- c('x' = 'x', 'annoying' = 'annoying',\n              'fit_x' = 'x')\nmsummary(list(confounded = lm1, ideal = lm2, tsls = tsls,\n              manual = tsls_manual2),\n         gof_map = gof_map, coef_map = coef_map)\n\n\n\n \n  \n      \n    confounded \n    ideal \n    tsls \n    manual \n  \n \n\n  \n    x \n    0.747 \n    1.007 \n    1.040 \n    1.040 \n  \n  \n     \n    (0.010) \n    (0.010) \n    (0.061) \n    (0.074) \n  \n  \n    annoying \n     \n    2.970 \n     \n     \n  \n  \n     \n     \n    (0.058) \n     \n     \n  \n  \n    Num.Obs. \n    10000 \n    10000 \n    10000 \n    10000 \n  \n  \n    R2 \n    0.378 \n    0.506 \n    0.320 \n    0.019 \n  \n\n\n\n\n\nIn the results, you can notice the following. The confounded regression is biased. We do not get the correct coefficient of \\(\\beta = 1\\). The ideal regression estimates the coefficient precisely with a low standard error. Despite the high number of observations (10^{4}), the two-stage least squares estimates also have an unbiased estimate but the standard error is substantially larger. The manual two-stage least squares model gets exactly the same coefficient but the standard error is larger again.\nIt turns out that the standard error is wrong in the manual version (Gelman and Hill 2006, 223). That is why we should use the fixest package to estimate the two-stage least squares model. The problem is that in our second stage regression we use fit_x as the independent variable which is the predicted value of x in the first stage regression. When lm calculates the standard error of the coefficient, it uses the standard deviation of the residuals in the regression, i.e. the difference between y and the predicted y based on x_fit. However, we know that we should actually use x to predict y. So, we need to adjust the standard errors of the coefficient and that is what the following code does manually.\nThe following code shows how to do that adjustment 1. First, we get the coefficient table from the summary which is just a matrix, where we can access the values based on the name or row/column number of the values that we need. I am extracting the coefficient and the standard error that we are interested in. We can also extract the unadjusted standard deviation of the residuals which is called sigma in the summary. Finally, we calculate the adjusted standard deviation if we use x instead of fit_x to predict y. The last line adjusts the standard error.\n\ncoef_table <- summary(tsls_manual2)$coef\nintercept <- coef_table[\"(Intercept)\", 1]\nbeta <- coef_table[\"fit_x\", 1]\nse <- coef_table[\"fit_x\", 2]\nsd_unadj <- summary(tsls_manual2)$sigma\nsd_adj <- data_2sls %>%\n  mutate(e = y - intercept - beta * x) %>%\n  summarise(sd = sd(e)) %>%\n  pull(sd)\nse * sd_adj / sd_unadj\n\n[1] 0.06145269\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith two-stage least squares, statisticians and econometricians have figured out the correct adjustments to standard errors and they are build into the software packages. For more complicated models with multiple steps this might not always be possible. One possible solution is to bootstrap the standard errors."
  },
  {
    "objectID": "generated/introduction.html#bayesian-bootstrap",
    "href": "generated/introduction.html#bayesian-bootstrap",
    "title": "Introduction",
    "section": "Bayesian Bootstrap",
    "text": "Bayesian Bootstrap\nI have explained the bootstrap before. In this section, I introduce the Bayesian Bootstrap and it’s R implementation 2. In the traditional bootstrap we resample observations from the data with replacement so that we create new datasets which are similar but different from the original data. The disadvantage of the traditional approach is that the implicit weight on an observation in each resampled data is discrete and can be 0. The Bayesian bootstrap bootstrap creates explicit weights based on a Dirichlet distribution which gives a weight between 0 and 1 for each observation so that the sum of the weights equals 1.\nWe do not need to know this to use the Bayesian bootstrap. We can just use the bayesboot package. The only thing we need to do is to create a function that returns the estimate that we are interested in based on the data and the weights. We can use the bootstrap function and tell it to start with the data_2sls, resample it R = 1000 times, calculate the beta with get_beta, and use weights in that calculation.\n\nget_beta <- function(data, weights){\n  tsls_manual1 <- lm(x ~ iv, data = data, weights = weights)\n  data$fit_x <- fitted(tsls_manual1)\n  tsls_manual2 <- lm(y ~ fit_x, data = data, weights = weights)\n  beta <- coefficients(tsls_manual2)[\"fit_x\"]\n  return(beta)\n}\nget_beta(data_2sls, weights = rep(1/N, times = N))\n\n  fit_x \n1.03956 \n\nbootstrap <- bayesboot(data_2sls, get_beta, R = 1000,\n                       use.weights = TRUE)\nsummary(bootstrap)\n\nBayesian bootstrap\n\nNumber of posterior draws: 1000 \n\nSummary of the posterior (with 95% Highest Density Intervals):\n statistic     mean         sd   hdi.low hdi.high\n     fit_x 1.038238 0.06325506 0.9135051 1.163498\n\nQuantiles:\n statistic     q2.5%      q25%  median     q75%   q97.5%\n     fit_x 0.9146771 0.9978153 1.03673 1.080914 1.166789\n\nCall:\n bayesboot(data = data_2sls, statistic = get_beta, R = 1000, use.weights = TRUE)\n\n\nIf you want to calculate the Bayesian bootstrap by hand, I give the code below. It’s a good introduction on how to run efficient simulations with the tidyverse 3. I need to slightly change the function because we are going to put the simulated weights in the data. The next part is to actual create nsim versions of the data. The trick here is to say that we want the data data_2sls where we repeat each row nsim times.\nThe rest of the code follows more easy to understand patterns. We first create a sim variable so that we can keep track of different data simulations. In this case, the data is the same but we create new weights for each simulation. The weights are derived from a Dirichlet distribution which you can generate by an exponential distribution divided by the sum of the generated values (per simulation). We only keep the variables that are necessary in our get_beta_2 function. Then we create a dataset per simulation with the nest function and calculate the estimate in the last mutate step. And that is the Bayesian bootstrap as a simulation exercise.\n\nget_beta_2 <- function(data){\n  tsls_manual1 <- lm(x ~ iv, data = data, weights = weights)\n  data$fit_x <- fitted(tsls_manual1)\n  tsls_manual2 <- lm(y ~ fit_x, data = data, weights = weights)\n  beta <- coefficients(tsls_manual2)[\"fit_x\"]\n  return(beta)\n}\nnsim <- 1000\nnrows <- nrow(data_2sls)\nbootstrap2 <-\n  data_2sls[rep(1:nrows, times = nsim),] %>%\n  mutate(sim = rep(1:nsim, each = nrows),\n         raw_weights = rexp(nsim *  nrows, 1)) %>%\n  mutate(weights = raw_weights/sum(raw_weights), .by = sim) %>%\n  select(sim, iv, x, y, weights) %>%\n  nest(.by = sim) %>%\n  mutate(beta = map_dbl(data, get_beta_2)) %>%\n  print()\n\n# A tibble: 1,000 × 3\n     sim data                   beta\n   <int> <list>                <dbl>\n 1     1 <tibble [10,000 × 4]> 0.932\n 2     2 <tibble [10,000 × 4]> 1.15 \n 3     3 <tibble [10,000 × 4]> 0.994\n 4     4 <tibble [10,000 × 4]> 1.01 \n 5     5 <tibble [10,000 × 4]> 1.06 \n 6     6 <tibble [10,000 × 4]> 1.12 \n 7     7 <tibble [10,000 × 4]> 1.01 \n 8     8 <tibble [10,000 × 4]> 1.04 \n 9     9 <tibble [10,000 × 4]> 1.07 \n10    10 <tibble [10,000 × 4]> 1.08 \n# ℹ 990 more rows\n\nsummarise(bootstrap2,\n          estimate = mean(beta),\n          se = sd(beta))\n\n# A tibble: 1 × 2\n  estimate     se\n     <dbl>  <dbl>\n1     1.04 0.0622"
  },
  {
    "objectID": "generated/inverse_probability_weight.html",
    "href": "generated/inverse_probability_weight.html",
    "title": "IPW",
    "section": "",
    "text": "The previous pages highlighted the dangers to coefficients and standard errors of using two-step approaches in the research design. If they are done well, they can benefit your analysis and make it more robust. This is the general idea of double robust estimators.\nSpecifically, we are going to look at a situation where we have a binary treatment-control x and an outcome of interest y with two confounding factors z1 and z2 that have a complicated effect on x and y. The two-step approach that we are using is combining matching with regression. In the matching step, we first estimate the propensity, \\(p\\), that an observation belongs to the treatment group. Then, we will use the inverse predicted propensity to weight the observations in the regression.\nSpecifically, we want to put less weight on observations that are in the treatment group and that we could expect to be in the treatment group. So, the weight for those observations will be \\(\\frac{1}{p}\\) in the regression. We will also put less weight on the observations in the treatment group, so we weight them by \\(\\frac{1}{1 - p}\\). The intuition is that we want to put more weight on observations that are not expected to be control or treatment groups because we assume that this unexpected assignment is because of some random variation 1. As we have seen before, we are trying to bring cause of the (weighted) difference between the treatment and the control group back to random variation.\nThere is a lot more to matching, weighting and double robust estimators. An excellent introduction is Chapter 14 in Huntington-Klein (2021). You will also find some dedicated packages for matching and weighting estimators."
  },
  {
    "objectID": "generated/inverse_probability_weight.html#setup",
    "href": "generated/inverse_probability_weight.html#setup",
    "title": "IPW",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(here)\nlibrary(cowplot)\nlibrary(bayesboot)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generated/inverse_probability_weight.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#simulated-data",
    "href": "generated/inverse_probability_weight.html#simulated-data",
    "title": "IPW",
    "section": "Simulated data",
    "text": "Simulated data\n\nN <- 500\nnsim <- 200\nntotal <- N * nsim\n\nThe data generating process for x and y is more complicated that what I usually use. First, we have two binary variables z2 and x. I use the transformation 1 - 2*x and 1 - 2*z2 a number of times. This transformation takes a binary (0,1) and transforms it in (1, -1). The main goal with this transformation is to make sure that interactions with these variables are more meaningful. If they take the value 0, interactions without the transformation would also take a value of 0.\nIt might not be immediately clear but x just follows a probit-binomial distribution which depends on the control variables z1 and z2 and it’s interactions. y is a normally distributed function of x and the control variables and the effect of x depends on the interaction between the controls.\n\nsim_data <-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    z1 = rnorm(ntotal, 0, 1),\n    z2 = rbinom(ntotal, 1, .5)\n    ) %>%\n  mutate(\n    x = rbinom(ntotal, 1, pnorm((z2 + 2 * z1 + (1 - 2 * z2) * z1))),\n    y = rnorm(ntotal, x + z1 + z2 + (1 - 2 * x) * z1 * ( 1 - 2 * z2), 5)\n  )\n\nOne of the problems with this data is that there is little overlap in the z1 variable between the treatment and the control group. In our case, this an indication that z1 is causing x and we have a potential confounder.\n\nggplot(sim_data, aes(x = z1, group = x)) +\n  geom_density()"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#bias",
    "href": "generated/inverse_probability_weight.html#bias",
    "title": "IPW",
    "section": "Bias",
    "text": "Bias\nAt first, we want to figure out whether different regression specifications are biased. One way to do that is to work with the full data and ignore the sim variable.\nFirst, we run our regular regressions with different but imperfect functional forms for the control variables.\n\nbind_rows(\n  lm(y ~ x, data = sim_data) %>% tidy() %>% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = sim_data) %>% tidy() %>% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = sim_data) %>% tidy() %>% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic  p.value\n  <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n1 x        2.63     0.0343     76.6  0       \n2 x        0.232    0.0449      5.17 2.41e- 7\n3 x        0.289    0.0460      6.27 3.63e-10\n\n\nIt’s clear that while adding the control variables (and the interaction) improves the estimate, we are still far off the correct estimate which is 1.\nNext, we first run a logit regression to predict the probability that an observations is in the treatment group based on the control variables. I deliberately misspecify the regression to demonstrate that even an imperfect the propensity model can already help. Nevertheless, for a method to be truly double robust either the propensity model or the regression model needs to be correctly specified.\nWe calculate the weights and trimmed weights where I set the maximum weight to 100 which means that propensities smaller than 0.01 are winsorised at the value of 0.01 to avoid that the estimation is dominated by a couple of observations. We can run the regressions above again but now with the weights included. The disadvantage of trimming the weight is that we might be reintroducing a little bit of the bias that we are trying to avoid but the advantage is that because outliers are less likely to dominate, the standard errors are likely to be smaller.\n\nprop_glm <- glm(x ~ z1 + z2, data = sim_data,\n                family = binomial(link = \"logit\"))\nipw_data <- sim_data %>%\n  mutate(propensity = fitted(prop_glm),\n         weight = if_else(x == 1, 1/propensity, 1/(1 - propensity)),\n         trim_weight = pmin(weight, 100))\n\nbind_rows(\n  lm(y ~ x, data = ipw_data, weights = weight) %>% tidy() %>% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = ipw_data, weights = weight) %>% tidy() %>% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = ipw_data, weights = weight) %>% tidy() %>% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic   p.value\n  <chr>    <dbl>     <dbl>     <dbl>     <dbl>\n1 x        0.683    0.0338      20.2 9.32e- 91\n2 x        1.11     0.0335      33.2 6.71e-240\n3 x        1.12     0.0378      29.7 1.05e-192\n\nbind_rows(\n  lm(y ~ x, data = ipw_data, weights = trim_weight) %>% tidy() %>% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = ipw_data, weights = trim_weight) %>% tidy() %>% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = ipw_data, weights = trim_weight) %>% tidy() %>% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic   p.value\n  <chr>    <dbl>     <dbl>     <dbl>     <dbl>\n1 x        0.851    0.0331      25.7 4.67e-145\n2 x        0.903    0.0322      28.0 7.17e-172\n3 x        0.972    0.0355      27.4 3.05e-164"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#bootstrap-the-standard-errors",
    "href": "generated/inverse_probability_weight.html#bootstrap-the-standard-errors",
    "title": "IPW",
    "section": "Bootstrap the standard errors",
    "text": "Bootstrap the standard errors\n\nipw <- function(data, weights){\n  prop_glm <- glm(x ~ z1 + z2, data = data,\n                 # this is necessary for the bayesian boostrap weights\n                 # otherwise R gives warnings which can slow things down \n                  family = quasibinomial(link = \"logit\"),\n                  weights = weights)\n  ipw_data <- data %>%\n    mutate(propensity = fitted(prop_glm),\n           weight = weights * if_else(x == 1, 1/propensity, 1/(1 - propensity)),\n           trim_weight = weights * pmin(weight, 100))\n  # These weights are the propensity weights\n  lm <- lm(y ~ x + z1 * z2, data = ipw_data, weights = trim_weight)\n  beta <- coefficients(lm)[\"x\"]\n  return(beta)\n}\nipw(sim_data, rep(1, ntotal))\n\n        x \n0.9723643 \n\nboot_ipw <- bayesboot(sim_data, ipw, R = 200, use.weights = TRUE)\nsummary(boot_ipw)\n\nBayesian bootstrap\n\nNumber of posterior draws: 200 \n\nSummary of the posterior (with 95% Highest Density Intervals):\n statistic     mean        sd   hdi.low hdi.high\n         x 1.138719 0.2023061 0.7195526 1.479771\n\nQuantiles:\n statistic     q2.5%     q25%   median     q75% q97.5%\n         x 0.7246761 1.013514 1.152372 1.274558 1.4802\n\nCall:\n bayesboot(data = sim_data, statistic = ipw, R = 200, use.weights = TRUE)\n\n\n\nlibrary(furrr)\ncores <- parallel::detectCores()\nplan(multisession, workers = cores - 2)\nboot_function <- function(data){\n  bb <- bayesboot(data, ipw, R = 200, use.weights = TRUE)\n  se <- sd(bb$x)\n  return(se)\n}\n\nsim <- sim_data %>%\n  nest(.by = sim) %>%\n  mutate(se = future_map_dbl(.x = data, .f = ~ boot_function(.x),\n                             .options = furrr_options(seed = TRUE),\n                             .progress = TRUE))\n\nsummarise(sim, mean = mean(se), sd = sd(se))\n\n# A tibble: 1 × 2\n   mean    sd\n  <dbl> <dbl>\n1  2.37 0.523"
  },
  {
    "objectID": "generated/residual_dependent.html",
    "href": "generated/residual_dependent.html",
    "title": "Generated Dependent",
    "section": "",
    "text": "This page is based on Chen, Hribar, and Melessa (2018) who show the potential problems of using a two-stage approach where in the second stage, we use the residuals from the first regression as a dependent variable. This approach is quite popular in hte accounting and finance literature. The paper shows formally and with simulation that (1) these models can typically be estimated with 1 regression and that (2) the two-step procedure can be biased if they are not used carefully. The paper also re-analyses a number of accounting studies and shows that the results sometimes meaningfully change.\nI am going to illustrate the issue with a number of simulated examples. As always, we will be interested in the effect of a variable x on y where we want to control for a variable z. The bias of the two-stage procedure will often depend on unobserved correlations between those variables which I will model with the unobserved variable w.\nThe two-step approach is to: - First run a regression y ~ z and calculate the residuals. - Second use those residuals to estimate the effect of x on the residuals residuals ~ x\nThe one-step approach is to use just run the regression y ~ x + z."
  },
  {
    "objectID": "generated/residual_dependent.html#setup",
    "href": "generated/residual_dependent.html#setup",
    "title": "Generated Dependent",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\nlibrary(broom)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"generated/residual_dependent.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(modelsummary)\n\n\nN <- 1000\ngof_map <- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generated/residual_dependent.html#correlated-step-1-control",
    "href": "generated/residual_dependent.html#correlated-step-1-control",
    "title": "Generated Dependent",
    "section": "Correlated Step 1 control",
    "text": "Correlated Step 1 control\nThe basic problem is that in the two-step approach researchers regularly include z in the first regression and not in the second regression. If z and x are correlated, the two-step procedure will be biased downwards if the second step does not control for z. This can be seen in the simulated example below. I also show that the bias can be avoided by running the one-step regression, add z as a control variable, or use z to residualise both x and y.\n\nd1 <- tibble(w = rnorm(N)) %>%\n  mutate(z = rnorm(N, w, 1),\n         x = rnorm(N, w, 1),\n         y = rnorm(N, x + z, 10))\nlm1 <- lm(y ~ x + z, data = d1)\nlm1x <- lm(y ~ z, data = d1)\nlm1y <- lm(x ~ z, data = d1)\nd1 <- mutate(d1,\n             resid_y = resid(lm1x),\n             resid_x = resid(lm1y))\nlm1resy <- lm(resid_y ~ x, data = d1)\nlm1resyz <- lm(resid_y ~ x + z, data = d1)\nlm1resyx <- lm(resid_y ~ resid_x, data = d1)\nmodelsummary(list(onestep = lm1, no_control = lm1resy,\n                  with_control = lm1resyz, double_resid = lm1resyx),\n             gof_map = gof_map)\n\n\n\n \n  \n      \n    onestep \n    no_control \n    with_control \n    double_resid \n  \n \n\n  \n    (Intercept) \n    −0.070 \n    0.001 \n    0.002 \n    0.000 \n  \n  \n     \n    (0.305) \n    (0.306) \n    (0.305) \n    (0.305) \n  \n  \n    x \n    1.294 \n    0.938 \n    1.294 \n     \n  \n  \n     \n    (0.255) \n    (0.218) \n    (0.255) \n     \n  \n  \n    z \n    0.495 \n     \n    −0.672 \n     \n  \n  \n     \n    (0.253) \n     \n    (0.253) \n     \n  \n  \n    resid_x \n     \n     \n     \n    1.294 \n  \n  \n     \n     \n     \n     \n    (0.255) \n  \n  \n    Num.Obs. \n    1000 \n    1000 \n    1000 \n    1000 \n  \n  \n    R2 \n    0.052 \n    0.018 \n    0.025 \n    0.025"
  },
  {
    "objectID": "generated/residual_dependent.html#extra-controls-in-step-2",
    "href": "generated/residual_dependent.html#extra-controls-in-step-2",
    "title": "Generated Dependent",
    "section": "Extra controls in step 2",
    "text": "Extra controls in step 2\nIn the next step, we can include extra controls, z2, in the second step. The bias will now depend on the correlations that the extra control has with the first stage controls and x and y. More importantly, the bias can now be an overestimation or underestimation. If the additional control is not correlated to x or z1, we still have the downward bias from before.\n\nd2 <- d1 %>%\n  rename(z1 = z) %>%\n  mutate(z2 = rnorm(N, 0, 1),\n         y = rnorm(N, x + z1 + z2, 10))\nlm2 <- lm(y ~ x + z1 + z2, data = d2)\nlm2y <- lm(y ~ z1, data = d2)\nd2 <- d2 %>%\n  mutate(resid_y = resid(lm2y))\nlm2resy <- lm(resid_y ~ x + z2, data = d2)\nmodelsummary(list(onestep = lm2, twostep = lm2resy),\n             gof_map = gof_map)\n\n\n\n \n  \n      \n    onestep \n    twostep \n  \n \n\n  \n    (Intercept) \n    0.123 \n    0.000 \n  \n  \n     \n    (0.320) \n    (0.321) \n  \n  \n    x \n    0.961 \n    0.682 \n  \n  \n     \n    (0.269) \n    (0.229) \n  \n  \n    z1 \n    1.438 \n     \n  \n  \n     \n    (0.266) \n     \n  \n  \n    z2 \n    0.938 \n    0.929 \n  \n  \n     \n    (0.318) \n    (0.318) \n  \n  \n    Num.Obs. \n    1000 \n    1000 \n  \n  \n    R2 \n    0.089 \n    0.018"
  },
  {
    "objectID": "generated/residual_dependent.html#extra-correlated-controls-in-step-2",
    "href": "generated/residual_dependent.html#extra-correlated-controls-in-step-2",
    "title": "Generated Dependent",
    "section": "extra correlated controls in step 2",
    "text": "extra correlated controls in step 2\nHowever, when the additional control is negatively correlated to both z1 and x but positively to y, we get an upward bias.\n\nd3 <- d2 %>%\n  mutate(z3 = rnorm(N, - 2 * w, 1),\n         y = rnorm(N, x + z1 + z3, 10)) %>%\n  mutate(resid_y = resid(lm(y ~ z1, data = .)))\nlm3 <- lm(y ~ x + z1 + z3, data = d3)\nlm3resy <- lm(resid_y ~ x + z3, data = d3)\nmodelsummary(list(onestep = lm3, twostep = lm3resy),\n             gof_map = gof_map)\n\n\n\n \n  \n      \n    onestep \n    twostep \n  \n \n\n  \n    (Intercept) \n    0.080 \n    −0.012 \n  \n  \n     \n    (0.321) \n    (0.321) \n  \n  \n    x \n    0.929 \n    1.009 \n  \n  \n     \n    (0.297) \n    (0.291) \n  \n  \n    z1 \n    0.773 \n     \n  \n  \n     \n    (0.306) \n     \n  \n  \n    z3 \n    0.850 \n    0.717 \n  \n  \n     \n    (0.207) \n    (0.180) \n  \n  \n    Num.Obs. \n    1000 \n    1000 \n  \n  \n    R2 \n    0.021 \n    0.017"
  },
  {
    "objectID": "generated/residual_dependent.html#run-simulations-as-a-large-dataframe",
    "href": "generated/residual_dependent.html#run-simulations-as-a-large-dataframe",
    "title": "Generated Dependent",
    "section": "Run simulations as a large dataframe",
    "text": "Run simulations as a large dataframe\nWe can use a simulation approach to see whether that bias is persistent. First, we create the two functions that run the two step and one step approach and return the estimate and the p-value from the coefficient table.\n\ntwo_step <- function(data){\n  lm1 <- lm(y ~ z1, data)\n  lm2 <- lm(resid(lm1) ~ x + z3, data = data)\n  coefs <- summary(lm2)$coefficients\n  result <- coefs[\"x\", c(1, 4)]\n  names(result) <- c(\"estimate\", \"pvalue\")\n  return(result)\n}\none_step <- function(data){\n  lm <- lm(y ~ x + z1 + z3, data = data)\n  coefs <- summary(lm)$coefficients\n  result <- coefs[\"x\", c(1, 4)]\n  names(result) <- c(\"estimate\", \"pvalue\")\n  return(result)\n}\ntwo_step(d3)\n\n    estimate       pvalue \n1.0094585099 0.0005442113 \n\none_step(d3)\n\n   estimate      pvalue \n0.929023153 0.001822299 \n\n\nNext, we set up the simulation for 1000 simulations with 100 observations per data set. One trick for efficient simulations is to generate the data in one big data frame. You can see that we can still use the two step approach with tibble and mutate for exogenous and endogenous variables. We just need to be careful with the number of observations and use ntotal.\n\nN <- 100\nnsim <- 1000\nntotal <- nsim * N\nsimdata <-\n  tibble(\n    sample = rep(1:nsim, each = N),\n    w = rnorm(ntotal)) %>%\n  mutate(\n    z1 = rnorm(ntotal, w, 1),\n    x = rnorm(ntotal, w, 1),\n    z3 = rnorm(ntotal, - 2 * w, 1),\n    y = rnorm(ntotal, x + z1 + z3, 10))\n\nWe can than use nest to create separate data by sample and calculate the estimate and p-value for each sample with our functions. The results are a vector with two values and we use unnest_wider to create separate columns. Finally, I calculate the difference between the two estimates.\n\nresults <- simdata %>%\n  nest(.by = sample) %>%\n  mutate(two = map(.x = data, .f = ~ two_step(.x)),\n         one = map(.x = data, .f = ~ one_step(.x))) %>%\n  select(-data) %>%\n  unnest_wider(c(one, two), names_sep = \"_\") %>%\n  mutate(bias = two_estimate - one_estimate)\n\nNext, we can summarise the results and we see that the two step approach is likely to overestimate the effect of x.\n\nresults %>%\n  summarise(M = mean(bias), se = sd(bias)/sqrt(n()),\n            M_one = mean(one_estimate), M_two = mean(two_estimate))\n\n# A tibble: 1 × 4\n       M      se M_one M_two\n   <dbl>   <dbl> <dbl> <dbl>\n1 0.0860 0.00427  1.02  1.10\n\n\n\nresults %>% select(sample, two_pvalue, one_pvalue) %>%\n  pivot_longer(c(two_pvalue, one_pvalue),\n               names_to = \"variable\", values_to = \"pvalue\") %>%\n  mutate(is_sign = if_else(pvalue < 0.05, 1, 0)) %>%\n  summarise(mean = mean(is_sign), .by = variable)\n\n# A tibble: 2 × 2\n  variable    mean\n  <chr>      <dbl>\n1 two_pvalue  0.21\n2 one_pvalue  1"
  },
  {
    "objectID": "generated/residual_independent.html",
    "href": "generated/residual_independent.html",
    "title": "Generated Independent",
    "section": "",
    "text": "This is probably not worthwhile further investigating. It’s quite niche."
  },
  {
    "objectID": "generated/residual_independent.html#residuals",
    "href": "generated/residual_independent.html#residuals",
    "title": "Generated Independent",
    "section": "Residuals",
    "text": "Residuals\n\nsim_data <-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    x1 = rnorm(ntotal),\n    x2 = rnorm(ntotal),\n    z1 = rnorm(ntotal)\n  ) %>%\n  mutate(\n    z = rnorm(ntotal, z1, 5),\n    y = rnorm(ntotal, x1 + (z - z1), 3)\n  ) %>%\n  nest(.by = sim)\ntest_data <- pull(sim_data, data)[[1]]\n\n\ntwo_step <- function(data){\n  lm1 <- lm(z ~ z1, data = data)\n  resid_z <- residuals(lm1)\n  lm2 <- lm(y ~ x1 + x2 + resid_z, data = data)\n  coefs <- summary(lm2)$coefficients\n  return(coefs[, 4])\n}\ntwo_step(test_data)\n\n (Intercept)           x1           x2      resid_z \n7.680650e-01 7.963916e-05 8.095932e-01 4.606800e-28 \n\n\n\npvalues <- sim_data %>%\n  mutate(pvalues = map(.x = data, .f = ~ two_step(.))) %>%\n  unnest_wider(pvalues) %>%\n  pivot_longer(cols = c(x1, x2, resid_z),\n               values_to = \"pvalue\", names_to = \"variable\") %>%\n  select(sim, pvalue, variable)\n\npvalues %>%\n  mutate(is_sign = if_else(pvalue < 0.05, 1, 0)) %>%\n  summarise(proportion = mean(is_sign), .by = variable)\n\n# A tibble: 3 × 2\n  variable proportion\n  <chr>         <dbl>\n1 x1            0.873\n2 x2            0.043\n3 resid_z       1"
  },
  {
    "objectID": "generated/residual_independent.html#absolute-residuals",
    "href": "generated/residual_independent.html#absolute-residuals",
    "title": "Generated Independent",
    "section": "Absolute Residuals",
    "text": "Absolute Residuals\n\nsim_data <-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    x1 = rnorm(ntotal),\n    x2 = rnorm(ntotal),\n    z1 = rnorm(ntotal)\n  ) %>%\n  mutate(\n    z = rnorm(ntotal, z1, 5),\n    y = rnorm(ntotal, x1 + abs(z - z1), 3)\n  ) %>%\n  nest(.by = sim)\ntest_data <- pull(sim_data, data)[[1]]\n\n\ntwo_step_abs <- function(data){\n  lm1 <- lm(z ~ z1, data = data)\n  abs_resid_z <- abs(residuals(lm1))\n  lm2 <- lm(y ~ x1 + x2 + abs_resid_z, data = data)\n  coefs <- summary(lm2)$coefficients\n  return(coefs[, 4])\n}\ntwo_step_abs(test_data)\n\n (Intercept)           x1           x2  abs_resid_z \n8.102549e-01 5.978551e-03 8.308279e-01 2.633114e-11 \n\n\n\npvalues <- sim_data %>%\n  mutate(pvalues = map(.x = data, .f = ~ two_step_abs(.))) %>%\n  unnest_wider(pvalues) %>%\n  pivot_longer(cols = c(x1, x2, abs_resid_z),\n               values_to = \"pvalue\", names_to = \"variable\") %>%\n  select(sim, pvalue, variable)\n\npvalues %>%\n  mutate(is_sign = if_else(pvalue < 0.05, 1, 0)) %>%\n  summarise(proportion = mean(is_sign), .by = variable)\n\n# A tibble: 3 × 2\n  variable    proportion\n  <chr>            <dbl>\n1 x1               0.874\n2 x2               0.053\n3 abs_resid_z      1"
  }
]
[
  {
    "objectID": "generated/residual_independent.html",
    "href": "generated/residual_independent.html",
    "title": "Generated Independent",
    "section": "",
    "text": "This is probably not worthwhile further investigating. It’s quite niche.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modelsummary)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"generated/residual_independent.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\nN &lt;- 100\nnsim &lt;- 1000\nntotal &lt;- N * nsim"
  },
  {
    "objectID": "generated/residual_independent.html#residuals",
    "href": "generated/residual_independent.html#residuals",
    "title": "Generated Independent",
    "section": "Residuals",
    "text": "Residuals\n\nsim_data &lt;-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    x1 = rnorm(ntotal),\n    x2 = rnorm(ntotal),\n    z1 = rnorm(ntotal)\n  ) %&gt;%\n  mutate(\n    z = rnorm(ntotal, z1, 5),\n    y = rnorm(ntotal, x1 + (z - z1), 3)\n  ) %&gt;%\n  nest(.by = sim)\ntest_data &lt;- pull(sim_data, data)[[1]]\n\n\ntwo_step &lt;- function(data){\n  lm1 &lt;- lm(z ~ z1, data = data)\n  resid_z &lt;- residuals(lm1)\n  lm2 &lt;- lm(y ~ x1 + x2 + resid_z, data = data)\n  coefs &lt;- summary(lm2)$coefficients\n  return(coefs[, 4])\n}\ntwo_step(test_data)\n\n (Intercept)           x1           x2      resid_z \n7.680650e-01 7.963916e-05 8.095932e-01 4.606800e-28 \n\n\n\npvalues &lt;- sim_data %&gt;%\n  mutate(pvalues = map(.x = data, .f = ~ two_step(.))) %&gt;%\n  unnest_wider(pvalues) %&gt;%\n  pivot_longer(cols = c(x1, x2, resid_z),\n               values_to = \"pvalue\", names_to = \"variable\") %&gt;%\n  select(sim, pvalue, variable)\n\npvalues %&gt;%\n  mutate(is_sign = if_else(pvalue &lt; 0.05, 1, 0)) %&gt;%\n  summarise(proportion = mean(is_sign), .by = variable)\n\n# A tibble: 3 × 2\n  variable proportion\n  &lt;chr&gt;         &lt;dbl&gt;\n1 x1            0.873\n2 x2            0.043\n3 resid_z       1"
  },
  {
    "objectID": "generated/residual_independent.html#absolute-residuals",
    "href": "generated/residual_independent.html#absolute-residuals",
    "title": "Generated Independent",
    "section": "Absolute Residuals",
    "text": "Absolute Residuals\n\nsim_data &lt;-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    x1 = rnorm(ntotal),\n    x2 = rnorm(ntotal),\n    z1 = rnorm(ntotal)\n  ) %&gt;%\n  mutate(\n    z = rnorm(ntotal, z1, 5),\n    y = rnorm(ntotal, x1 + abs(z - z1), 3)\n  ) %&gt;%\n  nest(.by = sim)\ntest_data &lt;- pull(sim_data, data)[[1]]\n\n\ntwo_step_abs &lt;- function(data){\n  lm1 &lt;- lm(z ~ z1, data = data)\n  abs_resid_z &lt;- abs(residuals(lm1))\n  lm2 &lt;- lm(y ~ x1 + x2 + abs_resid_z, data = data)\n  coefs &lt;- summary(lm2)$coefficients\n  return(coefs[, 4])\n}\ntwo_step_abs(test_data)\n\n (Intercept)           x1           x2  abs_resid_z \n8.102549e-01 5.978551e-03 8.308279e-01 2.633114e-11 \n\n\n\npvalues &lt;- sim_data %&gt;%\n  mutate(pvalues = map(.x = data, .f = ~ two_step_abs(.))) %&gt;%\n  unnest_wider(pvalues) %&gt;%\n  pivot_longer(cols = c(x1, x2, abs_resid_z),\n               values_to = \"pvalue\", names_to = \"variable\") %&gt;%\n  select(sim, pvalue, variable)\n\npvalues %&gt;%\n  mutate(is_sign = if_else(pvalue &lt; 0.05, 1, 0)) %&gt;%\n  summarise(proportion = mean(is_sign), .by = variable)\n\n# A tibble: 3 × 2\n  variable    proportion\n  &lt;chr&gt;            &lt;dbl&gt;\n1 x1               0.874\n2 x2               0.053\n3 abs_resid_z      1"
  },
  {
    "objectID": "generated/inverse_probability_weight.html",
    "href": "generated/inverse_probability_weight.html",
    "title": "IPW",
    "section": "",
    "text": "The previous pages highlighted the dangers to coefficients and standard errors of using two-step approaches in the research design. Still, if they are done well, they can benefit your analysis and make it more robust. This is the general idea of double robust estimators which is in my view a better way to think of most (propensity score) matching approaches.\nSpecifically, we are going to look at a situation where we have a binary treatment-control x and an outcome of interest y with two confounding factors z1 and z2 that have a complicated effect on x and y. The two-step approach that we are using is combining matching with regression. In the matching step, we first estimate the propensity, \\(p\\), that an observation belongs to the treatment group. Then, we will use the inverse predicted propensity to weight the observations in the regression.\nSpecifically, we want to put less weight on observations that are in the treatment group and that we could expect to be in the treatment group. So, the weight for those observations will be \\(\\frac{1}{p}\\) in the regression. We will also put less weight on the observations in the treatment group, so we weight them by \\(\\frac{1}{1 - p}\\). The intuition is that we want to put more weight on observations that are not expected to be control or treatment groups because we assume that this unexpected assignment is because of some random variation 1. As we have seen before, we are trying to bring cause of the (weighted) difference between the treatment and the control group back to random variation.\nThere is a lot more to matching, weighting and double robust estimators. An excellent introduction is Chapter 14 in Huntington-Klein (2021). You will also find some dedicated packages for matching and weighting estimators."
  },
  {
    "objectID": "generated/inverse_probability_weight.html#introduction",
    "href": "generated/inverse_probability_weight.html#introduction",
    "title": "IPW",
    "section": "",
    "text": "The previous pages highlighted the dangers to coefficients and standard errors of using two-step approaches in the research design. Still, if they are done well, they can benefit your analysis and make it more robust. This is the general idea of double robust estimators which is in my view a better way to think of most (propensity score) matching approaches.\nSpecifically, we are going to look at a situation where we have a binary treatment-control x and an outcome of interest y with two confounding factors z1 and z2 that have a complicated effect on x and y. The two-step approach that we are using is combining matching with regression. In the matching step, we first estimate the propensity, \\(p\\), that an observation belongs to the treatment group. Then, we will use the inverse predicted propensity to weight the observations in the regression.\nSpecifically, we want to put less weight on observations that are in the treatment group and that we could expect to be in the treatment group. So, the weight for those observations will be \\(\\frac{1}{p}\\) in the regression. We will also put less weight on the observations in the treatment group, so we weight them by \\(\\frac{1}{1 - p}\\). The intuition is that we want to put more weight on observations that are not expected to be control or treatment groups because we assume that this unexpected assignment is because of some random variation 1. As we have seen before, we are trying to bring cause of the (weighted) difference between the treatment and the control group back to random variation.\nThere is a lot more to matching, weighting and double robust estimators. An excellent introduction is Chapter 14 in Huntington-Klein (2021). You will also find some dedicated packages for matching and weighting estimators."
  },
  {
    "objectID": "generated/inverse_probability_weight.html#setup",
    "href": "generated/inverse_probability_weight.html#setup",
    "title": "IPW",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(here)\nlibrary(cowplot)\nlibrary(bayesboot)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generated/inverse_probability_weight.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#simulated-data",
    "href": "generated/inverse_probability_weight.html#simulated-data",
    "title": "IPW",
    "section": "Simulated data",
    "text": "Simulated data\n\nN &lt;- 500\nnsim &lt;- 200\nntotal &lt;- N * nsim\n\nThe data generating process for x and y is more complicated that what I usually use. First, we have two binary variables z2 and x. I use the transformation 1 - 2*x and 1 - 2*z2 a number of times. This transformation takes a binary (0,1) and transforms it in (1, -1). The main goal with this transformation is to make sure that interactions with these variables are more meaningful. If they take the value 0, interactions without the transformation would also take a value of 0.\nIt might not be immediately clear but x just follows a probit-binomial distribution which depends on the control variables z1 and z2 and its interactions. y is a normally distributed function of x and the control variables and the effect of x depends on the interaction between the controls.\n\nsim_data &lt;-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    z1 = rnorm(ntotal, 0, 1),\n    z2 = rbinom(ntotal, 1, .5)\n    ) %&gt;%\n  mutate(\n    x = rbinom(ntotal, 1, pnorm((z2 + 2 * z1 + (1 - 2 * z2) * z1))),\n    y = rnorm(ntotal, x + z1 + z2 + (1 - 2 * x) * z1 * ( 1 - 2 * z2), 5)\n  )\n\nOne of the problems with this data is that there is little overlap in the z1 variable between the treatment and the control group. In our case, this an indication that z1 is causing x and we have a potential confounder.\n\nggplot(sim_data, aes(x = z1, group = x)) +\n  geom_density()"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#bias",
    "href": "generated/inverse_probability_weight.html#bias",
    "title": "IPW",
    "section": "Bias",
    "text": "Bias\nAt first, we want to figure out whether different regression specifications are biased. One way to do that is to work with the full data and ignore the sim variable.\nFirst, we run our regular regressions with different but imperfect functional forms for the control variables.\n\nbind_rows(\n  lm(y ~ x, data = sim_data) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = sim_data) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = sim_data) %&gt;% tidy() %&gt;% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic      p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 x        2.67     0.0344     77.6  0           \n2 x        0.196    0.0448      4.39 0.0000115   \n3 x        0.253    0.0460      5.50 0.0000000386\n\n\nIt’s clear that while adding the control variables (and the interaction) improves the estimate, we are still far off the correct estimate which is 1.\nNext, we first run a logit regression to predict the probability that an observations is in the treatment group based on the control variables. I deliberately misspecify the regression to demonstrate that even an imperfect the propensity model can already help. Nevertheless, for a method to be truly double robust either the propensity model or the regression model needs to be correctly specified.\nWe calculate the weights and trimmed weights where I set the maximum weight to 100 which means that propensities smaller than 0.01 are winsorised at the value of 0.01 to avoid that the estimation is dominated by a couple of observations. We can run the regressions above again but now with the weights included. The disadvantage of trimming the weight is that we might be reintroducing a little bit of the bias that we are trying to avoid but the advantage is that because outliers are less likely to dominate, the standard errors are likely to be smaller.\n\nprop_glm &lt;- glm(x ~ z1 + z2, data = sim_data,\n                family = binomial(link = \"logit\"))\nipw_data &lt;- sim_data %&gt;%\n  mutate(propensity = fitted(prop_glm),\n         weight = if_else(x == 1, 1/propensity, 1/(1 - propensity)),\n         trim_weight = pmin(weight, 100))\n\nbind_rows(\n  lm(y ~ x, data = ipw_data, weights = weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = ipw_data, weights = weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = ipw_data, weights = weight) %&gt;% tidy() %&gt;% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 x         1.13    0.0337      33.6 1.84e-246\n2 x         1.44    0.0340      42.2 0        \n3 x         1.27    0.0384      33.0 2.37e-237\n\nbind_rows(\n  lm(y ~ x, data = ipw_data, weights = trim_weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = ipw_data, weights = trim_weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = ipw_data, weights = trim_weight) %&gt;% tidy() %&gt;% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 x        0.927    0.0336      27.6 7.39e-167\n2 x        0.978    0.0328      29.8 1.50e-194\n3 x        1.04     0.0361      28.9 1.66e-182"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#bootstrap-the-standard-errors",
    "href": "generated/inverse_probability_weight.html#bootstrap-the-standard-errors",
    "title": "IPW",
    "section": "Bootstrap the standard errors",
    "text": "Bootstrap the standard errors\nWe know by now that the standard errors of these type of procedures with multiple steps are likely to be wrong and we can use the Bayesian bootstrap again to estimate, the standard error for our estimate of interest. The unadjusted standard errors above are around 0.035.\n\nipw &lt;- function(data, weights){\n  prop_glm &lt;- glm(x ~ z1 + z2, data = data,\n                 # the quasibinomial is necessary for the bayesian boostrap\n                 # weights otherwise R gives warnings which can slow things down\n                  family = quasibinomial(link = \"logit\"),\n                  weights = weights)\n  ipw_data &lt;- data %&gt;%\n    mutate(propensity = fitted(prop_glm),\n           weight = weights * if_else(x == 1, 1/propensity, 1/(1 - propensity)),\n           trim_weight = weights * pmin(weight, 100))\n  # These weights are the propensity weights\n  lm &lt;- lm(y ~ x + z1 * z2, data = ipw_data, weights = trim_weight)\n  beta &lt;- coefficients(lm)[\"x\"]\n  return(beta)\n}\nipw(sim_data, rep(1, ntotal))\n\n       x \n1.043357 \n\nboot_ipw &lt;- bayesboot(sim_data, ipw, R = 200, use.weights = TRUE)\nsummary(boot_ipw)\n\nBayesian bootstrap\n\nNumber of posterior draws: 200 \n\nSummary of the posterior (with 95% Highest Density Intervals):\n statistic     mean        sd   hdi.low hdi.high\n         x 1.244239 0.2137545 0.8754819 1.660996\n\nQuantiles:\n statistic     q2.5%     q25%   median     q75%   q97.5%\n         x 0.8899417 1.088038 1.236333 1.378097 1.662164\n\nCall:\n bayesboot(data = sim_data, statistic = ipw, R = 200, use.weights = TRUE)\n\n\nThe bootstrapped standard error is around 0.20 and noticeably higher. The standard errors are also quite wide given that we are using a quite large simulated data set. Unfortunately, these type of estimators come at a cost. The estimated uncertainty can be quite high because ultimiately when the data is far from ideal, there is only so much information you can glean from it.\nThis is only reinforced by doing the simulation properly and calculating the bootstrapped standard error separately for each of our 200 simulated datasets with 500 observations. The average standard error is more than 2.3 and thus with a sample of 500, we will never be able to report a significant effect. Again, this is not necessarily a weakness of the inverse probability approach. It just highlights that more advanced problems that correct for potential biases are not a free lunch. You cannot learn more from the data than what is in your data.\n\nlibrary(furrr)\ncores &lt;- parallel::detectCores()\nplan(multisession, workers = cores - 2)\nboot_function &lt;- function(data){\n  bb &lt;- bayesboot(data, ipw, R = 200, use.weights = TRUE)\n  se &lt;- sd(bb$x)\n  return(se)\n}\n\nsim &lt;- sim_data %&gt;%\n  nest(.by = sim) %&gt;%\n  mutate(se = future_map_dbl(.x = data, .f = ~ boot_function(.x),\n                             .options = furrr_options(seed = TRUE),\n                             .progress = TRUE))\n\nsummarise(sim, mean = mean(se), sd = sd(se))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2.37 0.523"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#footnotes",
    "href": "generated/inverse_probability_weight.html#footnotes",
    "title": "IPW",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the accounting and finance literature, you are more likely to see propensity score matching where we find appropriate matching control observations for each treatment observations based on the propensity scores. The disadvantage of propensity score matching is similar to the regular bootstrap. The inclusion of an observation is discrete which can lead to weird behaviour. That’s why I recommend inverse probability weights.↩︎"
  },
  {
    "objectID": "generalised/poisson.html",
    "href": "generalised/poisson.html",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nset.seed(830323)\nN &lt;- 4000\nn_firm &lt;- 500\ngof_omit &lt;- \"Adj|Lik|IC|RMSE\""
  },
  {
    "objectID": "generalised/poisson.html#setup",
    "href": "generalised/poisson.html#setup",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nset.seed(830323)\nN &lt;- 4000\nn_firm &lt;- 500\ngof_omit &lt;- \"Adj|Lik|IC|RMSE\""
  },
  {
    "objectID": "generalised/poisson.html#introduction",
    "href": "generalised/poisson.html#introduction",
    "title": "Poisson Regression",
    "section": "Introduction",
    "text": "Introduction\nIn the previous section, I made the case for using OLS regressions even when the outcome variable is a discrete variable. This is especially true in the case that we are interested in estimating the effect of an intervention. The coefficient that we are getting can easily be interpreted as the difference in the probability of getting one outcome over the other."
  },
  {
    "objectID": "generalised/poisson.html#a-multiplicative-process",
    "href": "generalised/poisson.html#a-multiplicative-process",
    "title": "Poisson Regression",
    "section": "A multiplicative process",
    "text": "A multiplicative process\nThere is an exception for count and count-like data. The type of outcome variables I have in mind are the result of a stable multiplicative process. In earlier lectures, I have made the point that we can think of the contribution of a CEO to the firm as a multiplicative effect. The CEO’s ability has a larger contribution to firm value if they are working in a larger firm. So, if they grow the value of the firm by making the right decisions, the effect will be larger for a large firm.\nProbably the most basic example in finance of a multiplicative process is compound interest. If we start with $100 and the yearly interest rate is 5%, we can write our wealth as a function of time \\(T\\) (in number of years).\n\\[\nW(T) = 100 (1 + 0.05)^T\n\\]\nNow, imagine that we divide the interest by \\(N &gt; 1\\). That is, imagine that we pay an interest of \\(\\frac{0.05}{N}\\) every period with \\(N\\) periods per year.\n\\[\nW(T) = 100 (1 + \\frac{0.05}{N})^{T N}\n\\]\nIn the where we have a lot of small periods (\\(N \\to \\infty\\)), we can write our wealth as follows.\n\\[\nW(T) = 100 e^{0.05 T}\n\\]\nIn general, if we have a variable \\(V\\) that is the results of a multiplicative process of small components with a rate of change \\(r\\) and \\(S\\) steps and a starting value \\(V_0\\), we can write \\(V(S)\\) as follows.\n\\[\n\\begin{aligned}\nV(S) &= V_0 e^{rS} \\\\\n\\textrm{log} (V(S)) &= \\textrm{log} (V_0) + rS \\\\\n\\frac{V(S)}{V_0} &= e^{rS} \\\\\n\\textrm{log} \\frac{V(S)}{V_0} &= rS \\\\\n\\end{aligned}\n\\]\nThe Poisson distribution itself is the discrete equivalent of this idea. The distribution models the number of events, \\(V(S)\\), for a population, \\(V_0\\), when the underlying process follows a fixed occurrence rate \\(r\\) per unit of time and per element of the population. For instance, the number of patents a firm has can be expected to be higher when the firm is larger. The theoretical case for the Poisson regression is that the coefficients on the linear scale targets, \\(r\\), the instantaneous rate of change or the rate of occurrences 1 has a meaningful economic interpretation for non-negative variables such as number of corporate patents, carbon emissions, or distance between companies (Cohn, Liu, and Wardlaw 2022). For instance, it allows us to ask the question what the effect is of increasing R&D investments with a certain percentage to the percentage change in the number of patents. The Poisson approach also make sense for variables that naturally grow like firm size, revenues, or CEO wealth and income."
  },
  {
    "objectID": "generalised/poisson.html#the-case-for-a-poisson-regression",
    "href": "generalised/poisson.html#the-case-for-a-poisson-regression",
    "title": "Poisson Regression",
    "section": "The Case for a Poisson regression",
    "text": "The Case for a Poisson regression\n\nIntuition\nThe statistical case for the Poisson regression is extensively documented in Cohn, Liu, and Wardlaw (2022). Here I will just list the main advantages and shortly demonstrate them with a simulated example. There are a number of alternative approaches that we could use to model these type of variables. The obvious alternative is to model the variable \\(\\textrm{log}(V)\\) in a linear regression. However, this does not work if we have a lot of observations where \\(V = 0\\). One proposed solution in the literature is then to use the transformation \\(\\textrm{log}(V + 1)\\). Cohn, Liu, and Wardlaw (2022) show that the coefficients with the log plus 1 approach are hard to interpret and can have a different sign than with a poisson regression, where the poisson regression has a more straightforward interpretation. A further strength of the Poisson approach is that it allows for the inclusion of fixed effects in the regression without changing the interpretation of the coefficients. Remember that with generalised linear models in general the effect depends on other parts of the model if we are interested in the non-transformed scale, \\(V\\). The concession that we have to make is that we are interpreting the effects on the transformed scale of \\(r\\) (the change) and not on the scale of \\(V_0\\) (the size). As I explained above, this can often be a reasonable assumption to make.\nOne criticism of the Poisson regression is that it assumes that the variation around the mean is proportional to the mean. However, if this assumption does not hold, the estimates of the coefficients will not be biased and (cluster) robust standard errors are robust against violations of this assumption.\nA last point is that, just like in the binomial case, we could just use a linear model on \\(V\\) or \\(\\frac{V}{V_0}\\). However, because with multiplicative effects (or exponential growth) \\(V\\) can vary by multiple orders of magnitude, the estimates of the coefficients can be noisy and have large standard errors.\nCohn, Liu, and Wardlaw (2022) retest six published papers that use a log transformed dependent variable and compare it to a Poisson regression. They find that in all six cases the coefficient is markedly different and in three cases the sign changes. Moreover, the change in the coefficient is larger than removing any control variables. The type of regression matters more than the control variables.\n\n\n\n\n\n\nNote\n\n\n\nIn my view, Cohn, Liu, and Wardlaw (2022) makes a strong case that for a lot of non-negative outcome variables in accounting and finance research designs, the Poisson regression should be the default. This is also my recommendation.\n\n\n\n\nSimulation\nIn the simulation below, I create a dataset for a discrete and a continuous \\(y\\) where the expected value of \\(y\\) is given by\n\\[\nE(y|x_1, x_2) = e^{-0.3 x_1 + x_2}\n\\]\nThis is the data generating process that we associate with a multiplicative process or from a Poisson count process. We will be interested in estimating the effect of \\(x_1\\) on \\(y\\) which in the Poisson regression should give an estimate of \\(-0.3\\).\nThe data generating process also includes fixed effects and additional variation around this expected value which violates the assumptions of the naive Poisson regression. The details of this approach are not important and require knowledge of the negative binomial distribution(rnbinom) to get count data and the chi-squared distribution (rchisq) for the continuous case.\n\noverdispersion &lt;- 0.5\nhetero &lt;- 1\nbeta &lt;- - 0.3\nfirm &lt;-\n  tibble(\n    firm = 1:n_firm,\n    fixed = rnorm(n_firm, 0, .5))\npanel &lt;-\n  tibble(\n    firm = sample(1:100, N, replace = TRUE),\n    x1_noise = rnorm(N, 0, .5),\n    noise = rnorm(N, 0, .5)) %&gt;%\n  left_join(firm) %&gt;%\n  mutate(\n    x1 = fixed + x1_noise,\n    x2 = rnorm(N, x1 + x1^2, 2),\n    ydiscrete = rnbinom(n = N, mu = exp(beta * x1 + x2),\n                        size = 1/overdispersion),\n    ycontinuous = rchisq(n = N, ydiscrete))\n\nJoining with `by = join_by(firm)`\n\n\nI plot the data on log + 1 scale and you can see that the figure looks distorted or weird for lower values of ydiscrete or ycontinuous. This is by now means proof but it is indicative of some of the problems with the log or log plus 1 transformation.\n\npanel %&gt;%\n  pivot_longer(c(ydiscrete, ycontinuous), values_to = \"y\") %&gt;%\n  ggplot(aes(y = y + 1, x = x1)) +\n  geom_point() +\n  scale_y_log10() +\n  facet_wrap(~name)\n\n\n\n\nFor both the continuous and count variable, I run four regression models with fixed effects.\n\nThe Poisson regression with \\(y\\) as dependent variable.\nAn OlS regression with \\(\\textrm{log}(y + 1)\\) as dependent variable.\nAn OLS regression with \\(y\\) as dependent variable. Because this regression is not on the rate of change scale, we do not expect a coefficient of -0.3 here. The main purpose is to show how noisy the estimate is.\nAn OLS regression with \\(\\frac{y/x_2\\) as dependent variable. This approach will suffer from the same noisy estimates.\n\n\npoisson_disc &lt;- feglm(ydiscrete ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\npoisson_cont &lt;- feglm(ycontinuous ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\nlog_plus1_disc &lt;- feols(log(ydiscrete + 1) ~ x1 + x2 | firm,\n                        data = panel)\nlog_plus1_cont &lt;- feols(log(ycontinuous + 1) ~ x1 + x2 | firm,\n                        data = panel)\nols_disc &lt;-feols(ydiscrete ~ x1 + x2 | firm, data = panel)\nols_cont &lt;-feols(ycontinuous ~ x1 + x2 | firm, data = panel)\nrate_disc &lt;-feols(I(ydiscrete / exp(x2)) ~ x1 | firm, data = panel)\nrate_cont &lt;-feols(I(ycontinuous / exp(x2)) ~ x1 | firm, data = panel)\nmsummary(list(poisson = poisson_disc, log1 = log_plus1_disc,\n              ols = ols_disc, rate = rate_disc),\n         gof_omit = gof_omit)\n\n\n\n\n\npoisson\nlog1\nols\nrate\n\n\n\n\nx1\n−0.261\n−0.081\n12.167\n−0.304\n\n\n\n(0.095)\n(0.030)\n(9.304)\n(0.062)\n\n\nx2\n0.983\n0.541\n21.597\n\n\n\n\n(0.021)\n(0.012)\n(4.107)\n\n\n\nNum.Obs.\n4000\n4000\n4000\n4000\n\n\nR2\n0.923\n0.705\n0.110\n0.034\n\n\nR2 Within\n0.902\n0.684\n0.081\n0.005\n\n\nStd.Errors\nby: firm\nby: firm\nby: firm\nby: firm\n\n\nFE: firm\nX\nX\nX\nX\n\n\n\n\n\n\nmsummary(list(poisson = poisson_cont, log1 = log_plus1_cont,\n              ols = ols_cont, rate = rate_cont),\n         gof_omit = gof_omit)\n\n\n\n\n\npoisson\nlog1\nols\nrate\n\n\n\n\nx1\n−0.259\n−0.087\n12.037\n−0.292\n\n\n\n(0.095)\n(0.035)\n(9.336)\n(0.085)\n\n\nx2\n0.979\n0.536\n21.686\n\n\n\n\n(0.020)\n(0.012)\n(4.076)\n\n\n\nNum.Obs.\n4000\n4000\n4000\n4000\n\n\nR2\n0.916\n0.653\n0.112\n0.031\n\n\nR2 Within\n0.893\n0.630\n0.082\n0.002\n\n\nStd.Errors\nby: firm\nby: firm\nby: firm\nby: firm\n\n\nFE: firm\nX\nX\nX\nX\n\n\n\n\n\n\n\nYou can see that the Poisson regression is pretty close to recovering the true estimate of -0.3 both for the count as for the continuous case with small standard errors. The rate estimate with OLS is also pretty good but this only works if we know the scale variable exp(x2) before hand. The log plus 1 approach gives a considerably different estimate and the OLS estimates are positive instead of negative but with large standard errors."
  },
  {
    "objectID": "generalised/poisson.html#footnotes",
    "href": "generalised/poisson.html#footnotes",
    "title": "Poisson Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA semi-elasticity in economics terms (Cohn, Liu, and Wardlaw 2022).↩︎"
  },
  {
    "objectID": "generalised/interpretation.html",
    "href": "generalised/interpretation.html",
    "title": "Interpretation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(fixest)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nN &lt;- 1001\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generalised/interpretation.html#setup",
    "href": "generalised/interpretation.html#setup",
    "title": "Interpretation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(fixest)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nN &lt;- 1001\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generalised/interpretation.html#interpretation",
    "href": "generalised/interpretation.html#interpretation",
    "title": "Interpretation",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe problem\nThe problem with non-linear, generalised linear models is that the interpretation of the coefficients is more murky than in our linear models. In a linear model, we can interpret the coefficient, \\(\\beta\\), of the variable \\(x\\) easily. When \\(x\\) is a continuous variable, we can interpret the coefficient as the increase in \\(y\\) when \\(x\\) increases by 1. When \\(x\\) indicates whether an observation is in the treatment group (\\(x=1\\)) or in the control group (\\(x=0\\)), the coefficient estimates the difference between the control and treatment group for \\(y\\).\nUnfortunately, the non-linear transformation for the generalised linear models complicates the interpretation. Let’s illustrate this with an example where we know the true model for the probability that a firm has a female CEO (\\(y_1\\)) and the probability that the firm has a female CFO (\\(y_2\\)). The true probabilities are given by the following logistic models.\n\\[\n\\begin{aligned}\ny_1 &= \\frac{e^{2 + 3x}}{1 + e^{2 + 3x}} &&= g(2 + 3x) \\\\\ny_2 &= \\frac{e^{-2 + 3x}}{1 + e^{-2 + 3x}} &&= g(-2 + 3x)\n\\end{aligned}\n\\]\nYou can think of \\(x\\) as a characteristic of the company that increases the likelihood of female executives in the company. A casual glance of the equations would give the impression that the effect of the on the CEOs and CFOs is the same because \\(\\beta = 3\\) is the same in both equations. We can also plot the two probabilities as functions of \\(x\\) and you can see that that one curve is just the other one shifted horizontally.\n\ninterpretation &lt;-\n  tibble(x = rnorm(N, 0, 1)) %&gt;%\n  mutate(y1 = plogis(2 + 3 * x),\n         y2 = plogis(-2 + 3 * x))\n\ninterpretation %&gt;%\n  pivot_longer(cols = c(y1, y2)) %&gt;%\nggplot(aes(y = value, x = x,\n                        colour = name)) +\n  scale_color_viridis_d() +\n  geom_line()\n\n\n\n\nHowever, from the plots you can already see that firms at \\(x = 0\\) will see a larger increase in the probability of having a female CFO (\\(y_2\\)) than a female CEO (\\(y_1\\)) when \\(x\\) increases. Because at \\(x=0\\) most firms are already more likely to have a female CEO and thus the effect of \\(x\\) cannot be very large. In other words, despite the fact that the two probabilities follow a very similar function, the effect of an increase in \\(x\\) can be very different depending on the value of \\(x\\).\nAnother way to look at the problem is to think of \\(x\\) as a policy that is either present (\\(x=1\\)) or not (\\(x=0\\)). We can calculate the causal effect of the policy as follows.\n\ny1_effect &lt;- plogis(2 + 3 * 1) - plogis(2 + 3 * 0)\ny2_effect &lt;- plogis(-2 + 3 * 1) - plogis(-2 + 3 * 0)\nprint(c(y1_effect, y2_effect))\n\n[1] 0.1125101 0.6118557\n\n\nAgain, we see that the policy has a large effect (0.61) for \\(y_2\\) and a small effect for \\(y_1\\) (0.11).\n\n\nWhy does it matter?\nThere are a number of reasons when this will matter. First of all, the coefficient (\\(\\beta = 3\\)) does not directly map onto the causal effect. The same coefficient can lead to different causal effects depending on the value of \\(x\\) and depending on other parts of the model (e.g. the intercept). That means that we cannot just rely on the coefficient.\nBecause the true effect depends on other parts of the model, generalised linear models make the use of fixed effects and robust standard errors more tricky as well. I am not going to go into the details of these issues but that is the overall problem with generalised linear model: the effect of each term is dependent on the other parts of the model.\nThere are two questions to ask whether this is a problem for your research question.\n\nThe first question is whether you are interested in predicting the variable \\(y\\) or whether you are interested in estimating an effect on the variable \\(y\\). As I discussed in the introduction to machine learning, when we are mainly interested in prediction, the coefficients or effects are less important. In the case of predictions, we definitely want to use a model that does not allow to make impossible predictions (e.g. probabilities that are higher than 1).\nThe second question is whether we are interested in the effect on the linear scale (\\(\\alpha + \\beta x\\)) or on the transformed scale (\\(g(\\alpha + \\beta x)\\)). In the working example I have used so far, you probably would be interested in the transformed scale, i.e. the probability that the firm has a female CEO. In a lot of cases in accounting and finance, we would be interested directly in those probabilities e.g. the probability that a firm goes bankrupt, or discloses certain information. In contrast, studies in consumer finance or behavioural economics are more interested in the utility that consumers derive from certain interventions. While the utility is often unobserved, we can observe the consumers choices (e.g. which mortgage they choose). A typical approach is to model the unobserved utility on the a linear scale (\\(z\\)) and model the probability of a choice as a transformation of the utility (\\(g(z)\\)). In fact, the logistic transformation is the workhorse function in this literature. Finally, there is literature that prefers the linear scale for logistic models because they argue that we can interpret the effects on the log odds scale, \\(\\textrm{log}(\\frac{p}{1-p})\\). That is, the effect on the linear scale represents an effect on the relative probability.1 Betting markets often present winning probabilities as odds or relative probabilities."
  },
  {
    "objectID": "generalised/interpretation.html#solution-without-controls",
    "href": "generalised/interpretation.html#solution-without-controls",
    "title": "Interpretation",
    "section": "Solution without controls",
    "text": "Solution without controls\nIn the next, section I am going to assume that we are only interested in the effect on the probability scale and we would like to summarise the effect in one number. We will start with the simplest case where the variable of interest is a simple treatment (\\(x = 1\\)) versus control (\\(x = 0\\)). We generate data according to the two functions above. In this simple case, it’s relatively easy to estimate the causal effects by just looking at the difference between the treatment and the control condition for both variables.\n\nsolutions &lt;-\n  tibble(x = rbinom(N, 1, 0.5)) %&gt;%\n  mutate(y1 = rbinom(N, 1, plogis(2 + 3 * x)),\n         y2 = rbinom(N, 1, plogis(-2 + 3 * x)))\n\nsolutions %&gt;%\n  summarise(across(c(y1, y2), mean), .by = x)\n\n# A tibble: 2 × 3\n      x    y1    y2\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 0.986 0.708\n2     0 0.886 0.136\n\n\nYou can see that the differences are pretty close to the theoretical effects y1_effect = 0.11 and y2_effect = 0.61 that we calculated before.\n\nLinear model\nThe next step is to figure out which regression approach gives us similar estimates. You can see that the OLS model, where we ignore that the outcome variable is restricted, gives us the estimate that we are interested in. To account for the distribution of the outcome variable, it is a good idea to specify se = \"hetero\". This way, feols uses a more robust estimation for the standard errors, accounting for the likely non-normal distribution of the error term.\nThe code also shows who you can run a logit or probit regression in R with the glm function. The main difference with the lm function is that you need to specify the family of models and the transformation link.\n\nols1 &lt;- feols(y1 ~ x, data = solutions, se = \"hetero\")\nlogit1 &lt;- glm(y1 ~ x, data = solutions, family = binomial(link = logit))\nprobit1 &lt;- glm(y1 ~ x, data = solutions, family = binomial(link = probit))\nols2 &lt;- feols(y2 ~ x, data = solutions, se = \"hetero\")\nlogit2 &lt;- glm(y2 ~ x, data = solutions, family = binomial(link = logit))\nprobit2 &lt;- glm(y2 ~ x, data = solutions, family = binomial(link = probit))\nmsummary(list(ols1 = ols1, logit1 = logit1, probit1 = probit1,\n              ols2 = ols2, logit2 = logit2, probit2 = probit2),\n         gof_map = gof_map, fmt = 2)\n\n\n\n\n\nols1\nlogit1\nprobit1\n ols2\n logit2\n probit2\n\n\n\n\n(Intercept)\n0.89\n2.05\n1.21\n0.14\n−1.85\n−1.10\n\n\n\n(0.01)\n(0.14)\n(0.07)\n(0.02)\n(0.13)\n(0.07)\n\n\nx\n0.10\n2.22\n1.00\n0.57\n2.73\n1.64\n\n\n\n(0.02)\n(0.41)\n(0.16)\n(0.03)\n(0.16)\n(0.09)\n\n\nNum.Obs.\n1001\n1001\n1001\n1001\n1001\n1001\n\n\nR2\n0.043\n\n\n0.333\n\n\n\n\n\n\n\n\n\n\n\nMarginal Effects\nYou can get the best of both worlds in this simple case. We can use the non-linear models to give predictions on the probability scale with fitted and then look at the average between the treatment and the control group.\n\nsolutions %&gt;%\n  mutate(pred_logit = fitted(logit1),\n         pred_probit = fitted(probit1)) %&gt;%\n  summarise(logit = mean(pred_logit),\n            probit = mean(pred_probit), .by = x)\n\n# A tibble: 2 × 3\n      x logit probit\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 0.986  0.986\n2     0 0.886  0.886\n\n\nWith the regression objects, we can use the marginaleffects package to get these estimates directly.\n\nlibrary(marginaleffects)\navg_comparisons(logit1, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %\n    x    1 - 0      0.1     0.0152 6.58   &lt;0.001 0.0705   0.13\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\navg_comparisons(probit1, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %\n    x    1 - 0      0.1     0.0152 6.58   &lt;0.001 0.0705   0.13\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high"
  },
  {
    "objectID": "generalised/interpretation.html#solution-with-controls",
    "href": "generalised/interpretation.html#solution-with-controls",
    "title": "Interpretation",
    "section": "Solution with controls",
    "text": "Solution with controls\nThings become more complicated when we introduce control variables. Remember, the effect of a variable is not constant in a non-linear model and it depends on other parts of the model. That means that the size of effect of x depends on the value of the control variable. Below, I simulate a dataset with a discrete control variable that takes three values (-1, 0, 1) with different probabilities. We can then run our three models and report them as before.\n\nsol_controls &lt;-\n  tibble(x = rbinom(N, 1, 0.5),\n         control = sample(c(-1, 0, 1), N, replace = TRUE,\n                          prob = c(0.5, 0.3, 0.2))) %&gt;%\n  mutate(y = rbinom(N, 1, plogis(-2 + 3 * x + 2 * control)))\n\nols_controls &lt;- feols(y ~ x + control, data = sol_controls,\n                      se = \"hetero\")\nlogit_controls &lt;- glm(y ~ x + control, data = sol_controls,\n                      family = binomial(link = logit))\nprobit_controls &lt;- glm(y ~ x + control, data = sol_controls,\n                       family = binomial(link = probit))\nmsummary(list(ols = ols_controls, logit = logit_controls,\n              probit = probit_controls),\n         gof_map = gof_map, fmt = 2)\n\n\n\n\n\nols\nlogit\nprobit\n\n\n\n\n(Intercept)\n0.24\n−1.99\n−1.10\n\n\n\n(0.02)\n(0.16)\n(0.09)\n\n\nx\n0.42\n3.19\n1.74\n\n\n\n(0.02)\n(0.24)\n(0.12)\n\n\ncontrol\n0.29\n2.06\n1.14\n\n\n\n(0.02)\n(0.15)\n(0.08)\n\n\nNum.Obs.\n1001\n1001\n1001\n\n\nR2\n0.418\n\n\n\n\n\n\n\n\n\nLet’s now calculate the effect for each value of the control variable 2. We use the same procedure but we just do it by control. We calculate the difference between the predicted probability for the control and treatment group. We also keep track of the number of observations in each cell. You can see in the printed intermediate calculation that the estimated effect differs for the three different values of the control variable. Because we want a one number summary, we can take the weighted (by number of observation) average of the effect to get the Average Marginal Effect.\n\nsol_controls %&gt;%\n  mutate(predictions = fitted(logit_controls)) %&gt;%\n  summarise(prob = mean(predictions), n = n(),\n            .by = c(x, control)) %&gt;%\n  pivot_wider(values_from = c(prob, n), names_from = x) %&gt;%\n  mutate(effect = prob_1 - prob_0, n = n_1 + n_0) %&gt;%\n  print() %&gt;%\n  summarise(AME = sum(effect * n)/sum(n))\n\n# A tibble: 3 × 7\n  control prob_0 prob_1   n_0   n_1 effect     n\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n1      -1 0.0171  0.296   249   254  0.279   503\n2       1 0.517   0.963    99   100  0.446   199\n3       0 0.120   0.768   146   153  0.648   299\n\n\n# A tibble: 1 × 1\n    AME\n  &lt;dbl&gt;\n1 0.422\n\n\nThere is another way of doing the calculation. We can also predict the probability assuming that an observation is in the treatment group (pred_x1) and assuming that an observation is in the control group (pred_x2). We can then calculate the Average Marginal Effect as the mean effect within the sample and we get a very similar result.\n\npred_new &lt;- function(x = 0, control = 1){\n  plogis(predict(logit_controls, newdata = tibble(x = x, control = control)))\n}\nsol_controls %&gt;%\n  mutate(pred_x1 = map_dbl(control, ~ pred_new(1, ..1)),\n         pred_x0 = map_dbl(control, ~ pred_new(0, ..1)),\n         effect = pred_x1 - pred_x0) %&gt;%\n  print() %&gt;%\n  summarise(AME = mean(effect))\n\n# A tibble: 1,001 × 6\n       x control     y pred_x1 pred_x0 effect\n   &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     0      -1     0   0.296  0.0171  0.279\n 2     1       1     1   0.963  0.517   0.446\n 3     1       0     1   0.768  0.120   0.648\n 4     0       1     0   0.963  0.517   0.446\n 5     0      -1     0   0.296  0.0171  0.279\n 6     1       1     1   0.963  0.517   0.446\n 7     0      -1     0   0.296  0.0171  0.279\n 8     0      -1     0   0.296  0.0171  0.279\n 9     1      -1     1   0.296  0.0171  0.279\n10     0      -1     0   0.296  0.0171  0.279\n# ℹ 991 more rows\n\n\n# A tibble: 1 × 1\n    AME\n  &lt;dbl&gt;\n1 0.422\n\n\nThe last approach is what the avg_comparisons package does. One advantage of the last approach is that it scales better if we have multiple control variables and some of them are continuous. The predictions will account for the heterogeneity in the effect because of the differences in the control variables. The flip side of all of this is that the estimate really depends on the sample. A different sample, with radically different values for the control variables will have a different estimate for the effect of interest.\n\navg_comparisons(logit_controls, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n    x    1 - 0    0.422     0.0225 18.8   &lt;0.001 0.378  0.466\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\navg_comparisons(probit_controls, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n    x    1 - 0    0.419     0.0229 18.3   &lt;0.001 0.374  0.464\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the linear model ols_controls recovers the average marginal effect directly in the regression. If you are interested in a treatment effect of a treatment compared to a control group, in my opinion, you should just use a linear model with robustly estimated standard errors. The linear model makes it easy to incorporate fixed effects, deal with panel data, and expand the model to two-stage-least-squares or difference-in-differences while giving the estimate that you care about.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe average marginal effect calculation highlights that the effect that we estimate depends on the values of the control variables. As a result, we need to be careful about extrapolating the effect to different settings where the control variables might take different values.\n\n\n\nSolution with continuous variable\nThe case of a continuous variable x is slightly more complicated because we already know that the relation between the outcome variable y and x is not perfectly linear. However, even with a continuous outcome variable, we also have to make the assumption of a linear relation. However, if we can make the assumption that the relation between y and x is roughly linear, the OLS estimate will be a good approximation of the average marginal effect as we can see below. As before, we can easily incorporate all the benefits of using feols while getting the estimate that we are targeting.\n\nsol_continuous &lt;-\n  tibble(x = rnorm(N, 0, 1),\n         control = rnorm(N, 0, 1)) %&gt;%\n  mutate(y = rbinom(N, 1, plogis(-2 + 3 * x + 2 * control)))\n\nols_continuous &lt;- feols(y ~ x + control, data = sol_continuous,\n                        se = \"hetero\")\nlogit_continuous &lt;- glm(y ~ x + control, data = sol_continuous,\n                      family = binomial(link = logit))\nprobit_continuous &lt;- glm(y ~ x + control, data = sol_continuous,\n                       family = binomial(link = probit))\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nmsummary(list(ols = ols_continuous, logit = logit_continuous,\n              probit = probit_continuous),\n         gof_map = gof_map, fmt = 2)\n\n\n\n\n\nols\nlogit\nprobit\n\n\n\n\n(Intercept)\n0.30\n−2.41\n−1.35\n\n\n\n(0.01)\n(0.18)\n(0.09)\n\n\nx\n0.26\n3.46\n1.92\n\n\n\n(0.01)\n(0.25)\n(0.13)\n\n\ncontrol\n0.19\n2.43\n1.35\n\n\n\n(0.01)\n(0.19)\n(0.10)\n\n\nNum.Obs.\n1001\n1001\n1001\n\n\nR2\n0.487\n\n\n\n\n\n\n\n\n\nTo get the average marginal effect with a continuous x, we use the avg_slopes function from marginaleffects. The function does something similar as avg_comparisons does for the discrete x. It will predict the probability of the outcome y for x and for x + 0.0013 and then take the difference. In other words, the function will approximate the estimate slope between y and x for each observations. The average marginal effect is again the average for the whole sample.\nThe fact that the calculation needs to be done for every observation can be a disadvantage of you have a lot of observations and a lot of effects to estimate.\n\navg_slopes(logit_continuous, variables = \"x\")\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n    x    0.265    0.00703 37.6   &lt;0.001 0.251  0.278\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high \n\navg_slopes(probit_continuous, variables = \"x\")\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n    x    0.263    0.00703 37.4   &lt;0.001 0.249  0.276\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high \n\n\nAll estimates point to the following effect: if x increases by .01, we expect the probability of y to increase by about .25% in our sample."
  },
  {
    "objectID": "generalised/interpretation.html#further-information",
    "href": "generalised/interpretation.html#further-information",
    "title": "Interpretation",
    "section": "Further information",
    "text": "Further information\nThere is a lot more that can be done to with marginal effects and they can also be used to estimate non-linear effects in linear models. Andrew Heiss has a good overview of the theory and the marginaleffects package has excellent vignettes on how to implement the different possibilities."
  },
  {
    "objectID": "generalised/interpretation.html#footnotes",
    "href": "generalised/interpretation.html#footnotes",
    "title": "Interpretation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\[\n\\begin{aligned}\np &= \\frac{e^z}{1 + e^z} \\\\\np &= \\frac{1}{e^{-z} + 1} \\\\\n\\frac{1}{p} &= e^{-z} + 1 \\\\\n\\frac{1 - p}{p} &= e^{-z} \\\\\n\\frac{p}{1-p} &= e^z \\\\\n\\textrm{log}(\\frac{p}{1-p}) &= z\n\\end{aligned}\n\\]↩︎\nThis is the reason why I used a discrete control variable. It makes all of this a bit easier to illustrate.↩︎\nor another amount that can be chosen in the function.↩︎"
  },
  {
    "objectID": "machine_learning/predictions.html",
    "href": "machine_learning/predictions.html",
    "title": "Predictions",
    "section": "",
    "text": "Introduction\nSo far in the course, I have made the assumption that our research is mainly interested in estimating a certain parameter of interest, i.e. we want to know the effect of a certain variable on a certain outcome variable. For instance, we are interested in the effect of a policy uncertainty on a firm’s investment decisions (Falk and Shelton 2018), or the use of options in CEO compensation on risk taking (Shue and Townsend 2017). In terms of a regression model, \\(y_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i\\), we are interested in \\(\\beta\\), the effect of \\(x\\) on \\(y\\) after controlling for \\(z\\). All the statistical and conceptual tools in the lectures had the aim of making sure that the estimate \\(\\hat{\\beta}\\) is the best possible estimate of the true underlying and unobserved \\(\\beta\\) (Mullainathan and Spiess 2017).\nIn contrast, some research questions focus on building a model to predict the outcomes. In that case, we are not interested in the estimated parameter \\(\\beta\\) (or \\(\\alpha\\) or \\(\\gamma\\)). We are interested in the prediction \\(\\hat{y}\\), especially for future or unobserved cases (Mullainathan and Spiess 2017). One of the biggest risks for prediction tasks is that our model excels at predicting the outcomes in the data that we have but it is rubbish at predicting for new data. In that case, our model is overfitting for the purpose of predicting future outcomes. The strength of a lot of machine learning applications is that they excel at incorporating a lot of variables while at the same time guard against overfitting. If prediction is the research question these are the methods we should turn to.\n\n\nContent\nFor this course, I will give a very gentle introduction to the tidymodels framework from the same team as the tidyverse. The framework provides a unified workflow to work with different machine learning (and traditional regression) models for prediction tasks. I will not use it for a typical prediction task. I will use it for the research question whether (initial) stock price reactions to Friday earnings announcements are muted (Dellavigna and Pollet 2009).\nFor that research question, we want to estimate the abnormal return after a company’s earnings announcement. That means that we need to predict what the counterfactual return would have been in the absence of an announcement. That is a prediction task! We did not really care whether we correctly estimated the market beta or the effect of the factors. We cared whether we predicted the expected returns correctly. When we only have one or three predictors the advantage of the machine learning methods is likely to be minimal but we could use more predictors. The market return and the factors are essentially stand ins for controlling for other factors besides that announcement that could effect the return. A straight forward extension of the factor models would be to include the returns of a number of peer firms as predictors and allow the weight (i.e. the betas) to vary (Baker and Gelbach 2020). In this case, we might have a lot of predictors and only a limited amount of observations to estimate the betas. This is a classical case where we might be at risk of overfitting.\n\n\nOther Applications\nThe inclusion of abnormal returns is a specific application of a more general use case for machine learning techniques in accounting and finance research. Sometimes, we are not interested in some parameters. We just want to include (control) predictors to remove some potential confounding effect. For instance, in my original regression model, if we have a lot of variables \\(z\\) but we don’t care about the parameters \\(\\gamma\\), we can use machine learning techniques to include more variables than a typical regression would allow (Mullainathan and Spiess 2017).\n\\[\ny_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i\n\\]\nOne way to think about this is that we are splitting up the regression in two steps. First, we predict the outcome variable, \\(\\hat{y}\\) based on the many predictors \\(z\\). Second, we estimate the parameter \\(\\beta\\) with \\(y - \\hat{y}\\) as the dependent variable (Mullainathan and Spiess 2017).\nAnother related application is that we want to use the machine learning techniques to predict the causal variable of interest with the predictors z, so that we can use \\(x - \\hat{x}\\) as the new causal variable and estimate it’s effect on \\(y - \\hat{y}\\). For some research questions, it will be appropriate to say that after we removed the influence of a bunch of observable factors, we are more confident that the remaining variation in \\(x\\) and \\(y\\) is the capturing the causal effect we are interested in. However, remember that sometimes the opposite is true. In the (Shue and Townsend 2017) paper, we wanted to predict the changes in the option awards that followed a predictable schedule. The paper essentially used a very simple prediction model to predict the option awards that we could treat as if they are random (or exogenous).\nA final application is when you want to increase the dataset when you need to collect some variable by manually labelling or categorising some data. Sometimes it is possible to collect the data manually for a subset of the data. You can then use the machine learning tools to predict the key variable based on the relation between predictors and your measure in the manually collected data.\n\n\n\n\n\nReferences\n\nBaker, Andrew, and Jonah B. Gelbach. 2020. “Machine Learning and Predicted Returns for Event Studies in Securities Litigation.” Journal of Law, Finance, and Accounting 5 (2): 231–72. https://doi.org/10.1561/108.00000047.\n\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x.\n\n\nFalk, Nathan, and Cameron A. Shelton. 2018. “Fleeing a Lame Duck: Policy Uncertainty and Manufacturing Investment in US States.” American Economic Journal: Economic Policy 10 (4): 135–52. https://doi.org/10.1257/pol.20160365.\n\n\nMullainathan, Sendhil, and Jann Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106. https://doi.org/10.1257/jep.31.2.87.\n\n\nShue, Kelly, and Richard R. Townsend. 2017. “How Do Quasi-Random Option Grants Affect CEO Risk-Taking?” The Journal of Finance 72 (6): 2551–88. https://doi.org/10.1111/jofi.12545."
  },
  {
    "objectID": "machine_learning/application.html",
    "href": "machine_learning/application.html",
    "title": "Peer Firms",
    "section": "",
    "text": "In this section, I will show how you can code a prediction model for the return of a company’s stock price based on the returns of a number of peer companies. I loosely follow the same approach as in Baker and Gelbach (2020). In that paper, peer firms are identified based on the SIC code in the Compustat data which indicates what the industry is of the firm’s primary product.\nWe are using the tidymodel package for the application and I will follow the introduction to tidymodels. The advantage of the tidymodels approach is that you can follow a very similar workflow for other machine learning methods than the one I will be showing here."
  },
  {
    "objectID": "machine_learning/application.html#get-some-data",
    "href": "machine_learning/application.html#get-some-data",
    "title": "Peer Firms",
    "section": "Get Some Data",
    "text": "Get Some Data\nWe are going to use some example data from one earnings announcement and we are going to use data from an earnings announcement where the firm has a lot of peers because in that situation prediction is more likely to suffer from overfitting. The data_ml data excludes all peers with missing observations because most of the algorithms do not deal well with missing values. There are more sophisticated approaches possible to deal with missing data but they are not necessary to illustrate the main workflow.\n\npeers &lt;- readRDS(here(\"data\", \"machine_learning\", \"peers.RDS\")) %&gt;%\n  filter(N == max(N))\nglimpse(peers)\n\nRows: 4\nColumns: 6\n$ gvkey   &lt;chr&gt; \"063799\", \"063799\", \"063799\", \"017208\"\n$ permno  &lt;dbl&gt; 84058, 84058, 84058, 83729\n$ anndat  &lt;date&gt; 2003-01-23, 2003-04-22, 2003-07-24, 2003-04-18\n$ permnos &lt;list&gt; &lt;62770, 65138, 20694, 85073, 22032, 86685, 23916, 35167, 36469…\n$ N       &lt;dbl&gt; 200, 200, 200, 200\n$ data    &lt;list&gt; [&lt;tbl_df[157 x 199]&gt;], [&lt;tbl_df[156 x 200]&gt;], [&lt;tbl_df[155 x 2…\n\ndata_ml &lt;- pull(peers, data)[[1]] %&gt;%\n  select_if(~!any(is.na(.)))\nprint(data_ml)\n\n# A tibble: 157 × 193\n    `10002`  `10304`  `10562`  `10563`  `10588`   `10623`  `10725`  `10825`\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0508  -0.00128  0.00498  0.00394 -0.00187 -0.0106    0.00609 -0.0147 \n 2  0        0.00511  0.00142  0        0.00900  0         0.00700 -0.00199\n 3  0.0242  -0.00551  0.00566 -0.0196   0.00747 -0.00392  -0.00474  0.0115 \n 4  0.0142   0.00469  0.00141  0.0299   0.0221   0.000357  0.0133   0.00296\n 5  0.0349  -0.00551  0.00351  0        0.0227   0.025     0.0282  -0.00246\n 6  0.00674  0.0128  -0.00210 -0.0275   0.0113   0.0157    0.0146   0.0173 \n 7  0.0179  -0.0105   0.00421  0.00806  0.0192   0.0257    0.0231   0.0155 \n 8  0.0263   0.0195   0.0216   0.016    0.0137   0.00836   0.0349   0.00382\n 9  0.0231  -0.0187  -0.0219   0        0.0287  -0.00166  -0.0182   0.00429\n10 -0.0141   0.0187   0.0238   0.0331   0.00952  0.0498    0.0142   0.0491 \n# ℹ 147 more rows\n# ℹ 185 more variables: `10906` &lt;dbl&gt;, `10913` &lt;dbl&gt;, `10916` &lt;dbl&gt;,\n#   `11216` &lt;dbl&gt;, `11348` &lt;dbl&gt;, `11389` &lt;dbl&gt;, `11634` &lt;dbl&gt;, `11808` &lt;dbl&gt;,\n#   `11823` &lt;dbl&gt;, `16644` &lt;dbl&gt;, `20694` &lt;dbl&gt;, `22032` &lt;dbl&gt;, `23326` &lt;dbl&gt;,\n#   `23916` &lt;dbl&gt;, `24628` &lt;dbl&gt;, `27254` &lt;dbl&gt;, `35167` &lt;dbl&gt;, `35503` &lt;dbl&gt;,\n#   `35829` &lt;dbl&gt;, `35917` &lt;dbl&gt;, `36469` &lt;dbl&gt;, `39766` &lt;dbl&gt;, `41807` &lt;dbl&gt;,\n#   `44688` &lt;dbl&gt;, `47159` &lt;dbl&gt;, `52265` &lt;dbl&gt;, `52840` &lt;dbl&gt;, …"
  },
  {
    "objectID": "machine_learning/application.html#linear-model",
    "href": "machine_learning/application.html#linear-model",
    "title": "Peer Firms",
    "section": "Linear model",
    "text": "Linear model\nWe first going to run a traditional linear model within the tidymodels framework. The approach is overkill for just running a linear model but it will help us to build up the full workflow. In the code, I first split the data in a proportion \\(\\frac{N-20}{N}\\) of training data and a proportion \\(\\frac{20}{N}\\) of test data, where \\(N\\) is the number of days of data available. As a result, we will use the training data to determine the parameters and than use that model to predict the last 20 days as test data. Next, we set which type of model we want to use, a linear model. We than fit the model where we specify the return variable as the outcome variable and we use the dot . to indicate that we want to use all the other variables as predictors. Finally, the tidy function gives the estimates and statistics for the estimates in a format that works with the tidyverse.\n\nN &lt;- nrow(data_ml)\ndata_split &lt;- initial_time_split(data_ml, prop = (N - 20)/N)\ntrain &lt;- training(data_split)\ntest &lt;- testing(data_split)\nlm_mod &lt;- linear_reg()\nlm_fit &lt;- lm_mod %&gt;%\n  fit(return ~ ., data = train)\ntidy(lm_fit)\n\n# A tibble: 193 × 5\n   term         estimate std.error statistic p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  -0.00257       NaN       NaN     NaN\n 2 `10002`       2.23          NaN       NaN     NaN\n 3 `10304`     -21.6           NaN       NaN     NaN\n 4 `10562`      -5.43          NaN       NaN     NaN\n 5 `10563`      -2.23          NaN       NaN     NaN\n 6 `10588`      -5.37          NaN       NaN     NaN\n 7 `10623`       1.65          NaN       NaN     NaN\n 8 `10725`       5.77          NaN       NaN     NaN\n 9 `10825`      -5.45          NaN       NaN     NaN\n10 `10906`       9.65          NaN       NaN     NaN\n# ℹ 183 more rows\n\n\nYou can immediately see that the model does not report any standard errors or p-values. This happens because we have more predictors than we have trading days and thus we can perfectly fit the in-sample returns. This type of data is exactly why would want to use a regularised regression approach.\nBefore we do that, we need to fix another issue. In the previous code, we used the day variable as a predictor which was not what we intended. You can see that we can get an estimate for day in the code below. Also notice the advantage of using tidy representation of the estimated coefficients is that we can use filter to focus on some terms. To solve the issue with the day variable, we will use a recipe to set the formula and do some data cleaning. Specifically, we will specify that the day variable is an ID variable which means that it should not be used for prediction but we want to keep it in the data for investigating the fit later.\n\ntidy(lm_fit) %&gt;%\n  filter(term == \"day\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 day         NA        NA        NA      NA\n\nlm_recipe &lt;- recipe(return ~ ., data = train) %&gt;%\n  update_role(day, new_role = \"ID\")\nsummary(lm_recipe)\n\n# A tibble: 193 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 10002    &lt;chr [2]&gt; predictor original\n 2 10304    &lt;chr [2]&gt; predictor original\n 3 10562    &lt;chr [2]&gt; predictor original\n 4 10563    &lt;chr [2]&gt; predictor original\n 5 10588    &lt;chr [2]&gt; predictor original\n 6 10623    &lt;chr [2]&gt; predictor original\n 7 10725    &lt;chr [2]&gt; predictor original\n 8 10825    &lt;chr [2]&gt; predictor original\n 9 10906    &lt;chr [2]&gt; predictor original\n10 10913    &lt;chr [2]&gt; predictor original\n# ℹ 183 more rows\n\n\nLet’s put everything together in a workflow, fit the workflow and make predictions for the test data.\n\nlm_workflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_mod) %&gt;%\n  add_recipe(lm_recipe)\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(data = train)\npreds &lt;- predict(lm_fit, test, type = \"numeric\")\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\nglimpse(preds)\n\nRows: 20\nColumns: 1\n$ .pred &lt;dbl&gt; -1.0978031, -0.8385601, 2.2026579, -1.1725332, -0.4099225, 0.490…\n\n\nThe predictions give us 20 predictions for the days. We can also calculate the RMSE for the training data (in-sample fit) and the test data (out-of-sample fit).\n\naugment(lm_fit, train) %&gt;%\n  rmse(return, .pred)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard    8.83e-16\n\naugment(lm_fit, test) %&gt;%\n  rmse(return, .pred)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        1.47\n\n\nWhile the RMSE is almost 0 within the sample, it’s 1.47 out-of-sample. That is, there is a large difference between the prediction error within the training data and the prediction error in the test data. This is the reason why we want to use a regularised regression to do out-of-sample predictions 2"
  },
  {
    "objectID": "machine_learning/application.html#elastic-net",
    "href": "machine_learning/application.html#elastic-net",
    "title": "Peer Firms",
    "section": "Elastic Net",
    "text": "Elastic Net\nIn this section, we are going to run the regularised regression as described in the theory. We will center and scale the data so that all returns have a mean of 0 and a standard deviation of 1. Remember that the penalty term punishes coefficients that are higher, however the size of the coefficient also depends on the variation in the variable. To put all the coefficients on equal footing, we scale them first.\nWe will set the penalty \\(\\lambda = .01\\) and the mixture proportion \\(\\alpha = 0.5\\) in the elastic net. We will need to install the glmnet package because that is the engine we use in our linear regression model. As before, we can put the updated data cleaning and the linear model in a new workflow and fit the model. Finally, we report the in-sample and out-of-sample RMSE.\n\nnet_recipe &lt;- lm_recipe %&gt;%\n  step_center(all_predictors()) %&gt;%\n  step_scale(all_predictors())\nnet_model &lt;- linear_reg(penalty = .01, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\")\nnet_workflow &lt;-\n  workflow() %&gt;%\n  add_model(net_model) %&gt;%\n  add_recipe(net_recipe)\nnet_fit &lt;- net_workflow %&gt;%\n  fit(data = train)\ntidy(net_fit) %&gt;% filter(estimate != 0)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-7\n\n\n# A tibble: 3 × 3\n  term          estimate penalty\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.000102     0.01\n2 76733        0.000169     0.01\n3 87218        0.0000706    0.01\n\npredict(net_fit, test, type = \"numeric\")\n\n# A tibble: 20 × 1\n        .pred\n        &lt;dbl&gt;\n 1  0.000138 \n 2  0.000145 \n 3 -0.000432 \n 4  0.0000165\n 5 -0.0000674\n 6 -0.000210 \n 7 -0.000243 \n 8 -0.0000165\n 9 -0.000350 \n10  0.000184 \n11 -0.000350 \n12  0.000175 \n13 -0.000147 \n14 -0.000223 \n15  0.000234 \n16 -0.000164 \n17 -0.0000728\n18 -0.000120 \n19 -0.000269 \n20 -0.000194 \n\naugment(net_fit, train) %&gt;%\n  rmse(return, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0241\n\naugment(net_fit, test) %&gt;%\n  rmse(return, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0278\n\n\nIn contrast to the ordinary linear model, we see that the regularised model has a similar prediction error in-sample and out-of-sample. Furthermore, the out-of-sample prediction is much better with the regularised regression."
  },
  {
    "objectID": "machine_learning/application.html#tuning-elastic-net",
    "href": "machine_learning/application.html#tuning-elastic-net",
    "title": "Peer Firms",
    "section": "Tuning Elastic Net",
    "text": "Tuning Elastic Net\nThe problem with the previous analysis is that we had to set the penalty and mixture values. We can do better and test multiple values and see which ones give us the best prediction. This process is called tuning of the hyper parameters and it is a standard procedure in many machine learning applications.\nThe above code shows the general principle. We are dividing the training data in 10 randomly selected folds. We use the same workflow as above but we use the part of the training data that is not in a fold to predict the data in the fold. Now, we have 10 out-of-sample RMSEs. With collect_metrics, we get an overview of the mean RMSE and the standard error around the mean.\n\nfolds &lt;- vfold_cv(train, v = 10)\nnet_fit_rf &lt;-\n  net_workflow %&gt;%\n  fit_resamples(folds, metrics = metric_set(rmse))\ncollect_metrics(net_fit_rf)\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.0238    10 0.00184 Preprocessor1_Model1\n\n\nWhile the randomly selected folds is often appropriate, in our case it is not. We have a time series and we want to predict future returns based on earlier returns. So, we are going to use a different function to create rolling windows of 75 days of data to build the model to predict the next 5 days. So for instance, we use a day 1-75 to predict day 76-80. Next, we use day 6-80 to predict day 81-85. We repeat the procedure until we run out of data. When we print windows, you can see that we have 10 splits with 75 modeling observations and 5 observations that we want to predict.\n\nwindows = rolling_origin(train, initial = 75, assess = 5,\n                         skip = 5, cumulative = FALSE)\nprint(windows)\n\n# Rolling origin forecast resampling \n# A tibble: 10 × 2\n   splits         id     \n   &lt;list&gt;         &lt;chr&gt;  \n 1 &lt;split [75/5]&gt; Slice01\n 2 &lt;split [75/5]&gt; Slice02\n 3 &lt;split [75/5]&gt; Slice03\n 4 &lt;split [75/5]&gt; Slice04\n 5 &lt;split [75/5]&gt; Slice05\n 6 &lt;split [75/5]&gt; Slice06\n 7 &lt;split [75/5]&gt; Slice07\n 8 &lt;split [75/5]&gt; Slice08\n 9 &lt;split [75/5]&gt; Slice09\n10 &lt;split [75/5]&gt; Slice10\n\nnet_fit_windows &lt;-\n  net_workflow %&gt;%\n  fit_resamples(windows, metrics = metric_set(rmse))\ncollect_metrics(net_fit_windows)\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.0176    10 0.00367 Preprocessor1_Model1\n\n\nAgain, we find that the regularised version gives better out-of-sample predictions than the non-regularised version. In the next step, we will use different values for the penalty and the mixture. With the same procedure as before, we can estimate the mean out-of-sample RMSE for each combination of the penalty and mixture and then decide to use the combination that leads to the best predictions. This process is also called cross validation.\nFirst, we specify that we want a linear model with two hyper-parameters, the penalty and the mixture, that need to be tuned during the cross validation process. I specify 6 possible values for the penalty and 11 for the mixture. Next we specify the workflow and then fit the model over the grid of all 66 possible values of the penalty and the mixture.\n\nnet_tune_spec &lt;-\n  linear_reg(\n    penalty = tune(),\n    mixture = tune()\n  ) %&gt;%\n  set_engine(\"glmnet\")\ngrid_hyper &lt;- expand.grid(\n  penalty = c(0.0025, 0.005, 0.01, 0.02, 0.04, 0.1),\n  mixture = seq(0, 1, length.out = 11)\n)\nnet_tune_wf &lt;-\n  workflow() %&gt;%\n  add_recipe(net_recipe) %&gt;%\n  add_model(net_tune_spec)\n\nnet_tune &lt;-\n  net_tune_wf %&gt;%\n  tune_grid(\n    resamples = windows,\n    grid = grid_hyper,\n    metrics = metric_set(rmse)\n  )\n\nNext, we collect the RMSE measures for the out-of-sample predictions for each combination of penalty and mixture. We see that a lot of models with some regularisation give the best out-of-sample predictions. In a more realistic, complicated example, I would expect there to be one combination that is the best.\n\ncollect_metrics(net_tune) %&gt;%\n  arrange(-mean)\n\n# A tibble: 66 × 8\n   penalty mixture .metric .estimator   mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1  0.0025     0.1 rmse    standard   0.0264    10 0.00433 Preprocessor1_Model07\n 2  0.0025     0.2 rmse    standard   0.0252    10 0.00404 Preprocessor1_Model13\n 3  0.005      0.1 rmse    standard   0.0244    10 0.00403 Preprocessor1_Model08\n 4  0.0025     0.3 rmse    standard   0.0239    10 0.00373 Preprocessor1_Model19\n 5  0.0025     0.4 rmse    standard   0.0230    10 0.00357 Preprocessor1_Model25\n 6  0.0025     0.5 rmse    standard   0.0226    10 0.00344 Preprocessor1_Model31\n 7  0.005      0.2 rmse    standard   0.0224    10 0.00359 Preprocessor1_Model14\n 8  0.0025     0.6 rmse    standard   0.0221    10 0.00335 Preprocessor1_Model37\n 9  0.01       0.1 rmse    standard   0.0217    10 0.00366 Preprocessor1_Model09\n10  0.0025     0.7 rmse    standard   0.0215    10 0.00330 Preprocessor1_Model43\n# ℹ 56 more rows\n\n\nNext, we select the best combination of penalty and mixture and use the final model on all the training data. We can then evaluate the prediction quality based on the RMSE metric.\n\nbest_model &lt;- select_best(net_tune, \"rmse\")\nbest_model\n\n# A tibble: 1 × 3\n  penalty mixture .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     0.1     0.2 Preprocessor1_Model18\n\nfinal_wf &lt;- net_tune_wf %&gt;%\n  finalize_workflow(best_model)\nfinal_fit &lt;-\n  final_wf %&gt;%\n  last_fit(data_split, metrics = metric_set(rmse))\nextract_fit_parsnip(final_fit) %&gt;% tidy %&gt;% filter(estimate != 0)\n\n# A tibble: 1 × 3\n  term         estimate penalty\n  &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.000102     0.1\n\ncollect_metrics(final_fit)\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      0.0278 Preprocessor1_Model1\n\n\nFinally, we can calculate the predictions for the test set which we split off at the beginning and add the day column from the test set. We can than graph the deviations between the actual return and the predicted return for the 25 days in the test data.\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\naug_fit &lt;- bind_cols(collect_predictions(final_fit), select(test, day))\nggplot(aug_fit, aes(x = day, y = return - .pred)) +\n  geom_point()\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\nI can see no clear patterns in the deviations between the actual return and the predicted values."
  },
  {
    "objectID": "machine_learning/application.html#the-data",
    "href": "machine_learning/application.html#the-data",
    "title": "Peer Firms",
    "section": "The data",
    "text": "The data\nI rewrote the function to get the returns necessary for the machine learning model. The biggest difficulty is to make sure that we have the correct dates because the training data and the test data are not an uninterrupted time series. So, the extra parameters are mainly to deal with the end and start day relative to the earnings date. At the end of the data cleaning pipe, I also delete all the columns with missing data. Because, we have need to run the data for a lot of earnings announcements, we might end up with some less than ideal cases with missing data, insufficient variables, or missing returns for the firm that we are interested in. There are more elegant solutions to avoid getting errors for these issues but I will just try to avoid them or try to make sure that the code keeps running even when an error occurs for one of the earnings announcements.\nI left some of my test code at the end of the code chunk. It’s a good idea to have these quick tests when you write more extensive functions. Over time, you might need to update the functions and you want to make sure that they still work on examples that worked before.\n\nget_returns_event &lt;- function(anndat, permno_y, permnos,\n                        before_start = 300, before_end = 25,\n                        after_start = 0, after_end = 75\n                        ){\n  before_begin &lt;- anndat - before_start\n  before_last &lt;- anndat - before_end\n  after_begin &lt;- anndat + after_start\n  after_last &lt;- anndat + after_end\n  data &lt;- returns %&gt;%\n    # all dates within the range\n    filter(permno %in% permnos, date &gt;= before_begin, date &lt;= after_last) %&gt;%\n    # exclude the dates after training and before event\n    filter(date &lt; before_last | date &gt;= after_begin) %&gt;%\n    filter(!is.na(ret)) %&gt;%\n    pivot_wider(values_from = ret, id_cols = date, names_from = permno) %&gt;%\n    mutate(day = date - anndat) %&gt;%\n    select(-date) %&gt;%\n    arrange(day) %&gt;%\n    select_if(~!any(is.na(.)))\n\n  vars &lt;- names(data)\n  names(data) &lt;- if_else(vars == as.character(permno_y),\n                         \"return\", vars)\n  return(data)\n}\nget_returns_event(anndat, permnos[3], permnos) %&gt;%\n  arrange(-day)\n\n# A tibble: 244 × 3\n    `50906`   return day    \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;drtn&gt; \n 1  0.0194  -0.0184  74 days\n 2  0.0665   0.0155  73 days\n 3  0.0380   0.00323 72 days\n 4  0.0262  -0.0178  71 days\n 5  0.00620 -0.00316 70 days\n 6  0.0179  -0.00629 67 days\n 7  0.0485   0.0108  66 days\n 8 -0.00110 -0.0193  65 days\n 9  0.0231   0.0926  64 days\n10  0.0114  -0.00339 60 days\n# ℹ 234 more rows\n\nget_returns_event(anndat = peers$anndat[1],\n                  permno_y = peers$permno[1],\n                  permnos = peers$permnos[[1]])\n\n# A tibble: 243 × 192\n    `10002`  `10304`  `10562`  `10563`   `10623`  `10725`  `10825`   `10906`\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1  0.0508  -0.00128  0.00498  0.00394 -0.0106    0.00609 -0.0147  -0.00645 \n 2  0        0.00511  0.00142  0        0         0.00700 -0.00199  0.0138  \n 3  0.0242  -0.00551  0.00566 -0.0196  -0.00392  -0.00474  0.0115  -0.00791 \n 4  0.0142   0.00469  0.00141  0.0299   0.000357  0.0133   0.00296 -0.00532 \n 5  0.0349  -0.00551  0.00351  0        0.025     0.0282  -0.00246  0.00573 \n 6  0.00674  0.0128  -0.00210 -0.0275   0.0157    0.0146   0.0173   0.0137  \n 7  0.0179  -0.0105   0.00421  0.00806  0.0257    0.0231   0.0155  -0.000749\n 8  0.0263   0.0195   0.0216   0.016    0.00836   0.0349   0.00382  0.0124  \n 9  0.0231  -0.0187  -0.0219   0       -0.00166  -0.0182   0.00429 -0.0118  \n10 -0.0141   0.0187   0.0238   0.0331   0.0498    0.0142   0.0491   0.00824 \n# ℹ 233 more rows\n# ℹ 184 more variables: `10913` &lt;dbl&gt;, `10916` &lt;dbl&gt;, `11216` &lt;dbl&gt;,\n#   `11348` &lt;dbl&gt;, `11389` &lt;dbl&gt;, `11634` &lt;dbl&gt;, `11808` &lt;dbl&gt;, `11823` &lt;dbl&gt;,\n#   `16644` &lt;dbl&gt;, `20694` &lt;dbl&gt;, `22032` &lt;dbl&gt;, `23326` &lt;dbl&gt;, `23916` &lt;dbl&gt;,\n#   `24628` &lt;dbl&gt;, `27254` &lt;dbl&gt;, `35167` &lt;dbl&gt;, `35503` &lt;dbl&gt;, `35829` &lt;dbl&gt;,\n#   `35917` &lt;dbl&gt;, `36469` &lt;dbl&gt;, `39766` &lt;dbl&gt;, `41807` &lt;dbl&gt;, `44688` &lt;dbl&gt;,\n#   `47159` &lt;dbl&gt;, `52265` &lt;dbl&gt;, `52840` &lt;dbl&gt;, `56232` &lt;dbl&gt;, …\n\n\nThe next functions runs the full elastic net workflow with cross validation for the hyper parameters. As you can see in the test code at the end, you can just use the function above to get data and then pass it on to the function to run the full elastic net. I make sure that the training data is from before the event day and the test data is from after the event day. For the rolling prediction windows in the cross validation step I choose 100 observations to predict the next 15 days which will result in 5 windows in the typical situation. I also limit the number of hyper parameters to test, just to limit the computations. The remainder of the code is very similar to the code in the previous section for the single prediction task.\n\nrun_elastic_net &lt;- function(data, event_day = 0){\n  # split the data on day 0\n  n_total &lt;- nrow(data)\n  n_before &lt;- nrow(filter(data, day &lt; event_day))\n  prop &lt;- n_before/n_total\n  data_split &lt;- initial_time_split(data, prop = prop)\n  train &lt;- training(data_split)\n  test &lt;- testing(data_split)\n  # prepare data and windows \n  net_recipe &lt;-\n    recipe(return ~ ., data = train) %&gt;%\n    update_role(day, new_role = \"ID\") %&gt;%\n    step_center(all_predictors()) %&gt;%\n    step_scale(all_predictors())\n  folds &lt;- rolling_origin(train, initial = 100, assess = 15,\n                          skip = 15, cumulative = FALSE)\n  # set up the hyper parameters\n  net_tune_spec &lt;-\n    linear_reg(\n      penalty = tune(),\n      mixture = tune()\n    ) %&gt;%\n      set_engine(\"glmnet\")\n  grid_hyper &lt;- expand.grid(\n    penalty = c(0.0025, 0.005, 0.01, 0.02, 0.04),\n    mixture = seq(0, 1, length.out = 6))\n  # run model\n  net_tune_wf &lt;-\n    workflow() %&gt;%\n    add_model(net_tune_spec) %&gt;%\n    add_recipe(net_recipe)\n  net_tune &lt;-\n    net_tune_wf %&gt;%\n    tune_grid(\n      resamples = folds,\n      grid = grid_hyper,\n      metrics = metric_set(rmse))\n # Get best model\n  best_model &lt;- select_best(net_tune, \"rmse\")\n  final_wf &lt;- net_tune_wf %&gt;%\n    finalize_workflow(best_model)\n  final_fit &lt;-\n    final_wf %&gt;%\n    last_fit(data_split, metric_set(rmse))\n  return(final_fit)\n}\n\ntest &lt;- get_returns_event(anndat = peers$anndat[1],\n                  permno_y = peers$permno[1],\n                  permnos = peers$permnos[[1]]) %&gt;%\n        run_elastic_net(event_day = 0)\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: ''\n\n\nFinally, we can combine the two functions above in one function and calculate the abnormal returns as the difference between the prediction from the elastic net and actual return. The cumulative return is the the sum of those abnormal returns. Notice that the bulk of the function is wrapped in the tryCatch function. This is an R function that let’s you control what happens if an error occurs in the code it wraps. I use the function to return NA in the case that any of the functions gives an error.\n\ncalculate_car &lt;- function(anndat, permno_y, permnos,\n                         before_start = 300, before_end = 25,\n                         after_start = 0, after_end = 75,\n                         event_day = 0, car_short_last = 1){\n  result &lt;- NA\n  tryCatch(\n    {data &lt;- get_returns_event(anndat, permno_y, permnos,\n                              before_start, before_end,\n                              after_start, after_end)\n    fit &lt;- run_elastic_net(data = data, event_day = event_day)\n    result &lt;- augment(fit) %&gt;%\n      select(return, .pred, day) %&gt;%\n      mutate(ar = return - .pred) %&gt;%\n      mutate(window = if_else(day &lt;= car_short_last, \"short\", \"long\")) %&gt;%\n      summarise(car = sum(ar), .by = window)},\n    error = function(err) {return(NA)})\n  return(result)\n}\n\ncalculate_car(anndat = peers$anndat[1], permno_y = peers$permno[1], permnos = peers$permnos[[1]],\n              event_day = 0)\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: ''\n\n\n# A tibble: 2 × 2\n  window     car\n  &lt;chr&gt;    &lt;dbl&gt;\n1 short  -0.0103\n2 long   -0.0108\n\n\nIn the last code block, we run the functions for N_announce earnings announcements. The code is very similar to the original abnormal return code. We first construct the data in a way that allows us to use the functions we just created. The last mutate step then runs the function in parallel depending on how many cores that are available in our computer. The reason I chose to only save the CARs is that in this way the function in future_map does not have to pass on huge amount of data as input or output which generally improves performance.\n\nN_announce &lt;- 500\ncores &lt;- parallel::detectCores()\ntictoc::tic()\nplan(multisession, workers = cores - 2)\ncars &lt;- earn_ann %&gt;%\n  head(N_announce) %&gt;%\n  left_join(compu_sic_new, by = join_by(gvkey)) %&gt;%\n  filter(!is.na(sic_new)) %&gt;%\n  left_join(compu_sic_new, by = join_by(sic_new),\n            relationship = \"many-to-many\",\n            suffix = c(\"\", \"_peer\")) %&gt;%\n  left_join(linking_table,\n            by = join_by(gvkey_peer == gvkey,\n                         between(anndat, start_date, end_date)),\n            suffix = c(\"\", \"_peer\")) %&gt;%\n  filter(!is.na(permno_peer)) %&gt;%\n  select(gvkey, permno, anndat, permno_peer) %&gt;%\n  distinct(.) %&gt;%\n  summarise(permnos = list(permno_peer),\n            .by = c(gvkey, permno, anndat)) %&gt;%\n  mutate(N = map_dbl(permnos, ~ length(.x))) %&gt;%\n  filter(N &gt; minimum_peers) %&gt;%\n  mutate(car = future_pmap(list(anndat, permno, permnos),\n                            ~ calculate_car(..1, ..2, ..3, event_day = 0),\n                           .options = furrr_options(seed = 230383),\n                           .progress = TRUE))\ntictoc::toc()\n\n237.189 sec elapsed"
  },
  {
    "objectID": "machine_learning/application.html#footnotes",
    "href": "machine_learning/application.html#footnotes",
    "title": "Peer Firms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr maybe it should not be that surprising anymore at this point.↩︎\nAlso notice the warning message. The rank-deficient fit is a consequence of having more predictors than observation days. Again, it reflects the difficulty of using an ordinary linear regression.↩︎"
  },
  {
    "objectID": "freaky_friday/regressions.html",
    "href": "freaky_friday/regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(modelsummary)\ngof_omit &lt;- \"Adj|RMS|IC\"\ni_am(\"freaky_friday/regressions.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-2",
    "href": "freaky_friday/regressions.html#table-2",
    "title": "Regressions",
    "section": "Table 2",
    "text": "Table 2\nThe tables do not really replicate which is interesting to me. For a number of reasons.\n\nThe results are more consistent. I wonder whether I got rid of more outliers earlier. Remember I did end up with less observations. One interpretation is that I have cleaned the data better, the other is that I got rid of important, influential observations by being too strict when cleaning the data.\nThe results for the short term CAR are consistent with the figure. Friday market reactions to bottom quantile surprises are more positive than non-friday market reactions and the sign flips for top quantile surprises.\nI also lose substantially more observations due to the inclusion of the volatility measures. I do not know exactly why that is the case.\n\n\nPanel A: Short Term CAR\n\nsubset &lt;- readRDS(here(\"data\", \"freaky_friday\", \"subset.RDS\"))\nmodel1a &lt;- feols(car_short ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2a &lt;- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3a &lt;- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,839 observations removed because of NA values (Fixed-effects: 6,839).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1a, model2a, model3a), gof_omit = gof_omit, stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n−0.036***\n\n\n\n\n\n(0.001)\n\n\n\n\nfriday\n0.014***\n0.012***\n0.013**\n\n\n\n(0.003)\n(0.003)\n(0.004)\n\n\ntop\n0.061***\n\n\n\n\n\n(0.002)\n\n\n\n\nfriday × top\n−0.023***\n−0.020***\n−0.021***\n\n\n\n(0.004)\n(0.004)\n(0.005)\n\n\nNum.Obs.\n22486\n22486\n15647\n\n\nR2\n0.086\n0.095\n0.110\n\n\nR2 Within\n\n0.001\n0.001\n\n\nStd.Errors\nby: anndat\nby: anndat\nby: anndat\n\n\nFE: size_decile\n\nX\nX\n\n\nFE: year\n\nX\nX\n\n\nFE: month\n\nX\nX\n\n\nFE: vol_decile\n\n\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nPanel B: Long Term CAR\n\nmodel1b &lt;- feols(car_long ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2b &lt;- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3b &lt;- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,839 observations removed because of NA values (Fixed-effects: 6,839).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1b, model2b, model3b), gof_omit = gof_omit, stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n−0.022***\n\n\n\n\n\n(0.005)\n\n\n\n\nfriday\n−0.012\n−0.012\n−0.022\n\n\n\n(0.013)\n(0.013)\n(0.015)\n\n\ntop\n0.037***\n\n\n\n\n\n(0.004)\n\n\n\n\nfriday × top\n0.041**\n0.043**\n0.052**\n\n\n\n(0.015)\n(0.014)\n(0.017)\n\n\nNum.Obs.\n22486\n22486\n15647\n\n\nR2\n0.006\n0.035\n0.041\n\n\nR2 Within\n\n0.001\n0.001\n\n\nStd.Errors\nby: anndat\nby: anndat\nby: anndat\n\n\nFE: size_decile\n\nX\nX\n\n\nFE: year\n\nX\nX\n\n\nFE: month\n\nX\nX\n\n\nFE: vol_decile\n\n\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-3",
    "href": "freaky_friday/regressions.html#table-3",
    "title": "Regressions",
    "section": "Table 3",
    "text": "Table 3\n\nmain_extra &lt;- main %&gt;%\n  mutate(log_size = log(market_value))  %&gt;%\n  mutate(log_size_adj = log_size - mean(log_size, na.rm = T),\n         .by = c(quarter, year)) %&gt;%\n  mutate(size_decile = ntile(log_size_adj, 10))\n\nmodel1 &lt;- feols(car_short ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel2 &lt;- feols(car_short ~ friday + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\nmodel3 &lt;- feols(car_long ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel4 &lt;- feols(car_long ~ friday  + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\n\nmsummary(list(model1, model2, model3, model4), gof_omit = gof_omit, stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n  (4)\n\n\n\n\n(Intercept)\n−0.041***\n\n−0.015***\n\n\n\n\n(0.001)\n\n(0.002)\n\n\n\nfriday\n0.016***\n0.014***\n−0.016*\n−0.015*\n\n\n\n(0.002)\n(0.002)\n(0.008)\n(0.007)\n\n\nquantile\n0.006***\n\n0.003***\n\n\n\n\n(0.000)\n\n(0.000)\n\n\n\nfriday × quantile\n−0.002***\n−0.002***\n0.003**\n0.003***\n\n\n\n(0.000)\n(0.000)\n(0.001)\n(0.001)\n\n\nNum.Obs.\n130759\n130759\n130759\n130759\n\n\nR2\n0.054\n0.057\n0.002\n0.015\n\n\nR2 Within\n\n0.000\n\n0.000\n\n\nStd.Errors\nby: anndat\nby: anndat\nby: anndat\nby: anndat\n\n\nFE: size_decile\n\nX\n\nX\n\n\nFE: year\n\nX\n\nX\n\n\nFE: month\n\nX\n\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "freaky_friday/abnormal_returns.html",
    "href": "freaky_friday/abnormal_returns.html",
    "title": "Abnormal returns",
    "section": "",
    "text": "The lubridate package is the tidyverse package that helps with time related data. Dates are a specific class of variables and the package helps with managing date variables. There are a bunch of other packages out there that can help with managing date variables. I just picked one.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/abnormal_returns.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nIn order to predict the expected market return reaction to earnings, we will want to take into account the general market reaction. Dellavigna and Pollet (2009) only adjust for the overall market return. There are other adjustments possible for specific risk factors. The original factor model is the Fama-French 3 Factors model and the data to run those models is available via the Kenneth French data library.\nI have downloaded the data as a .csv file. The code reads the data in skipping 5 lines and reading the variables as numbers (double precision). We then need to transform the date variable to a date type with a function from the lubridate package. Finally, we need to scale the returns by 100 because they are expressed in percentages. For replicating the Dellavigna and Pollet (2009) paper, we only need the market return minus the risk free rate (mkt_rf) and the risk free rate (rf). If I read the paper correctly, we need to use the raw market return which is calculated as mkt.\n\nearn_ann &lt;- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nanalyst &lt;- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\"))\nall_stocks &lt;- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench &lt;- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  mutate_if(is.numeric, ~ . / 100) %&gt;%\n  mutate(mkt = mkt_rf + rf) %&gt;% \n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\nshrout is shares outstanding in 1000 shares\nmarket_value is in million USD"
  },
  {
    "objectID": "freaky_friday/abnormal_returns.html#footnotes",
    "href": "freaky_friday/abnormal_returns.html#footnotes",
    "title": "Abnormal returns",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR is fundamentally a programming language that is build around functions. The tidyverse partially works around that by making tibbles the primary object. Nevertheless, when you need to do some more advanced programming, creating functions is quite natural in R.↩︎\nFor comparison on my 2020 Mac Mini, it takes about 1.5 seconds for 100 announcements and 4.0 seconds for 10000 announcements. On my 2014 Macbook pro, that is respectively 4.3 seconds and 11.2 seconds.↩︎"
  },
  {
    "objectID": "freaky_friday/download_data.html",
    "href": "freaky_friday/download_data.html",
    "title": "Earnings announcements",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\ni_am(\"freaky_friday/download_data.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\nlibrary(dtplyr)\nlibrary(dbplyr)\nlinking_table &lt;- readRDS(here(\"data\", \"freaky_friday\", \"linking_table.RDS\"))\nwrds &lt;- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  user='stimas',\n                  sslmode='require')"
  },
  {
    "objectID": "freaky_friday/download_data.html#ibes",
    "href": "freaky_friday/download_data.html#ibes",
    "title": "Earnings announcements",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nWe start with the earnings announcement data from I/B/E/S with the analyst estimates. According to the method section in Dellavigna and Pollet (2009), we need the data from the start of 1995 to the middle of 2006. We will want the analyst estimates for all the firms with a ticker in the master linking_table. I am going to use parameters that we can calculate or set in R and then pass them on to the database query.\n\nbegin_date &lt;- \"'1995-01-01'\"\nend_date &lt;- \"'2006-07-01'\"\ntickers &lt;- unique(linking_table$ticker)\n\nThe dates of the estimate and the actual earnings announcement will be critical to construct unexpected component of the earnings and to determine the exact event data, i.e. the date that (the unexpected component of) the earnings are announced. Thankfully, WRDS provides a description of the date variables. anndats is the first day that an analyst set their estimate for the earnings per share and the revdats is the last day that the analyst confirmed their estimate. We will use revdats as the defacto date that the analyst provided the estimate. anndats_act is the earnings announcement date. value is the estimated EPS by the analyst and actual is the actual EPS as announced by the firm. pdf is flag whether the EPS if for the primary share class or on a diluted basis. I included both and that is probably appropriate for this paper. fpi is the forecast period indicator if we set this to “6”, we get the earnings estimates that are done in the quarter before the earnings announcements. All these variables can be verified in the data descriptions on WRDS. As you can see, it’s quite important if you work with data that you have not collected yourself to read the data descriptions.\n\nibes_query &lt;- tbl(wrds, in_schema(\"ibes\", \"det_epsus\")) %&gt;%\n  select(ticker, cusip, fpi, anndats, revdats, pdf, value, anndats_act, actual, analys) %&gt;%\n  filter(fpi == \"6\", ticker %in% tickers, !is.null(actual)) %&gt;%\n  filter(anndats_act &gt;= begin_date, anndats_act &lt;= end_date)\nann_ibes &lt;- collect(ibes_query)\nsaveRDS(ann_ibes, here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\nglimpse(ann_ibes)\n\nRows: 1,046,663\nColumns: 10\n$ ticker      &lt;chr&gt; \"AA\", \"A2\", \"A2\", \"A2\", \"AA\", \"AA\", \"AA\", \"AA\", \"AA\", \"AA\"…\n$ cusip       &lt;chr&gt; \"02224910\", \"0039241X\", \"0039241X\", \"0039241X\", \"02224910\"…\n$ fpi         &lt;chr&gt; \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\"…\n$ anndats     &lt;date&gt; 1994-10-10, 2004-09-30, 2004-10-21, 2004-10-27, 1994-10-1…\n$ revdats     &lt;date&gt; 1995-07-06, 2004-09-30, 2004-10-22, 2004-11-03, 1994-10-1…\n$ pdf         &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"P\", \"P\", \"P\", \"P\", \"P\", \"D\", \"D\", \"P\"…\n$ value       &lt;dbl&gt; 0.1875, -0.0900, -0.0403, -0.0410, 0.2663, 0.2963, 0.7500,…\n$ anndats_act &lt;date&gt; 1995-01-11, 2004-10-21, 2004-10-21, 2004-10-21, 1995-01-1…\n$ actual      &lt;dbl&gt; 0.2812, -0.0400, -0.0400, -0.0400, 0.2812, 0.2812, 0.7575,…\n$ analys      &lt;dbl&gt; 297, 478, 5469, 43594, 288, 18082, 18082, 474, 288, 127, 6…"
  },
  {
    "objectID": "freaky_friday/download_data.html#compustat",
    "href": "freaky_friday/download_data.html#compustat",
    "title": "Earnings announcements",
    "section": "Compustat",
    "text": "Compustat\nFollowing the paper, we will verify the earnings announcement date in I/B/E/S with the earnings announcement date in Compustat. Given the importance of finding the exact date for an event study, it is not surprising that Dellavigna and Pollet (2009) spent a lot of effort to make sure that they have the date right.\n\ngvkeys &lt;- unique(linking_table$gvkey)\n\nrdq is the earnings announcement date in Compustat. It does not make it easier that different databases use different variable names. That is why it so important to read the documentation of the database.\n\ncompu_query &lt;- tbl(wrds, in_schema(\"comp\", \"fundq\")) %&gt;%\n  filter(rdq &gt;= begin_date, rdq &lt;= end_date, gvkey %in% gvkeys) %&gt;%\n  select(cusip, rdq, gvkey)\nann_compu &lt;- as_tibble(compu_query) %&gt;%\n  mutate(cusip = str_sub(cusip, 1, 8))\nsaveRDS(ann_compu, here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nglimpse(ann_compu)\n\nRows: 327,964\nColumns: 3\n$ cusip &lt;chr&gt; \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"000…\n$ rdq   &lt;date&gt; 1995-03-15, 1995-07-06, 1995-09-13, 1995-12-12, 1996-03-14, 199…\n$ gvkey &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001…"
  },
  {
    "objectID": "freaky_friday/download_data.html#combine-announcements",
    "href": "freaky_friday/download_data.html#combine-announcements",
    "title": "Earnings announcements",
    "section": "Combine Announcements",
    "text": "Combine Announcements\nTo combine the two datasets, we will link them through a simplified version of the larger linking table. I will also enforce that the first 6 characters of cusip are the same. I don’t think it is strictly necessary to do that but it does gives us more confidence that the links are of higher quality. We need to match the I/B/E/S data and the Compustat data based on the firm and its earnings announcement date. However, if you read the paper (Dellavigna and Pollet 2009), you will notice that the reason why we want to combine the I/B/E/S and Compustat data is because the date in both datasets does not always match. The paper gets around that by matching earnings announcements if the date is not more than 5 days apart in the two data sources. This is why I create anndat_begin and anndat_end to define the interval in which we want to match the data. Finally, we can calculate the actual event date as the minimum of the date in the I/B/E/S data and the Compustat data (Dellavigna and Pollet 2009) 1.\nYou can also see that I have two lines of code at the start to read in the datasets again. This is not strictly necessary to make this file fully reproducible but it does make debugging the code easier. If I want to make some changes to the code I do not have to download the data again from WRDS. I can just use the one in the data folder.\n\nann_ibes &lt;- readRDS(here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\nann_compu &lt;- readRDS(here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nsimple_link &lt;- linking_table %&gt;%\n  select(ticker, gvkey, permno, cusip) %&gt;%\n  mutate(cusip = str_sub(cusip, end = 6)) %&gt;%\n  distinct()\nearn_ann &lt;- ann_ibes %&gt;%\n  distinct(ticker, actual, pdf, cusip, anndats_act) %&gt;%\n  mutate(cusip = str_sub(cusip, end = 6)) %&gt;%\n  left_join(simple_link, by = join_by(ticker)) %&gt;%\n  filter(!is.na(gvkey), !(cusip.x != cusip.y)) %&gt;%\n  select(-starts_with(\"cusip\")) %&gt;%\n  mutate(anndat_begin = anndats_act - 5, anndat_end = anndats_act + 5) %&gt;%\n  left_join(ann_compu, by = join_by(gvkey == gvkey,\n                                    anndat_begin &lt;= rdq,  anndat_end &gt;= rdq)) %&gt;% \n  filter(!is.na(rdq)) %&gt;%\n  mutate(anndat = pmin(anndats_act, rdq)) %&gt;%\n  select(-anndat_begin, -anndat_end)\n\nWarning in left_join(., simple_link, by = join_by(ticker)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 151 of `x` matches multiple rows in `y`.\nℹ Row 11209 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nsaveRDS(earn_ann, here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nglimpse(earn_ann)\n\nRows: 154,464\nColumns: 9\n$ ticker      &lt;chr&gt; \"A2\", \"AA0G\", \"A2\", \"AA0A\", \"AA0G\", \"AA0G\", \"AA0R\", \"AA0R\"…\n$ actual      &lt;dbl&gt; -0.0400, -0.3000, -0.0800, 0.1067, -0.4100, -0.3100, 0.035…\n$ pdf         &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\"…\n$ anndats_act &lt;date&gt; 2004-10-21, 2005-03-01, 2006-04-26, 2003-05-08, 2004-03-0…\n$ gvkey       &lt;chr&gt; \"001081\", \"133724\", \"001081\", \"001094\", \"133724\", \"133724\"…\n$ permno      &lt;dbl&gt; 10560, 88784, 10560, 10656, 88784, 88784, 10659, 10659, 84…\n$ cusip       &lt;chr&gt; \"00392410\", \"00724X10\", \"00392410\", \"00444610\", \"00724X10\"…\n$ rdq         &lt;date&gt; 2004-10-21, 2005-03-01, 2006-04-26, 2003-05-08, 2004-03-0…\n$ anndat      &lt;date&gt; 2004-10-21, 2005-03-01, 2006-04-26, 2003-05-08, 2004-03-0…\n\n\nearn_ann serves as a linking table to match different earnings announcements (as opposed to firms and their securities in linking_table)\n\n\n\nvariable\ndata source\ndescription\n\n\n\n\nticker\nI/B/E/S\nIdentifier\n\n\nanndats_act\nI/B/E/S\nActual Announcement Date\n\n\ngvkey\nCompustat\nIdentifier\n\n\npermno\nCRSP\nIdentifier\n\n\ncusip\n\nIdentifier of length 6,8 or 9\n\n\nrdq\nCompustat\nActual Announcement Date\n\n\nanndat\n\npmin(rdq, anndats_act)\n\n\n\nanndat is the validated way of calculating the announcement date. However, we need to keep the other dates around because we will need them to link back to the original databases.\nThe paper states that they have 154,051 earnings announcements (Dellavigna and Pollet 2009). We have 154464 earnings announcements."
  },
  {
    "objectID": "freaky_friday/download_data.html#footnotes",
    "href": "freaky_friday/download_data.html#footnotes",
    "title": "Earnings announcements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe need to use pmin so that the mutate statement knows that it needs to take the minimum between the two columns for each row and not over the whole dataset.↩︎\nNevertheless, with some smart programming you can get the tidyverse to be very speedy as well. See further when we calculate the abnormal returns.↩︎"
  },
  {
    "objectID": "freaky_friday/download_linking.html",
    "href": "freaky_friday/download_linking.html",
    "title": "WRDS linking data",
    "section": "",
    "text": "I use four packages on this page and three of them require some more explanation. The here package helps with managing the different files in this larger project. I can refer to different files relative to the root folder all the files are in. The only thing that I need to do is to say where this file is compared to the root folder with the i_am function. You can see the simplified folder structure below for this website. By telling R which file this is, I can easily refer to other files and save the data we are going to download and clean in the data folder. If it is specific to the replication it will go into the data/freaky_friday folder as an .RDS file which is an efficient way of storing R objects.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/download_linking.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\nlibrary(dbplyr)\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\nThe second package is the RPostgres package that helps make a connection with the WRDS data sources. The final package is dbplyr which allows us to interact with almost any database with the tidyverse verbs. I’ll demonstrate how below."
  },
  {
    "objectID": "freaky_friday/download_linking.html#ibes",
    "href": "freaky_friday/download_linking.html#ibes",
    "title": "WRDS linking data",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nThe code instructs the WRDS database to get the variables ticker, cusip (another identifier), cname (company name), and sdates (the start date for this ticker) from the ibes.idsum (IBES ID summary) database of WRDS. In this paper, we only want U.S. firms which we do by filtering by usfirm == 1.\n\nibes_query &lt;- tbl(wrds, in_schema(\"ibes\", \"idsum\")) %&gt;%\n  filter(usfirm == 1) %&gt;%\n  select(ticker, cusip, cname, sdates) %&gt;%\n  collect()\n\nWith some R code, we clean the data and save it as a file in the data &gt; wrds folder in our main folder.\n\nibes_id &lt;- as_tibble(ibes_query) %&gt;%\n  rename_all(tolower)\nsaveRDS(ibes_id, here(\"data\", \"wrds\", \"ibes_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#crsp",
    "href": "freaky_friday/download_linking.html#crsp",
    "title": "WRDS linking data",
    "section": "CRSP",
    "text": "CRSP\nFrom the CRSP data, we get the permno and ncusip identifier where ncusip stands for the same cusip identifier as mentioned above. We also have the company name, start date, and end date.\n\ncrsp_query &lt;- tbl(wrds, in_schema(\"crsp\", \"stocknames\")) %&gt;%\n  select(permno, ncusip, comnam, st_date, end_date) %&gt;%\n  collect()\n\n\ncrsp_id &lt;- as_tibble(crsp_query) %&gt;%\n  rename_all(tolower)\nsaveRDS(crsp_id, here(\"data\", \"wrds\", \"crsp_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-ibes",
    "href": "freaky_friday/download_linking.html#compustat-with-ibes",
    "title": "WRDS linking data",
    "section": "Compustat with I/B/E/S",
    "text": "Compustat with I/B/E/S\nFrom Compustat we use the security file which has all the financial securities (and their identifiers) that are linked to the firms in Compustat. We select all the variables from that dataset. We only select the ones where the ibes ticker is available so that we can match via the ticker in the I/B/E/S files.\n\ncompu_security_query &lt;- tbl(wrds, in_schema(\"comp\", \"security\")) %&gt;%\n  filter(!is.null(ibtic)) %&gt;%\n  collect()\n\n\ncompu_security &lt;- as_tibble(compu_security_query) %&gt;%\n  rename_all(tolower)\nsaveRDS(compu_security, here(\"data\", \"wrds\", \"compu_security.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-crsp",
    "href": "freaky_friday/download_linking.html#compustat-with-crsp",
    "title": "WRDS linking data",
    "section": "Compustat with CRSP",
    "text": "Compustat with CRSP\nFinally, we get the linking file in compustat. According to the documentation, not all the links are reliable and they advice to use the linktype variable and the usedflag variable to filter only the links that are most reliable. I have implemented the rules that follow best practice according to this tutorial (https://wrds-www.wharton.upenn.edu/pages/wrds-research/applications/linking-databases/linking-crsp-and-compustat/)\n\n# compu_query\ncompu_query &lt;- tbl(wrds, in_schema(\"crsp\", \"ccmxpf_linktable\")) %&gt;%\n  select(gvkey, linktype, usedflag, iid = liid, permno = lpermno, stdt = linkdt, enddt = linkenddt) %&gt;%\n  filter(!is.na(permno), linktype %in% c(\"LU\", \"LC\"), usedflag == 1) %&gt;%\n  select(gvkey, permno, stdt, enddt) %&gt;%\n  distinct()\nsaveRDS(crsp_query, here(\"data\", \"wrds\", \"crsp_compu.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#footnotes",
    "href": "freaky_friday/download_linking.html#footnotes",
    "title": "WRDS linking data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some other overviews available for identifiers that you can use to merge datasets. I am only using the best practice advice according to WRDS.↩︎"
  },
  {
    "objectID": "slides/slides_proposal.html#guidance-on-word-count",
    "href": "slides/slides_proposal.html#guidance-on-word-count",
    "title": "Feedback on Pitch and Proposal",
    "section": "Guidance on word count",
    "text": "Guidance on word count\n\nIntroduction and motivation (500-1000 words)\nLiterature review and hypothesis development (2500-3500 words)\nMeasurement and Methodology (750 - 1250 words)"
  },
  {
    "objectID": "slides/slides_proposal.html#some-writing-tips",
    "href": "slides/slides_proposal.html#some-writing-tips",
    "title": "Feedback on Pitch and Proposal",
    "section": "Some writing tips",
    "text": "Some writing tips\n\nExplain important concepts and theories\nTheory first, then literature review\nAvoid vague and hedging words as much as possible"
  },
  {
    "objectID": "slides/slides_proposal.html#theory-different-explanations-for-the-same-phenomenon",
    "href": "slides/slides_proposal.html#theory-different-explanations-for-the-same-phenomenon",
    "title": "Feedback on Pitch and Proposal",
    "section": "Theory = Different explanations for the same phenomenon",
    "text": "Theory = Different explanations for the same phenomenon\n\nMore ESG disclosure because more capabilities\n\n\nMore ESG disclosure because more scrutiny\n\n\nMore ESG disclosure because more need for capital"
  },
  {
    "objectID": "slides/slides_proposal.html#theory-a-succint-summary-of-prior-literature",
    "href": "slides/slides_proposal.html#theory-a-succint-summary-of-prior-literature",
    "title": "Feedback on Pitch and Proposal",
    "section": "Theory = A succint summary of prior literature",
    "text": "Theory = A succint summary of prior literature\n\nTokenisation is a market structure change"
  },
  {
    "objectID": "slides/slides_proposal.html#theory-relevance-of-your-findings",
    "href": "slides/slides_proposal.html#theory-relevance-of-your-findings",
    "title": "Feedback on Pitch and Proposal",
    "section": "Theory = Relevance of your findings",
    "text": "Theory = Relevance of your findings\n\nIs a short-selling ban (sometimes) good?"
  },
  {
    "objectID": "slides/slides_proposal.html#theory-what-are-the-important-variables",
    "href": "slides/slides_proposal.html#theory-what-are-the-important-variables",
    "title": "Feedback on Pitch and Proposal",
    "section": "Theory = What are the important variables?",
    "text": "Theory = What are the important variables?\n\nReasons for delegation: lack of knowledge by decision makers"
  },
  {
    "objectID": "slides/slides4.html#did-we-not-cover-that-already",
    "href": "slides/slides4.html#did-we-not-cover-that-already",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Did we not cover that already?",
    "text": "Did we not cover that already?\n\nYes, but briefly\nYes, but starting from the perspective of a regression (and the code)\n\n\nThe regression perspective is not bad. It means that we can see that more advanced regression techniques can be implemented in our linear regression framework. It’s also how most researchers in accounting and finance have been thought to think about research methods. However, there is a shift coming from economics where the focus is more on the research design."
  },
  {
    "objectID": "slides/slides4.html#the-focus-is-on-research-design",
    "href": "slides/slides4.html#the-focus-is-on-research-design",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The focus is on Research Design",
    "text": "The focus is on Research Design\n\n\n\n\n\n\nImportant\n\n\n\nWhich data should we use?\nWhich comparison identifies the effect that we are interested in?\n\n\n\n\n\n\nIs there sufficient variation that can identify the effect. - See also the pitching document - A specific example is the identification of performance effects\n\n\n\nFor instance Alcohol and Mortality, Chapter 5 in Huntington-Klein (2021).\nIs there sufficient variation in the treatment and the outcome?\nAre we reasonably sure that there are no confounders or only a few and we can measure them?"
  },
  {
    "objectID": "slides/slides4.html#prevously-we-used-models-and-assumptions-to-identify-effects",
    "href": "slides/slides4.html#prevously-we-used-models-and-assumptions-to-identify-effects",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Prevously, we used models and assumptions to identify effects",
    "text": "Prevously, we used models and assumptions to identify effects\n\n\nMathematical models\n\\[\nV = T^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n                 \\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\] \\[\n\\alpha_T + \\alpha_K + \\alpha_L = 1\n\\]\n\n\\(V =\\) The value of the firm\n\\(K =\\) Capital of the firm\n\\(L =\\) Labour of the firm\n\\(T =\\) CEO talent/skills/ability/experience\n\n\n\n\nDAGs\n\n\n\n\n\n\n\nspeedboat\n\n  \n\nave_ability\n\n ave_ability   \n\nltime\n\n ltime   \n\nave_ability-&gt;ltime\n\n    \n\nmixed_race\n\n mixed_race   \n\nmixed_race-&gt;ltime\n\n    \n\nfemale\n\n female   \n\nfemale-&gt;ave_ability\n\n    \n\nfemale-&gt;ltime\n\n    \n\ncourse\n\n course   \n\ncircumstances\n\n circumstances   \n\ncourse-&gt;circumstances\n\n    \n\nmonth_location\n\n month_location   \n\nmonth_location-&gt;circumstances\n\n    \n\ncircumstances-&gt;ltime\n\n    \n\ncircumstances-&gt;female"
  },
  {
    "objectID": "slides/slides4.html#just-focus-on-a-setting-where-we-are-confident-in-the-assumptions",
    "href": "slides/slides4.html#just-focus-on-a-setting-where-we-are-confident-in-the-assumptions",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Just focus on a setting where we are confident in the assumptions",
    "text": "Just focus on a setting where we are confident in the assumptions\n\n\nActual random assignment\nSpeedboat racing, game shows, Vietnam draft\nNatural experiments\nSee Gippel, Smith, and Zhu (2015), Chapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nPolicy Changes\nChapter 18, Difference-in-Difference in Huntington-Klein (2021)\nDiscrete cutoffs\ne.g. WAM &gt; 75, Chapter 20 Regression Continuity Design in Huntington-Klein (2021)\nUnexpected news\nChapter 17 Event Studies in Huntington-Klein (2021)\n\n\n\nNatural experiments is not the best terminology because most of these instances are not natural nor real experiments. Nevertheless, I still prefer the name over an instrumental variable approach. In too many proposals, I read an off hand comment that the student proposes to use a robustness test where they are going to use an instrumental variable approach. My answer to that is (1) if you have a natural experiment where you can exploit an instrumental variable, this should be the main analysis and (2) instrumental variables need to be defended as a research design based on your understanding of the setting. Calling the design a natural experiment forces you to think more about the experiment (i.e. the research design)."
  },
  {
    "objectID": "slides/slides4.html#look-for-these-designs",
    "href": "slides/slides4.html#look-for-these-designs",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Look for these designs!",
    "text": "Look for these designs!\n\n\nBased on your understanding of the industry and setting or the Data Generating Process\nWhen you read good papers for this unit and other units.\n\n\n\nThis is one of the main reasons that I want you to read broadly. It is unlikely that you will find a paper with a good research design exactly for the research question that you are interested in. However, you might find inspiration in similar or related fields that help you to design a better study for the research question that you are interested in."
  },
  {
    "objectID": "slides/slides4.html#what-effect-can-we-identify",
    "href": "slides/slides4.html#what-effect-can-we-identify",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "What effect can we identify?",
    "text": "What effect can we identify?\n\nAverage Treatment Effect\nAverage Treatment on the Treated\nAverage Treatment on the Untreated\nLocal Average Treatment Effect\nWeigthed Average Treatment Effect\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)\n\n\nDo you have an example of an effect that we might be interested in in Accounting and Finance?\nAverage implies that not all firms will respond the same to the treatment. This is the source of a lot trouble.\nAverage over which population?\nHow would you put these different effects in your own words?\nWATE is evil and I am going to largely ignore it."
  },
  {
    "objectID": "slides/slides4.html#it-all-depends-on-where-the-variation-is-coming-from.",
    "href": "slides/slides4.html#it-all-depends-on-where-the-variation-is-coming-from.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "It all depends on where the variation is coming from.",
    "text": "It all depends on where the variation is coming from.\n\n\n\n\n\n\nWarning\n\n\nDifferent firms react differently and are differently represented in the control group and the treatment group.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith actual random assignment, you probably have an ATE for the population that received the assignment.\nIf you can use a control group because that is what the treated group would look like if they were not treated, you probably have an ATT.\nIf you use a natural experiment to identify part of the variation, you probably have a LATE.\n\n\n\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#why-do-we-care",
    "href": "slides/slides4.html#why-do-we-care",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Why do we care?",
    "text": "Why do we care?\n\n\n\n\n\n\nResearch Design\n\n\nThere is a deep connection between the variation in your research design and the effect you can identify.\n\n\n\n\n\n\n\n\n\n\nPolicy Implications\n\n\nWhether your study has implications for “regulators and investors” depends heavily on the type of effect you can identify.\n\n\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)\n\nThat is the setting of your data determines which research design you can use. The research design determines which effect you can identify. The effect you can identify determines which conclusions you can draw."
  },
  {
    "objectID": "slides/slides4.html#generate-the-data",
    "href": "slides/slides4.html#generate-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Generate the Data",
    "text": "Generate the Data\n\nN &lt;- 1000\nrd1 &lt;- tibble(\n  firm = 1:N,\n  high_performance = rbinom(N, 1, 0.5),\n  noise = rnorm(N, 0, 3)\n) %&gt;%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_donation = 4 - 8 / performance + noise,\n    payoff_no_donation = 1 + noise\n  )\nglimpse(rd1) \n\nRows: 1,000\nColumns: 7\n$ firm               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …\n$ high_performance   &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1…\n$ noise              &lt;dbl&gt; 0.8634427, 3.9991062, -2.208085…\n$ donation           &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1…\n$ performance        &lt;dbl&gt; 4, 1, 4, 1, 1, 1, 1, 4, 4, 4, 4…\n$ payoff_donation    &lt;dbl&gt; 2.863443e+00, -8.938289e-04, -2…\n$ payoff_no_donation &lt;dbl&gt; 1.8634427, 4.9991062, -1.208085…\n\n\n\n\nWhat is the effect that we are we interested in?\nWhat are the policy implications?"
  },
  {
    "objectID": "slides/slides4.html#have-a-look-at-the-data",
    "href": "slides/slides4.html#have-a-look-at-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Have a look at the data",
    "text": "Have a look at the data\n\n\nWe will talk more about the pivot_wider and pivot_longer functions in week 7."
  },
  {
    "objectID": "slides/slides4.html#have-a-second-look-at-the-data",
    "href": "slides/slides4.html#have-a-second-look-at-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Have a second look at the data",
    "text": "Have a second look at the data"
  },
  {
    "objectID": "slides/slides4.html#real-data-does-not-have-the-counterfactuals.-we-only-observe-blue",
    "href": "slides/slides4.html#real-data-does-not-have-the-counterfactuals.-we-only-observe-blue",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Real data does not have the counterfactuals. We only observe blue!",
    "text": "Real data does not have the counterfactuals. We only observe blue!\n\n\n\n\n\n\n\nNote\n\n\nThe actual sample determines which comparisons we can make.\n\n\n\n\nWhy does this work? What effect are we identifying and how."
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-simulated-example-with-averages",
    "href": "slides/slides4.html#lets-redo-the-simulated-example-with-averages",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the simulated example with averages",
    "text": "Let’s redo the simulated example with averages\n\nrd1 %&gt;%\n  mutate(causal_effect = payoff_donation - payoff_no_donation) %&gt;%\n  summarise(M_causal = mean(causal_effect),\n            sd_causal = sd(causal_effect),\n            N = n()) %&gt;%\n  knitr::kable(format = \"markdown\", digits = 2)\n\n\n\n\nM_causal\nsd_causal\nN\n\n\n\n\n-2.14\n3\n1000\n\n\n\n\n\n\n\nThe causal effect of donating for each firm is difference in payoff between donating and not donating.\nWhat effect are we estimating here?"
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-simulated-example-with-averages-1",
    "href": "slides/slides4.html#lets-redo-the-simulated-example-with-averages-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the simulated example with averages",
    "text": "Let’s redo the simulated example with averages\n\n\n\n\n\nM_causal\nsd_causal\nN\n\n\n\n\n-2.14\n3\n1000\n\n\n\n\n\n\nrd1 %&gt;%\n  mutate(causal_effect = payoff_donation - payoff_no_donation) %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_causal = mean(causal_effect),\n            sd_causal = sd(causal_effect),\n            N = n()) %&gt;%\n  knitr::kable(format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_causal\nsd_causal\nN\n\n\n\n\n0\n-5\n0\n523\n\n\n1\n1\n0\n477"
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-regression-with-averages",
    "href": "slides/slides4.html#lets-redo-the-regression-with-averages",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the regression with averages",
    "text": "Let’s redo the regression with averages\n\nsummary_data  &lt;- rd1 %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-4.17\n0.83\n\n\n1\n1.76\n0.76\n\n\n\n\ncausal_effect_true &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 1]\ncausal_effect_reg &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 0]\n\n\n\n\n\n\n\nNote\n\n\n\nThe true ATT is 1\nThe effect estimated by the regression is 0.926"
  },
  {
    "objectID": "slides/slides4.html#if-you-do-not-believe-me-here-is-the-regression",
    "href": "slides/slides4.html#if-you-do-not-believe-me-here-is-the-regression",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "If you do not believe me, here is the regression",
    "text": "If you do not believe me, here is the regression\n\n\n\nrd1 &lt;- mutate(rd1, actual_payoff =\n       ifelse(donation, payoff_donation, payoff_no_donation))\nols &lt;- feols(actual_payoff ~ donation, data = rd1)\n\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.833***\n\n\n\n(0.131)\n\n\ndonation\n0.926***\n\n\n\n(0.190)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides4.html#what-could-possibly-go-wrong",
    "href": "slides/slides4.html#what-could-possibly-go-wrong",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "What could possibly go wrong?",
    "text": "What could possibly go wrong?\n\nrd2 &lt;- tibble(\n  high_performance = rbinom(N, 1, 0.5),\n  noise = rnorm(N, 0, 3)) %&gt;%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_donation = 4 - 8 / performance + noise,\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + noise\n  )"
  },
  {
    "objectID": "slides/slides4.html#causal-effect-estimates-with-a-confounder",
    "href": "slides/slides4.html#causal-effect-estimates-with-a-confounder",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Effect Estimates with a Confounder",
    "text": "Causal Effect Estimates with a Confounder\n\nsummary_data  &lt;- rd2 %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-4.01\n1.99\n\n\n1\n1.97\n0.97\n\n\n\n\ncausal_effect_true &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 1]\ncausal_effect_reg &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 0]\n\n\nThe true ATT is 1\nThe effect estimated by the regression is -0.024"
  },
  {
    "objectID": "slides/slides4.html#where-is-the-variation-coming-from",
    "href": "slides/slides4.html#where-is-the-variation-coming-from",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Where is the variation coming from?",
    "text": "Where is the variation coming from?\n\n\n\n\n\n\nWe need firms that make mistakes\n\n\n\nFirms that should donate but do not always do it.\nFirms that should not donate but sometimes donate."
  },
  {
    "objectID": "slides/slides4.html#panel-data-simulation-100-firms",
    "href": "slides/slides4.html#panel-data-simulation-100-firms",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Panel Data Simulation (100 firms)",
    "text": "Panel Data Simulation (100 firms)\n\nN &lt;- 100\nrd_firm &lt;- tibble(\n  firm = 1:N,\n  high_performance = rbinom(N, 1, 0.5),\n  other_payoff = rnorm(N, 0, 3)) %&gt;%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + other_payoff,\n    payoff_donation = 4 - 8/performance + other_payoff\n  )\nsummary_data  &lt;- rd_firm %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, digits = 1)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-3.9\n2.1\n\n\n1\n1.8\n0.8"
  },
  {
    "objectID": "slides/slides4.html#panel-data-simulation-10-time-periods",
    "href": "slides/slides4.html#panel-data-simulation-10-time-periods",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Panel Data Simulation (10 time periods)",
    "text": "Panel Data Simulation (10 time periods)\nThe variation comes from high performers not donating some years\n\nT &lt;- 10\nrd_panel_forget &lt;- tibble(\n  firm = rep(1:N, each = T),\n  year = rep(1:T, times = N)) %&gt;%\n  left_join(rd_firm, by = \"firm\") %&gt;%\n  mutate(forget_donation = rbinom(N * T, 1, plogis(-other_payoff)),\n         actual_donation = (1 - forget_donation) * donation,\n         actual_payoff = ifelse(actual_donation == 1,\n                                payoff_donation, payoff_no_donation))\n\n\n\nThe way we simulate the data reflects the firm fixed effects and the time varying effects.\nWhich effect are we identifying with this sample?"
  },
  {
    "objectID": "slides/slides4.html#the-new-assignment",
    "href": "slides/slides4.html#the-new-assignment",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The New Assignment",
    "text": "The New Assignment\n\nRun a fixed effect model and interpret the result\nCreate a new dataset where all firms make mistakes\nRun a fixed effect model and interpret the result"
  },
  {
    "objectID": "slides/slides4.html#causal-diagram",
    "href": "slides/slides4.html#causal-diagram",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Diagram",
    "text": "Causal Diagram\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx-&gt;y\n\n    \n\ncollider\n\n collider   \n\nx-&gt;collider\n\n    \n\ny-&gt;collider\n\n    \n\nconfounder\n\n confounder   \n\nconfounder-&gt;x\n\n    \n\nconfounder-&gt;y"
  },
  {
    "objectID": "slides/slides4.html#causal-diagram-1",
    "href": "slides/slides4.html#causal-diagram-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Diagram",
    "text": "Causal Diagram\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx-&gt;y\n\n    \n\ncollider\n\n collider   \n\nx-&gt;collider\n\n    \n\ny-&gt;collider\n\n    \n\nconfounder\n\n confounder   \n\nconfounder-&gt;x\n\n    \n\nconfounder-&gt;y\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx-&gt;y\n\n    \n\ncollider\n\n collider   \n\nx-&gt;collider\n\n    \n\ny-&gt;collider\n\n    \n\niv\n\n iv   \n\niv-&gt;x\n\n    \n\nrandom\n\n random   \n\nrandom-&gt;iv\n\n    \n\nconfounder\n\n confounder   \n\nconfounder-&gt;x\n\n    \n\nconfounder-&gt;y\n\n   \n\n\n\n\n\n\n\nSee Instrumental Variables, Chapter 19 in Huntington-Klein (2021).\n\nMechanically, there are two regressions. (2-stage-least-squares) 1. Use the IV to estimate the randomly generated variation in X -&gt; fitted(X) 2. Use fitted(X) to estimate the effect of random variation in X on Y"
  },
  {
    "objectID": "slides/slides4.html#simulation-and-implementation-with-fixest",
    "href": "slides/slides4.html#simulation-and-implementation-with-fixest",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation and Implementation with fixest",
    "text": "Simulation and Implementation with fixest\n\n#|label: simulation-iv\nd &lt;- tibble(\n  iv = rnorm(N, 0, 1),\n  confounder = rnorm(N, 0, 1)) %&gt;%\n  mutate(\n    x = rnorm(N, .6 * iv - .6 * confounder, .6),\n    y = rnorm(N, .6 * x + .6 * confounder, .6),\n    survival = if_else(y &gt; 0, 1, 0)\n  )\nsurv &lt;- filter(d, survival == 1)\nlm1 &lt;- lm(y ~ x, d)\nlm2 &lt;- lm(y ~ x + confounder, d)\nlm3 &lt;- lm(y ~ x, surv)\nlm4 &lt;- lm(y ~ x + confounder, surv)\niv1 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = d)\niv2 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = surv)\n\n\nAll the exogenous variable are in the tibble statement, all the endogenous variables are in the mutate statement. That is not a coincidence. It also highlights the value and tight link between being able to simulate your theory and understanding it.\nNote, the collider bias is the biggest problem if the selection bias is on both x and y because then the collider bias effects the first stage regressions."
  },
  {
    "objectID": "slides/slides4.html#simulation-results-with-a-real-effect-of-0.6",
    "href": "slides/slides4.html#simulation-results-with-a-real-effect-of-0.6",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation results with a real effect of 0.6",
    "text": "Simulation results with a real effect of 0.6\n\nmsummary(list(\"confounded\" = lm1, \"with control\" = lm2, \"collider\" = lm3, \"collider\" = lm4,\n              \"iv no collider\" = iv1, \"iv with collider\" = iv2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\nconfounded\nwith control\ncollider\ncollider\niv no collider\niv with collider\n\n\n\n\n(Intercept)\n−0.082\n−0.082\n0.591***\n0.442***\n−0.095\n0.546***\n\n\n\n(0.082)\n(0.061)\n(0.072)\n(0.089)\n(0.104)\n(0.082)\n\n\nx\n0.257***\n0.595***\n0.103\n0.293***\n\n\n\n\n\n(0.081)\n(0.070)\n(0.067)\n(0.097)\n\n\n\n\nconfounder\n\n0.644***\n\n0.248**\n\n\n\n\n\n\n(0.070)\n\n(0.095)\n\n\n\n\nfit_x\n\n\n\n\n0.874***\n0.271**\n\n\n\n\n\n\n\n(0.195)\n(0.129)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not strictly a collider because there is no effect of x on survival. However, it already shows that there are problems with “simple” selection bias."
  },
  {
    "objectID": "slides/slides4.html#simulation-without-an-effect",
    "href": "slides/slides4.html#simulation-without-an-effect",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation without an effect",
    "text": "Simulation without an effect\n\nd &lt;- tibble(\n  iv = rnorm(N, 0, 1),\n  confounder = rnorm(N, 0, 1)) %&gt;%\n  mutate(\n    x = rnorm(N, .6 * iv - .6 * confounder, .6),\n    y = rnorm(N, .6 * confounder, .6),\n    survival = if_else(y &gt; 0, 1, 0)\n  )\nsurv &lt;- filter(d, survival == 1)\nlm1 &lt;- lm(y ~ x, d)\nlm2 &lt;- lm(y ~ x + confounder, d)\nlm3 &lt;- lm(y ~ x, surv)\nlm4 &lt;- lm(y ~ x + confounder, surv)\niv1 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = d)\niv2 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = surv)"
  },
  {
    "objectID": "slides/slides4.html#simulation-without-an-effect-1",
    "href": "slides/slides4.html#simulation-without-an-effect-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation without an effect",
    "text": "Simulation without an effect\n\nmsummary(list(\"confounded\" = lm1, \"with control\" = lm2, \"collider\" = lm3, \"collider\" = lm4,\n              \"iv no collider\" = iv1, \"iv with collider\" = iv2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\nconfounded\nwith control\ncollider\ncollider\niv no collider\niv with collider\n\n\n\n\n(Intercept)\n0.017\n0.005\n0.696***\n0.483***\n0.057\n0.714***\n\n\n\n(0.075)\n(0.058)\n(0.083)\n(0.077)\n(0.083)\n(0.087)\n\n\nx\n−0.125*\n0.151**\n−0.020\n0.118*\n\n\n\n\n\n(0.069)\n(0.064)\n(0.080)\n(0.068)\n\n\n\n\nconfounder\n\n0.619***\n\n0.484***\n\n\n\n\n\n\n(0.077)\n\n(0.093)\n\n\n\n\nfit_x\n\n\n\n\n0.181\n0.035\n\n\n\n\n\n\n\n(0.112)\n(0.113)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides4.html#examplesitting-duck-governors-by-falk2018",
    "href": "slides/slides4.html#examplesitting-duck-governors-by-falk2018",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Example:Sitting Duck Governors by Falk and Shelton (2018)",
    "text": "Example:Sitting Duck Governors by Falk and Shelton (2018)\n\n\n\n\n\n\nNote\n\n\n\nResearch Question: Does political uncertainty effect investment?\nMore uncertainty in a state when governor does not come up for reelection.\nState level laws with term limits (~ Random)\n\n\n\n\n\nAn exercise to be run in class"
  },
  {
    "objectID": "slides/slides4.html#data",
    "href": "slides/slides4.html#data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Data",
    "text": "Data\n\nlibrary(readit)\nduck &lt;- readit(here(\"data\", \"LameDuckData.dta\")) %&gt;%\n  select(-starts_with(\"nstate\"), -starts_with(\"stdum\"),\n         -starts_with(\"yd_alt\")) %&gt;%\n  group_by(statename) %&gt;%\n  arrange(year) %&gt;%\n  mutate(log_I_1 = lag(log_I), log_I_2 = lag(log_I, 2),\n         log_Y_1 = lag(log_Y), log_Y_2 = lag(log_Y, 2),\n         log_real_GDP_1 = lag(log_real_GDP),\n         log_real_GDP_2 = lag(log_real_GDP, 2)) %&gt;%\n  ungroup() %&gt;%\n  arrange(statename) %&gt;%\n  filter(year &gt;= 1967, year &lt;= 2004)"
  },
  {
    "objectID": "slides/slides4.html#reduced-form",
    "href": "slides/slides4.html#reduced-form",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Reduced Form",
    "text": "Reduced Form\n\nform_red &lt;- formula(\n  log_I ~ gov_exogenous_middling + log_I_1 + log_I_2 +\n  log_Y + log_Y_1 + log_Y_2 + log_real_GDP + log_real_GDP_1 +\n  log_real_GDP_2 | statename\n  )\nred_reg &lt;- feols(form_red, data = duck)"
  },
  {
    "objectID": "slides/slides4.html#stage-least-squares-2sls",
    "href": "slides/slides4.html#stage-least-squares-2sls",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "2 Stage Least Squares (2SLS)",
    "text": "2 Stage Least Squares (2SLS)\n\nform_iv &lt;- formula(log_I ~ log_I_1 + log_I_2 +\n  log_Y + log_Y_1 + log_Y_2 + log_real_GDP + log_real_GDP_1 +\n  log_real_GDP_2\n  # fixed effects\n  | statename\n  # 1st regression\n  | uncertainty_continuous ~ gov_exogenous_middling\n  )\niv_reg &lt;- feols(form_iv, data = duck)"
  },
  {
    "objectID": "slides/slides4.html#results",
    "href": "slides/slides4.html#results",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Results",
    "text": "Results\n\ncoef_map = c(\"gov_exogenous_middling\" = \"lame duck governor\",\n             \"fit_uncertainty_continuous\" = \"uncertainty\")\nmsummary(list(\"reduced\" = red_reg,\n              \"first stage iv\" = summary(iv_reg, stage = 1),\n              \"second stage iv\" = iv_reg),\n         gof_omit = gof_omit, stars = stars,\n         coef_map = coef_map)\n\n\n\n\n\nreduced\nfirst stage iv\nsecond stage iv\n\n\n\n\nlame duck governor\n−0.049**\n1.801***\n\n\n\n\n(0.021)\n(0.112)\n\n\n\nuncertainty\n\n\n−0.027**\n\n\n\n\n\n(0.012)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides4.html#diagnostics-test-for-endogeneity-durbin-wu-hausmann",
    "href": "slides/slides4.html#diagnostics-test-for-endogeneity-durbin-wu-hausmann",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Diagnostics: Test for endogeneity (Durbin-Wu-Hausmann)",
    "text": "Diagnostics: Test for endogeneity (Durbin-Wu-Hausmann)\n\n\n\n\n\n\nNote\n\n\nIs the IV result different from the OLS result?\n\n\n\n\nsumm_iv &lt;- summary(iv_reg)\nsumm_1st &lt;- summary(iv_reg, stage = 1)\nsumm_iv$iv_wh$stat  # iv wu hausmann\n\n[1] 3.829033\n\nsumm_iv$iv_wh$p     # iv wu hausmann\n\n[1] 0.05054677\n\n\nInstrumental Variables, Chapter 19 in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#diagnostics-test-for-weak-instrument",
    "href": "slides/slides4.html#diagnostics-test-for-weak-instrument",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Diagnostics: Test for weak instrument",
    "text": "Diagnostics: Test for weak instrument\n\n\n\n\n\n\nNote\n\n\nIs the instrument predicting the variable we want it to predict?\n\n\n\n\nfitstat(iv_reg, type = \"ivf\")\n\nF-test (1st stage), uncertainty_continuous: stat = 331.0, p &lt; 2.2e-16, on 1 and 1,640 DoF."
  },
  {
    "objectID": "slides/slides4.html#new-assignment",
    "href": "slides/slides4.html#new-assignment",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "New Assignment",
    "text": "New Assignment\nLet’s assume that firms are less likely to donate when there is a local election\n\nN &lt;- 5000\nrd_iv_el &lt;- tibble(\n  high_performance = rbinom(N, 1, .5),\n  extra_payoff = rnorm(N, 0, 3),\n  local_election = rbinom(N, 1, .33)) %&gt;%\n  mutate(\n    actual_donation = ifelse(high_performance == 1, 1 - local_election, 0),\n    payoff_donation = ifelse(high_performance == 1, 2, - 4) + extra_payoff,\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + extra_payoff,\n    actual_payoff = ifelse(actual_donation == 1,\n                           payoff_donation, payoff_no_donation))\n\n\n\nWhich effect can we identify with this data?\nRun the instrumental variable analyses and interpret the results."
  },
  {
    "objectID": "slides/slides4.html#this-paper-is-a-finished-product-your-pitch-proposal-or-dissertation-is-not.",
    "href": "slides/slides4.html#this-paper-is-a-finished-product-your-pitch-proposal-or-dissertation-is-not.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "This paper is a finished product, your pitch, proposal, or dissertation is not.",
    "text": "This paper is a finished product, your pitch, proposal, or dissertation is not.\nWe are grateful to Michael Roberts (the Editor), the Associate Editor, two anonymous referees, Marianne Bertrand, Ing-Haw Cheng, Ken French, Ed Glaeser, Todd Gormley, Ben Iverson (discus- sant), Steve Kaplan, Borja Larrain (discussant), Jonathan Lewellen, Katharina Lewellen, David Matsa (discussant), David Metzger (discussant), Toby Moskowitz, Candice Prendergast, Enrichetta Ravina (discussant), Amit Seru, and Wei Wang (discussant) for helpful suggestions. We thank seminar participants at AFA, BYU, CICF Conference, Depaul, Duke, Gerzensee ESSFM, Harvard, HKUST Finance Symposium, McGill Todai Conference, Finance UC Chile, Helsinki, IDC Herzliya Finance Conference, NBER Corporate Finance and Personnel Meetings, SEC, Simon Fraser Uni- versity, Stanford, Stockholm School of Economics, University of Amsterdam, UC Berkeley, UCLA, and Wharton for helpful comments. We thank David Yermack for his generosity in sharing data. We thank Matt Turner at Pearl Meyer, Don Delves at the Delves Group, and Stephen O’Byrne at Shareholder Value Advisors for helping us understand the intricacies of executive stock option plans. Menaka Hampole provided excellent research assistance. We acknowledge financial support from the Initiative on Global Markets.\n\nOn the one hand, we do not expect you to come up with a design like this. On the other hand, why not use these hard won insights."
  },
  {
    "objectID": "slides/slides4.html#this-paper-has-1-one-research-question.-this-is-a-good-thing",
    "href": "slides/slides4.html#this-paper-has-1-one-research-question.-this-is-a-good-thing",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "This paper has 1 (one!) research question. This is a good thing!",
    "text": "This paper has 1 (one!) research question. This is a good thing!\n\nIt’s not necessarily advantageous to have too many hypotheses. You want to answer one question well."
  },
  {
    "objectID": "slides/slides4.html#do-increases-in-option-grants-increase-risk-taking",
    "href": "slides/slides4.html#do-increases-in-option-grants-increase-risk-taking",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Do increases in option grants increase risk taking?",
    "text": "Do increases in option grants increase risk taking?\n\n\n\n\n\n\n\noptions\n\n  \n\nOption Grants\n\n Option Grants   \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants-&gt;Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances-&gt;Option Grants\n\n    \n\nAnnoyances-&gt;Risk Taking\n\n   \n\n\n\n\n\n\nExample of annoyances: Risk averse CEOs might take less risks and therefore receive more option grants."
  },
  {
    "objectID": "slides/slides4.html#iv-1-scheduled-discrete-increases-in-fixed-value-option-grants",
    "href": "slides/slides4.html#iv-1-scheduled-discrete-increases-in-fixed-value-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "IV 1: Scheduled Discrete Increases in Fixed-Value Option Grants",
    "text": "IV 1: Scheduled Discrete Increases in Fixed-Value Option Grants\n\n\n\n\n\n\n\noptions\n\n  \n\nPredicted New Grant Cycle\n\n Predicted New Grant Cycle   \n\nOption Grants\n\n Option Grants   \n\nPredicted New Grant Cycle-&gt;Option Grants\n\n    \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants-&gt;Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances-&gt;Option Grants\n\n    \n\nAnnoyances-&gt;Risk Taking\n\n   \n\n\n\n\n\n\nFor our first instrument, we use fixed-value firms, for which option grants can increase only at regularly prescheduled intervals (i.e., when new cycles start). For example, consider a fixed-value firm on regular three-year cycles. Other time-varying factors may drive trends in risk for this firm. However, these trends are unlikely to coincide exactly with the timing of when new cycles are scheduled to start.\n\n\nBasically saying the beginning of a cycle effect on option grants is not affected by the annoyances."
  },
  {
    "objectID": "slides/slides4.html#iv-2-within-cycle-grant-increases-due-to-industry-shocks-in-fixed-number-option-grants",
    "href": "slides/slides4.html#iv-2-within-cycle-grant-increases-due-to-industry-shocks-in-fixed-number-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "IV 2: Within Cycle Grant Increases due to Industry Shocks in Fixed-Number Option Grants",
    "text": "IV 2: Within Cycle Grant Increases due to Industry Shocks in Fixed-Number Option Grants\n\n\n\n\n\n\n\noptions\n\n  \n\nIndustry Shocks (Fixed Number)\n\n Industry Shocks (Fixed Number)   \n\nOption Grants\n\n Option Grants   \n\nIndustry Shocks (Fixed Number)-&gt;Option Grants\n\n    \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants-&gt;Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances-&gt;Option Grants\n\n    \n\nAnnoyances-&gt;Risk Taking\n\n   \n\n\n\n\n\n\nFor our second instrument, we focus on fixed-number firms. The value of options granted in any particular year varies with aggregate returns within a fixed-number cycle. This means that the timing of increases in option pay within a cycle will be random in the sense that the increases are driven in part by industry shocks that are beyond the control of the firm and are largely unpredictable. To account for the possibility that aggregate returns can directly affect risk, we use fixed-value firms as a control group because their option compensation must remain fixed despite changes in aggregate returns.\n\n\nThe identifying assumption is that fixed-number vs fixed-value might be a part of the annoyances. So might the industry shocks. However, the IV assumes that the industry shocks are not different except in how they effect the option grant value."
  },
  {
    "objectID": "slides/slides4.html#the-authors-know-their-setting",
    "href": "slides/slides4.html#the-authors-know-their-setting",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The authors know their setting!",
    "text": "The authors know their setting!\n\nOur identification strategy builds on Hall’s (1999)) observation that firms often award options according to multiyear plans. Two types of plans are commonly used: fixed-number and fixed-value. Under a fixed-number plan, an executive receives the same number of options each year within a cycle. Under a fixed-value plan, an executive receives the same value of options each year within a cycle.\n\n\n\nOur conversations with leading compensation consultants suggest that multiyear plans are used to minimize contracting costs, as option compensation only has to be set once every few years. Hall (1999, p. 97) argues that firms sort into the two types of plans somewhat arbitrarily, observing that “Boards seem to substitute one plan for another without much analysis or understanding of their differences.”\n\n\n\nRead qualitative studies and descriptions of actual practice!\nWe are looking at “slightly suboptimal” decision making to get variation."
  },
  {
    "objectID": "slides/slides4.html#key-assumption-1---relevance-iv-is-related-to-option-grants",
    "href": "slides/slides4.html#key-assumption-1---relevance-iv-is-related-to-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Key Assumption 1 - Relevance: IV is related to Option Grants",
    "text": "Key Assumption 1 - Relevance: IV is related to Option Grants\n\nWe find that the first-year indicator corresponds to a 15% larger increase in the Black-Scholes value of new option grants than in other years.\n\n\nAll estimates are highly significant, with F-statistics greatly exceeding 10, the rule of thumb threshold for concerns related to weak instruments (Staiger and Stock (1997). (III A.)\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#key-assumption-2---exclusion-or-validity-only-path-from-iv-to-risk-taking-is-through-option-grants.",
    "href": "slides/slides4.html#key-assumption-2---exclusion-or-validity-only-path-from-iv-to-risk-taking-is-through-option-grants.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Key Assumption 2 - Exclusion (or validity): Only path from IV to Risk Taking is through Option Grants.",
    "text": "Key Assumption 2 - Exclusion (or validity): Only path from IV to Risk Taking is through Option Grants.\n\nOne might be concerned that predicted first years provide exogenously timed but potentially anticipated increases in option compensation. However, this is not an issue for our empirical strategy. […] He would have no incentive to increase risk prior to an anticipated increase in the value of his option compensation next period.\n\n\nIn addition, we directly examine whether fixed-value cycles appear to be correlated with other firm cycles […]\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nThe key for the exclusion assumption is that anticipation would have an impact on the risk taking prior to the new cycle. This than would have an impact on the actual measure, i.e. the change in risk."
  },
  {
    "objectID": "slides/slides4.html#one-criticism",
    "href": "slides/slides4.html#one-criticism",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "One Criticism",
    "text": "One Criticism\n\nFirst, option compensation tends to follow an increasing step function for executives on fixed-value plans. This is because compensation tends to drift upward over time, yet executives on fixed-value plans cannot experience an upward drift within a cycle.\n\n\nWhile these two stylized facts do not hold in all cases—as can also be seen in Figure 1—our identification strategy only requires that they hold on average.\n\n\n\n\n\n\n\n\nSome more terminology\n\n\n\nCompliers\nAlways-takers/never-takers\nDefiers\n\n\n\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nThe LATE is identified for the compliers. IV assumes that there are no defiers because now our estimated effect becomes an average of the defiers and compliers. One solution is to just remove the defiers if you can (which they do in the paper as a robustness check)."
  },
  {
    "objectID": "slides/slides2.html#why-simulate-data",
    "href": "slides/slides2.html#why-simulate-data",
    "title": "Simulations, Regressions, and Significance",
    "section": "Why simulate data?",
    "text": "Why simulate data?\n\nVisualising your theory\nExperimenting with and understanding statistical tests\nExperimenting with statistical approaches without peaking at your data\n\nSee also Chapter 15 in Huntington-Klein (2021)\n\n\nVisualising can help you sharpen your intuition for your theory and for which values are reasonable and which are not.\nYou can simulate variables and causal structures that you cannot observe. See also this week’s homework\nYou don’t want to just decide on which statistical test to use because it gives you the “right” answer. If you want to experiment with different statistical models, you can do that with simulated data."
  },
  {
    "objectID": "slides/slides2.html#simulating-distributions-in-r",
    "href": "slides/slides2.html#simulating-distributions-in-r",
    "title": "Simulations, Regressions, and Significance",
    "section": "Simulating distributions in R",
    "text": "Simulating distributions in R\n\nN &lt;- 1000\nrandom &lt;- tibble(\n  normal = rnorm(N, mean = 2, sd = 5),\n  uniform = runif(N, min = 1, max = 5),\n  binomial = rbinom(N, size = 1, prob = .25),\n  sample = sample(1:10, size = N, replace = T)\n)\nglimpse(random)\n\nRows: 1,000\nColumns: 4\n$ normal   &lt;dbl&gt; 0.1350883, 6.7537684, 4.7469134, 7.884157…\n$ uniform  &lt;dbl&gt; 1.290152, 3.891102, 3.605716, 3.845837, 2…\n$ binomial &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ sample   &lt;int&gt; 1, 8, 8, 2, 3, 8, 6, 10, 7, 5, 10, 2, 3, …"
  },
  {
    "objectID": "slides/slides2.html#better-code-formatting",
    "href": "slides/slides2.html#better-code-formatting",
    "title": "Simulations, Regressions, and Significance",
    "section": "Better Code Formatting",
    "text": "Better Code Formatting\n\np1 &lt;- ggplot(random, aes(x = normal)) +\n  geom_density() +\n  ggtitle(\"normal density\")\np2 &lt;- ggplot(random, aes(x = uniform)) +\n  geom_histogram(bins = 50) +\n  ggtitle(\"uniform histogram\")\np3 &lt;- ggplot(random, aes(x = binomial, y = normal)) +\n  geom_point() +\n  ggtitle(\"binomal-normal\")\np4 &lt;- ggplot(random, aes(x = as.factor(sample), y = uniform)) +\n  geom_jitter(width = .2) +\n  ggtitle(\"sample-uniform\") + labs(x = \"sample\")\nplot_grid(p1, p2, p3, p4, ncol = 4)"
  },
  {
    "objectID": "slides/slides2.html#new-theory",
    "href": "slides/slides2.html#new-theory",
    "title": "Simulations, Regressions, and Significance",
    "section": "New theory",
    "text": "New theory\n\n\n\n\n\n\nSummary\n\n\n\nFirms have different size and CEOs have different talent.\nMore talented CEOs work for bigger firms.\nFirms pay just enough so that the CEO is not tempted to work for a smaller firm.\n\n\n\n\n\n\nThe model is the second one presented in Edmans and Gabaix (2016). A more rigorous proof is shown in Tervio (2008). A simplified explanation is in the supplementary reading on theory and simulations"
  },
  {
    "objectID": "slides/slides2.html#visualisation-of-matching-theory",
    "href": "slides/slides2.html#visualisation-of-matching-theory",
    "title": "Simulations, Regressions, and Significance",
    "section": "Visualisation of matching theory",
    "text": "Visualisation of matching theory\n\n\n\n\nCode\nobs &lt;- 500\nsize_rate &lt;- 1; talent_rate &lt;- 2/3;\nC &lt;- 1/60; w0 = 1;\nn &lt;- c(1:obs)\nsize &lt;-  600 * n ^ (-size_rate)\ntalent &lt;- - 1/talent_rate * n ^ (talent_rate)\n\nwage &lt;- rep(NA, obs)\nwage[obs] &lt;- w0\nfor (i in (obs - 1):1){\n  wage[i] &lt;- wage[i + 1] + C * size[i + 1] *\n      (talent[i] - talent[i + 1])\n}\n\nmatching_plot &lt;- qplot(x = size, y = wage) +\n    labs(x = \"Company Market Value\", y = \"CEO compensation\")\nplot(matching_plot + ggtitle(\"Value - Compensation\"))\n\n\n\n\n\n\n\n\nCode\nplot(matching_plot +\n     scale_x_continuous(trans = \"log\",\n                        breaks = scales::log_breaks(n=5, base=10)) +\n     scale_y_continuous(trans = \"log\",\n                        breaks = scales::log_breaks(n=5, base=5)) +\n     ggtitle(\"log(Value) - log(Compensation)\")\n     )"
  },
  {
    "objectID": "slides/slides2.html#ceo-compensation-data",
    "href": "slides/slides2.html#ceo-compensation-data",
    "title": "Simulations, Regressions, and Significance",
    "section": "CEO compensation data",
    "text": "CEO compensation data\n\nus_comp &lt;- readRDS(here(\"data\", \"us-compensation-new.RDS\")) %&gt;%\n  rename(total_comp = tdc1)\nus_value &lt;- readRDS(here(\"data\", \"us-value-new.RDS\")) %&gt;%\n  rename(year = fyear, market_value = mkvalt)\nsummary(us_value$market_value)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n      0.0     951.7    2702.9   14756.3    8637.5 2662325.9 \n     NA's \n     2792"
  },
  {
    "objectID": "slides/slides2.html#putting-it-all-together-with-_join",
    "href": "slides/slides2.html#putting-it-all-together-with-_join",
    "title": "Simulations, Regressions, and Significance",
    "section": "Putting it all together with _join",
    "text": "Putting it all together with _join\n\nus_comp_value &lt;-\n    select(us_comp, gvkey, year, total_comp) %&gt;% \n    left_join(\n        us_value,\n        by = c(\"year\", \"gvkey\"))\nglimpse(us_comp_value)\n\nRows: 31,692\nColumns: 5\n$ gvkey        &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004…\n$ year         &lt;dbl&gt; 2011, 2011, 2012, 2013, 2014, 2015, 2…\n$ total_comp   &lt;dbl&gt; 5786.400, 5786.400, 4182.832, 5247.77…\n$ market_value &lt;dbl&gt; 485.2897, 485.2897, 790.0029, 961.308…\n$ ni           &lt;dbl&gt; 67.723, NA, 55.000, 72.900, 10.200, 4…\n\n\n\n\nMore information on joins to merge two datasets on the tidyverse website. I prefer the left_join function as the default because it indicates that we have a main dataset (left) to which we want to add a second dataset (right). We will also spend more time with these functions in week 7."
  },
  {
    "objectID": "slides/slides2.html#first-plot",
    "href": "slides/slides2.html#first-plot",
    "title": "Simulations, Regressions, and Significance",
    "section": "First plot",
    "text": "First plot\n\nThe BasicsThe scalesZoom in\n\n\n\n\n\nplot_comp_value &lt;- ggplot(\n    us_comp_value,\n    aes(y = total_comp, x = market_value)) +\n    geom_point(alpha = .10) +\n    ylab(\"compensation ($ 000)\") +\n    xlab(\"market value ($ million)\")\n\n\n\nprint(plot_comp_value)\n\n\n\n\n\n\n\n\n\n\n\nplot_log &lt;- plot_comp_value +\n    scale_x_continuous(\n        trans = \"log1p\",\n        breaks = c(1e2, 1e3, 1e4, 1e5, 1e6),\n        labels = function(x)\n            prettyNum(x/1000, digits = 2)) +\n    scale_y_continuous(\n        trans = \"log1p\",\n        breaks = c(1e1, 1e2, 1e3, 1e4, 1e5),\n        labels = function(x)\n            prettyNum(x/1000, digits = 2)) +\n    ylab(\"compensation ($ million)\") +\n    xlab(\"market value ($ billion)\")\n\n\n\nprint(plot_log)\n\n\n\n\n\n\n\n\n\n\n\nplot_zoom &lt;- plot_log +\n    coord_cartesian(\n        xlim = c(1e1, NA), ylim = c(1e2, NA))\n\n\n\nprint(plot_zoom)"
  },
  {
    "objectID": "slides/slides2.html#notation",
    "href": "slides/slides2.html#notation",
    "title": "Simulations, Regressions, and Significance",
    "section": "Notation",
    "text": "Notation\n\n\n\\[\\begin{equation}\ny_i = a + b_1 x_{1i} + ... + b_n x_{ni} + \\epsilon_i\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vec{y} = a + b_1 \\vec{x_1} + ... b_n \\vec{x_n} + \\vec{\\epsilon}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vec{y} = a + \\vec{b} X + \\vec{\\epsilon}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vec{y} = \\mathcal{N}(a + \\vec{b} X, \\sigma)\n\\end{equation}\\]\nSee also Chapter 13 in Huntington-Klein (2021)\n\n\nreg &lt;- lm(y ~ x1 + x2, data = my_data_set)\nsummary(reg)"
  },
  {
    "objectID": "slides/slides2.html#finally-the-regression",
    "href": "slides/slides2.html#finally-the-regression",
    "title": "Simulations, Regressions, and Significance",
    "section": "Finally, the regression",
    "text": "Finally, the regression\n\nreg &lt;- lm(log(total_comp + 1) ~ log(market_value + 1),\n         data = us_comp_value)\n# summary(reg)\nprint(summary(reg)$coefficients, digits = 2)\n\n                      Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                5.2     0.0259     201        0\nlog(market_value + 1)      0.4     0.0032     124        0\n\n\n\n\nThe regression uses a trick with the log(... + 1) formulation which is probably not appropriate. We will see the poisson regression later for a more appropriate way of analysing the effect on a positive variable."
  },
  {
    "objectID": "slides/slides2.html#historical-discussion",
    "href": "slides/slides2.html#historical-discussion",
    "title": "Simulations, Regressions, and Significance",
    "section": "Historical Discussion",
    "text": "Historical Discussion\n\nOur estimates of the pay-performance relation (including pay, options, stockholdings, and dismissal) for chief executive officers indicate that CEO wealth changes $3.25 for every $1,000 change in shareholder wealth (Jensen and Murphy 1990).\n\n\n\n[…] The statistic in isolation can present a misleading picture of pay to performance relationships because the denominator - the change in firm value - is so large (Hall and Liebman 1998).\n\n\n\n\nThis article addresses four major concerns about the pay of U.S. CEOs: (1) failure to pay for performance; […]. The authors’ main message is that most if not all of these concerns are exaggerated by the popular tendency to focus on the annual income of CEOs (consisting of salary, bonus, and stock and option grants) while ignoring their existing holdings of company equity (Core, Guay, and Thomas 2005).\n\n\nThis is actually a good example of why a literature review is valuable. It’s not enough to just say that the three papers find different effects. They do more than that. The newer papers gradually build up a better theory of what happens in practice and use better measures to reflect that theory."
  },
  {
    "objectID": "slides/slides2.html#stock-holding-data",
    "href": "slides/slides2.html#stock-holding-data",
    "title": "Simulations, Regressions, and Significance",
    "section": "Stock holding data",
    "text": "Stock holding data\n\nus_comp &lt;- readRDS(here(\"data\", \"us-compensation-new.RDS\")) %&gt;%\n    rename(total_comp = tdc1, shares = shrown_tot_pct) %&gt;%\n    select(gvkey, execid, year, shares, total_comp)\nus_value &lt;- readRDS(here(\"data\", \"us-value-new.RDS\")) %&gt;%\n    rename(year = fyear, market_value = mkvalt)\nus_comp_value &lt;- left_join(\n    us_comp, us_value, by = c(\"year\", \"gvkey\")) %&gt;%\n    filter(!is.na(market_value) & !(is.na(shares))) %&gt;%\n    mutate(wealth = shares * market_value / 100)\nglimpse(us_comp_value)\n\nRows: 21,262\nColumns: 7\n$ gvkey        &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001004…\n$ execid       &lt;chr&gt; \"09249\", \"09249\", \"09249\", \"09249\", \"09249\", \"09249\", \"09…\n$ year         &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 202…\n$ shares       &lt;dbl&gt; 2.964, 2.893, 3.444, 3.877, 4.597, 5.417, 3.718, 1.277, 1…\n$ total_comp   &lt;dbl&gt; 5786.400, 4182.832, 5247.779, 5234.648, 4674.464, 6072.59…\n$ market_value &lt;dbl&gt; 485.2897, 790.0029, 961.3080, 1046.3954, 842.5112, 1200.3…\n$ wealth       &lt;dbl&gt; 14.3839867, 22.8547839, 33.1074475, 40.5687497, 38.730239…"
  },
  {
    "objectID": "slides/slides2.html#shares-to-market-value",
    "href": "slides/slides2.html#shares-to-market-value",
    "title": "Simulations, Regressions, and Significance",
    "section": "Shares to Market Value",
    "text": "Shares to Market Value\n\n\n\nplot_shares &lt;- ggplot(\n    data = us_comp_value,\n    aes(x = market_value/1000, y = shares)) +\n    geom_point(alpha = .10) +\n    ylab(\"CEO Ownership\") +\n    xlab(\"Firm Market Value (in Billions)\") +\n    scale_x_continuous(\n        trans = \"log\",\n        labels = function(x)\n            prettyNum(x, digits = 2),\n        breaks =\n            scales::log_breaks(n = 5,\n                               base = 10)) +\n    scale_y_continuous(\n        trans = \"log\",\n        labels =\n            function(x)\n                prettyNum(x, digits = 2),\n        breaks =\n            scales::log_breaks(n = 5,\n                               base = 10))\n\n\n\nprint(plot_shares)"
  },
  {
    "objectID": "slides/slides2.html#pay-to-performance-sensitivity",
    "href": "slides/slides2.html#pay-to-performance-sensitivity",
    "title": "Simulations, Regressions, and Significance",
    "section": "Pay to Performance Sensitivity",
    "text": "Pay to Performance Sensitivity\n\nDataPlot\n\n\n\nus_sens &lt;- us_comp_value %&gt;%\n    group_by(gvkey, execid) %&gt;%\n    arrange(year) %&gt;%\n    mutate(prev_market_value = lag(market_value),\n            prev_wealth = lag(wealth)) %&gt;%\n    ungroup() %&gt;%\n    mutate(change_log_value = log(market_value) - log(prev_market_value),\n           change_log_wealth = log(wealth) - log(prev_wealth)) %&gt;%\n    filter(!is.infinite(change_log_wealth)) %&gt;%\n    arrange(gvkey)\n\n\n\nThe assumption for pay-for-performance and incentives is that we want to measure whether a CEO has taken the correct decisions. In a bigger firm, the impact of a CEOs decisions are larger. If you improve management of employees, then the effects will be bigger for a firm with more employees.\nThe other assumption is that CEOs do not care about dollar increases in dollars but in increases in percentages. Partly\n\n\\[\n\\begin{align}\n\\frac{\\partial W}{W} \\frac{V}{\\partial V}  \\\\\n  &= \\frac{ln(W)}{ln(V)}\n\\end{align}\n\\]\n\n\n\n\n\n\nplot_hypothesis &lt;- ggplot(\n    us_sens,\n    aes(y = change_log_wealth / change_log_value,\n        x = market_value/1000)) +\n  geom_point(alpha = .1) +\n  scale_x_continuous(\n    trans = \"log\", \n    breaks = scales::log_breaks(n = 5, base = 10),\n    labels = function(x) prettyNum(x, dig = 2)) +\n  coord_cartesian(\n    ylim = c(-10, 10)) +\n  xlab(\"market value\") +\n  ylab(\"sensitivity\")\n\n\n\nprint(plot_hypothesis)"
  },
  {
    "objectID": "slides/slides2.html#randomisation-or-permutation-test",
    "href": "slides/slides2.html#randomisation-or-permutation-test",
    "title": "Simulations, Regressions, and Significance",
    "section": "Randomisation or Permutation Test",
    "text": "Randomisation or Permutation Test\n\nRandomisationTest\n\n\n\ndata_hypo &lt;- us_sens %&gt;%\n    mutate(\n      sensitivity = change_log_wealth / change_log_value) %&gt;%\n  select(sensitivity, market_value) %&gt;%\n  filter(complete.cases(.))\n\nobserved_cor &lt;- cor(\n  data_hypo$sensitivity, data_hypo$market_value)\n\nrandom_cor &lt;- cor(\n  data_hypo$sensitivity, sample(data_hypo$market_value))\n\nprint(prettyNum(c(observed_cor, random_cor), dig = 3))\n\n[1] \"-0.00192\" \"0.00512\" \n\n\n\n\n\n\n\nsimulate_cor &lt;- function(data){\n    return(cor(data$sensitivity,\n               sample(data$market_value)))}\nrand_cor &lt;- replicate(1e4,\n                      simulate_cor(data_hypo))\n\n\nhist_sim &lt;- ggplot(\n    mapping = aes(\n        x = rand_cor,\n        fill = abs(rand_cor) &lt; abs(observed_cor))) +\n    geom_histogram(bins = 1000) +\n    xlab(\"Random Correlations\") +\n    scale_fill_manual(values = c(uwa_blue, uwa_gold)) +\n    theme(legend.position = \"none\") +\n    coord_cartesian(\n        xlim = c(-0.1, 0.1))\n\n\n\nplot(hist_sim)"
  },
  {
    "objectID": "slides/slides2.html#bootstrap",
    "href": "slides/slides2.html#bootstrap",
    "title": "Simulations, Regressions, and Significance",
    "section": "Bootstrap",
    "text": "Bootstrap\n\n\n\ncalc_corr &lt;- function(d){\n  n &lt;- nrow(d)\n  id_sample &lt;- sample(1:n, size = n,\n                      replace = TRUE)\n  sample &lt;- d[id_sample, ]\n  corr &lt;- cor(sample$sensitivity,\n              sample$market_value)\n  return(corr)\n}\nboot_corr &lt;- replicate(\n    2000, calc_corr(data_hypo))\n\n\nplot_boot &lt;- ggplot(\n    mapping = aes(x = boot_corr)) +\n  geom_histogram(bins = 100, colour = uwa_blue,\n                 fill = uwa_blue) +\n    geom_vline(aes(xintercept = 0),\n               colour = uwa_gold) +\n    xlab(\"Bootstrapped Correlation\")\n\n\n\nprint(plot_boot)"
  },
  {
    "objectID": "slides/slides2.html#comparison",
    "href": "slides/slides2.html#comparison",
    "title": "Simulations, Regressions, and Significance",
    "section": "Comparison",
    "text": "Comparison\n\n\nPermutation Test\n\nCalculate the observed statistic\nRandomly resample the data by breaking the relation you want to test (= Null Hypothesis)\nCalculate the statistic for each random sample\nIs the observed statistic more extreme than the randomly resampled statistic?\n\nSee also Chapter 4.2 in Cunningham (2021)\n\nBootstrap\n\nRandomly sample observed observations with replacement.\nCalculate the statistic you are interested in.\nIs the distribution of resampled statistics unlikely to be 0 (= Null Hypothesis)?\n\nSee also Chapter 15 in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides2.html#formula-based-p-value",
    "href": "slides/slides2.html#formula-based-p-value",
    "title": "Simulations, Regressions, and Significance",
    "section": "Formula Based P-value",
    "text": "Formula Based P-value\n\ncor &lt;- cor.test(data_hypo$sensitivity, data_hypo$market_value)\npvalue_cor &lt;- cor$p.value\nprint(prettyNum(pvalue_cor, dig = 2))\n\n[1] \"0.81\"\n\n\n\n\nregr_sens &lt;- lm(sensitivity ~ I(market_value/1e3), data = data_hypo)\ncoefficients(summary(regr_sens)) %&gt;% print(dig = 2)\n\n                     Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)            1.3712      1.024    1.34     0.18\nI(market_value/1000)  -0.0037      0.015   -0.24     0.81"
  },
  {
    "objectID": "slides/slides2.html#models",
    "href": "slides/slides2.html#models",
    "title": "Simulations, Regressions, and Significance",
    "section": "Models",
    "text": "Models\n\n\nSignaling Model\n\n\n\nPeacock’s tail as a signal\n\n\n\nCheap Talk Model\n\n\n\nAssumptions are important"
  },
  {
    "objectID": "slides/slides2.html#answers",
    "href": "slides/slides2.html#answers",
    "title": "Simulations, Regressions, and Significance",
    "section": "Answers",
    "text": "Answers\n\nN &lt;- 1000\nhigh_performance &lt;- rbinom(.x, .y, .z)\ndonation &lt;- ifelse(.x, 1, 0)\nreturn &lt;- ifelse(donation == 1, .y, .z)\nobserved_donation &lt;- ifelse(rbinom(N, 1, .9) == 1, donation, 1 - donation)\nobserved_return &lt;- ... \nsig &lt;- tibble(return = ...,\n              donation = ...) %&gt;%\n    mutate(donated = ...)\nglimpse(sig)\n\n\n\nsig_plot &lt;- ggplot(..., aes(x = .x, y = .y)) +\n    geom_jitter(width = .3)\nplot(sig_plot)\n\n\n\n\nsig_reg &lt;- lm(..., data = sig)\nsummary(...)"
  },
  {
    "objectID": "slides/slides2.html#references",
    "href": "slides/slides2.html#references",
    "title": "Simulations, Regressions, and Significance",
    "section": "",
    "text": "Core, John E., Wayne R. Guay, and Randall S. Thomas. 2005. “Is U.S. CEO Compensation Broken?” Journal of Applied Corporate Finance 17 (4): 97–104. https://doi.org/10.1111/j.1745-6622.2005.00063.x.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Causal Inference. Yale University Press. https://doi.org/10.12987/9780300255881.\n\n\nEdmans, Alex, and Xavier Gabaix. 2016. “Executive Compensation: A Modern Primer.” Journal of Economic Literature 54 (4): 1232–87.\n\n\nHall, Brian J., and Jeffrey B. Liebman. 1998. “Are CEOs Really Paid Like Bureaucrats?” The Quarterly Journal of Economics 113 (3): 653–91. https://doi.org/10.1162/003355398555702.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. First. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003226055.\n\n\nJensen, Michael C., and Kevin J. Murphy. 1990. “Performance Pay and Top-Management Incentives.” Journal of Political Economy 98 (2): 225–64. https://doi.org/10.1086/261677.\n\n\nTervio, Marko. 2008. “The Difference That CEOs Make: An Assignment Model Approach.” American Economic Review 98 (3): 642–68. https://doi.org/10.1257/aer.98.3.642."
  },
  {
    "objectID": "auxilary/matching_theory.html",
    "href": "auxilary/matching_theory.html",
    "title": "Matching theory",
    "section": "",
    "text": "A good research project relies on strong theoretical foundation. Theories are a summary of prior research findings and present predictions you can test. In accounting and finance, a lot of theories are mathematical theories. Sometimes, research articles will give a good explanation of the arguments in a theory but sometimes you will have to put in some extra effort to understand the theory. In this section, I will give you two techniques that can help to understand the theory better. The first is making the theory simpler and less general and redo the derivations. The second is to simulate data based on the theory and to visualise the theory with plots. Computers are very good at doing calculations. Whenever possible, you should let computers do the work for you. Simulate and visualise is a technique, we will use a lot more in the rest of the notes."
  },
  {
    "objectID": "auxilary/matching_theory.html#introduction",
    "href": "auxilary/matching_theory.html#introduction",
    "title": "Matching theory",
    "section": "",
    "text": "A good research project relies on strong theoretical foundation. Theories are a summary of prior research findings and present predictions you can test. In accounting and finance, a lot of theories are mathematical theories. Sometimes, research articles will give a good explanation of the arguments in a theory but sometimes you will have to put in some extra effort to understand the theory. In this section, I will give you two techniques that can help to understand the theory better. The first is making the theory simpler and less general and redo the derivations. The second is to simulate data based on the theory and to visualise the theory with plots. Computers are very good at doing calculations. Whenever possible, you should let computers do the work for you. Simulate and visualise is a technique, we will use a lot more in the rest of the notes."
  },
  {
    "objectID": "auxilary/matching_theory.html#new-theory-ceo-firm-matching.",
    "href": "auxilary/matching_theory.html#new-theory-ceo-firm-matching.",
    "title": "Matching theory",
    "section": "New theory: CEO-firm matching.",
    "text": "New theory: CEO-firm matching.\nLet us introduce a new theory how the size of the company is related to the compensation of the CEO. In the first lecture I presented a basic model where talented CEOs hire more people and attract more capital and thus grow the company. This theory ignored that companies and CEOs can choose to work with each other. In this section, we introduce a new theory about matching firms and CEOs Tervio (2008).\nThe theory assumes that the increase in \\(V\\)alue of a firm from time \\(0\\) to time \\(1\\) is given by the following equation.\n\\[\nV_1(n) - V_0(n) = C V_0(n) T(m)\n\\tag{1}\\]\nThe increase in value depends on the \\(T\\)alent of the CEO, the inital \\(V\\)alue of the firm, and a scaling factor \\(C\\). 1 \\(n\\) is the rank of the size of the firm and \\(m\\) is the rank of the talent of the CEO. \\(n = 1\\) is the largest firm, \\(m = 1\\) is the most talented CEO. The question the theory is trying to answer which CEO, \\(m\\), is going to work for firm, \\(n\\).\nThe theory assumes that firms will make a decision about which CEO to hire and CEOs will only accept to work for a firm if they cannot do better in another firm. The firms have to compensate a manager with a \\(w\\)age and will maximise the residual value of the firm, \\(V_1(n) - V_0(n) - w(m)\\). The model assumes that the managers will need a compensation above \\(w_0\\).\nTervio (2008) and Edmans and Gabaix (2016) show that when \\(m = n\\), no firm or CEO can improve themselves by switching. This means that the most talented CEO works for the largest firm, the second most talented CEO works for the second largest firm, until we reach the least talented CEO and the smallest firm. The intuition is that CEOs have a larger impact in larger firms. 2 Therefor, most value is created when the largest firms are managed by the best CEOs. The difficulty is to determine how much each firm should pay the CEO. I first go over a simplified mathematical model that gets some of the intuition across, then I explain how you can simulate from the more complicated model"
  },
  {
    "objectID": "auxilary/matching_theory.html#three-firms---three-ceos-model",
    "href": "auxilary/matching_theory.html#three-firms---three-ceos-model",
    "title": "Matching theory",
    "section": "Three firms - three CEOs model",
    "text": "Three firms - three CEOs model\nLet us assume that there are only three CEOs and only three firms. In equilibrium, we want to make sure that the largest firm (\\(n=1\\)) cannot do better than hiring the most talented CEO (\\(m=1\\)). In other words, the performance of the most talented CEO after paying their compensation, should be higher than the residual performance of the other two CEOs.\n\\[\n\\begin{aligned}\nCV_0(1) T(1) - w(1) \\geq CV_0(1) T(2) - w(2) \\\\\nCV_0(1) T(1) - w(1) \\geq CV_0(1) T(3) - w(3)\n\\end{aligned}\n\\tag{2}\\]\nNext, the second largest firm has to be better off hiring the the second best CEO.\n\\[\nCV_0(2) T(2) - w(2) \\geq CV_0(2) T(3) - w(3)\n\\tag{3}\\]\nIf we add the first condition of Equation 2 to condition Equation 3. We can rewrite everything and get the second condition of (Equation 2).\n\\[\n\\begin{align*}\nCV_0(1) T(1) - w(1) + CV_0(2) T(2) - {\\color{blue}{w(2)}}\n\\geq CV_0(1) T(2) - {\\color{blue}{w(2)}} + CV_0(2) T(3) - w(3)\n\\\\\nCV_0(1) T(1) - w(1) \\geq C(V_0(1) - V_0(2))T(2) + C(V_0(2) -\n{\\color{blue}{V_0(1)}})T(3) + {\\color{blue}{CV_0(1)T(3)}}- w(3)\n\\\\\nCV_0(1) T(1) - w(1) \\geq\n{\\color{blue} {C(V_0(1) - V_0(2))(T(2) - T(3))}} + CV_0(1)T(3) - w(3)\n\\end{align*}\n\\]\nBecause \\(V_0(1) &gt; V_0(2)\\) and \\(T_0(2) &gt; T_0(3)\\), we can delete the first term on the right hand side. So, the two inequalities give us the third inequality for free.\nBecause firms will prefer to paying a lower compensation than more compensation, firms pay their CEO just enough so that smaller firms are not willing to pay the same amount of money to poach the CEO away. For each firm, we have to make sure that the advantage of having a more talented CEO is smaller than the extra wage of hiring the CEO.\n\\[\n\\begin{align*}\nCV_0(2) (T(1) - T(2)) \\geq w(1) - w(2) \\\\\nCV_0(3) (T(2) - T(3)) \\geq w(2) - w(3)\n\\end{align*}\n\\]\nBecause firm \\(n=1\\) and \\(2\\) will set the wage just high enough to make sure that a smaller firm does not poach their CEO, they will set the compensation \\(w(1)\\) and \\(w(2)\\) just high enough but not higher. We can simplify the resulting conditions than to equalities.\n\\[\n\\begin{aligned}\nCV_0(2) (T(1) - T(2)) + w(2) &= w(1) \\\\\nCV_0(3) (T(2) - T(3)) + w(3) &= w(2)\n\\end{aligned}\n\\tag{4}\\]\nThe deriviations are a bit tedious and I do not necessarily want you to be able to do this entirely on your own. However, it shows that with some small calculations and with some economic intuition about what we want to calculate, we can again derive the relation between firm value and CEO compensation."
  },
  {
    "objectID": "auxilary/matching_theory.html#n-firms---n-ceos-model",
    "href": "auxilary/matching_theory.html#n-firms---n-ceos-model",
    "title": "Matching theory",
    "section": "N firms - N CEOs model",
    "text": "N firms - N CEOs model\nIf the literature points you to a mathematical model and you want to understand it better. Breaking it down to a simpler model with only 2 or 3 firms can be very illuminating. I hope the three firm model helped you to get some of the intuition behind the model without resorting to too complicated maths. It is relatively easy to see that we can write the inequalities in Equation 4 in a more general form.\n\\[\nCV_0(n + 1) (T(n) - T(n+1)) + w(n + 1) = w(n) \\\\\n\\forall n = 1,.., N-1\n\\tag{5}\\]\nThe basic idea is that a larger firm will pay more for a CEO than a smaller firm. If the larger firm wants to make sure that a the more talented CEO works for them, they need to pay the CEO a high enough wage. The difference between the two wages for a talented CEO will be the surplus that the CEO would create in the smaller firm. So the smaller firm will not be willing to poach the more talented CEO because the costs (higher wage) would outweight the benefit (higher surplus).\nThe original papers go further with the derivations Tervio (2008). However, this is an algorithm we can give to R when we add some further assumptions. So instead of going over all the math, we are going to simulate wages and firm values based on the algorithm.\nThe original theoretical papers also need the extra assumptions. The goal of the simulation is to show that sometimes you do not need all the fancy maths when you can write a computer program to do the work for you. We will use similar assumptions as the original papers but implement them in an R program."
  },
  {
    "objectID": "auxilary/matching_theory.html#simulation",
    "href": "auxilary/matching_theory.html#simulation",
    "title": "Matching theory",
    "section": "Simulation",
    "text": "Simulation\nFirst, let us load the tidyverse package.\n\nlibrary(tidyverse)\n\nNext, we simulate data for obs=500 observations. size_rate is a parameter that controls the size of firms. A value of 1 means that firms have constant returns to size, the same assumption as in lecture 1. talent_rate is something similar for the Talent of the CEOs. A larger number for both rate parameters implies that differences between sizes or CEOs become larger at the top. C is the \\(C\\) constant in the model. scale is an additional parameter that helps me scale the size of the firms so that I get similar numbers as the real data. 3 w0 is the base wage for the least talented CEO.\n\nobs &lt;- 500\nsize_rate &lt;- 1; talent_rate &lt;- 2/3; \nC &lt;- 10; scale &lt;- 600; w0 &lt;- 0; \nn &lt;- c(1:obs)\nsize &lt;- scale * n ^ (-size_rate)\ntalent &lt;- - 1/talent_rate * n ^ (talent_rate)\n\nn is an R vector of length obs (i.e. 500) with values from 1 to obs. So it is the rank in size and talent for each firm and CEO. You can see what n look like by just typing n and enter in the R console.\nSize and talent follow an exponential distribution which has some theoretical motivation given in Tervio (2008) and Edmans and Gabaix (2016). If that interests you, please go have a look but it goes beyond what we need today. What we do is give a value for the size of each firm from 1 to 500 and for the talent of each CEO from 1 to 500.\nWe can also calculate the wage of each CEO-firm combination from Equation 5. We start by creating a wage vector with 500 NA values. 4 At the last (obs = 500) position of the vector, we set the wage equal to w0 for the least talented CEO and the smallest firm. Than for each firm (we go from i = 499 to i = 1), we set the wage as the wage of the smaller firm (i + 1) and subtract the surplus our CEO would generate in the smaller firm over what their CEO is now generating.5\n\nwage &lt;- rep(NA, obs)\nwage[obs] &lt;- w0\nfor (i in (obs - 1):1){\n  wage[i] &lt;- wage[i + 1] + 1/scale * C * size[i + 1] * \n    (talent[i] - talent[i + 1])\n}\n\nNow we can put all our variables in a dataset. The tidyverse calls datasets tibbles and they are the main object that tidyverse functions work on.\n\nsimulated_data &lt;- tibble(\n  n = n,\n  size = size,\n  talent = talent,\n  wage = wage\n)"
  },
  {
    "objectID": "auxilary/matching_theory.html#visualisations",
    "href": "auxilary/matching_theory.html#visualisations",
    "title": "Matching theory",
    "section": "Visualisations",
    "text": "Visualisations\nWith the data we simulated, we can visualise the theory and see whether our theory matches our intuition. Visulising a theory is one way to understand the assumptions and to check whether it matches the data. Even if you do not have your data yet, it will give you an idea of what the data should look like.\nFigure 1 shows the relation between the simulated CEO wage and simulated firm size. It’s not perfect but the plot does follow a similar non-linear pattern as what we found in the first lecture. 6\n\nqplot(data = simulated_data, y = wage, x = size)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nFigure 1: Relation between simulated CEO wage and firm size.\n\n\n\n\nTo better understand the assumptions in the theory, we can also plot how talent and size change as a function of the rank of respectively the CEO and the firm. I glossed over the details before but that does not mean that we cannot check whether those functions make sense.\nFigure 2 shows us which assumptions were necessary for the theory to work. We see that the difference in talent at the top of the distribution (n = 1) is not that large, the difference in firm sizes is much more pronounced and is driving the difference in wages according to this theory.\n\np_talent &lt;- qplot(data = simulated_data, y = talent, x = n)\np_size &lt;- qplot(data = simulated_data, y = size, x = n) \ncowplot::plot_grid(p_talent, p_size, ncol = 2, labels = \"AUTO\")\n\n\n\n\nFigure 2: Relation between talent and rank of CEO (A) and between size and rank of the firm (B)\n\n\n\n\nIn the code, I use a function from the cowplot package to plot different plots (p_talent and p_size) in 1 row. The automatic labels will add the A and B labels for the plots."
  },
  {
    "objectID": "auxilary/matching_theory.html#functions-in-r",
    "href": "auxilary/matching_theory.html#functions-in-r",
    "title": "Matching theory",
    "section": "Functions in R",
    "text": "Functions in R\nOne of the most valuable aspects of R is that you can write new functions. Functions allow you to create your own verbs to apply to objects. In the previous section, we simulated data with a number of parameters in the theory set at a fixed value. If we want to compare the sensitivity of the theory to changes in the parameters, we want to simulate new datasets with different parameter values. A function to simulate data is what we need.\nFunctions are created with the function function. In between brackets, you define the parameter you want to use in your functions and their default values. In between the curly braces {} you tell R what it should do with those parameters. Ideally, you should not rely on any parameter or data that is not defined in your function. R has some liberal defaults which might give you unexpected results if you do that. You can rely on external functions like I do to create the tibble.\nI do nothing in the function that I have not done before. The only addition is that at the end, the function returns the simulated data. You can then use the function to create a new simulated dataset. 7\n\ncreate_fake_data &lt;- function(obs = 500, size_rate = 1, \n                             talent_rate = 1,\n                             w0 = .001, C = 1){\n  scale = 600\n  n = 1:obs\n  size = scale * n ^ (-size_rate) \n  talent =  -1/talent_rate * n ^ talent_rate\n  wage = rep(NA, obs)\n  wage[obs] = w0 \n  for (i in (obs - 1):1){\n    wage[i] =  wage[i + 1] + 1/scale * C * size[i + 1] * \n      (talent[i] - talent[i + 1])\n  }\n  fake_data = dplyr::tibble(n = n, size = size, talent = talent, \n                            wage = wage)\n  return(fake_data)\n}\nfake_data &lt;- create_fake_data(talent_rate = 2/3, C = 0.01)\n\nWe can create new datasets where the talent_rate is increased to 1 (high talent) and the effect of CEOs on the surplus is increased to .015 instead of .10. I choose .10 in this simulation instead of 10 so that all values can be interpreted in billion USD to mimic the real data. Again, that is a rather arbitrary scaling issue.\n\ndata1 &lt;- create_fake_data(obs = 500, talent_rate = 2/3, C = .01) %&gt;%\n  mutate(talent_rate = \"low talent\", C = \"low effect\")\ndata2 &lt;- create_fake_data(obs = 500, talent_rate = 1, C = .01) %&gt;%\n  mutate(talent_rate = \"high talent\", C = \"low effect\")\ndata3 &lt;- create_fake_data(obs = 500, talent_rate = 2/3, C = .015) %&gt;%\n  mutate(talent_rate = \"low talent\", C = \"high effect\")\ndata4 &lt;- create_fake_data(obs = 500, talent_rate = 1, C = .015) %&gt;%\n  mutate(talent_rate = \"high talent\", C = \"high effect\")\ndata_exp &lt;- bind_rows(data1, data2, data3, data4)\ndata_exp\n\n# A tibble: 2,000 × 6\n       n  size talent   wage talent_rate C         \n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1     1 600    -1.5  0.0241 low talent  low effect\n 2     2 300    -2.38 0.0197 low talent  low effect\n 3     3 200    -3.12 0.0172 low talent  low effect\n 4     4 150    -3.78 0.0156 low talent  low effect\n 5     5 120    -4.39 0.0143 low talent  low effect\n 6     6 100    -4.95 0.0134 low talent  low effect\n 7     7  85.7  -5.49 0.0126 low talent  low effect\n 8     8  75    -6    0.0120 low talent  low effect\n 9     9  66.7  -6.49 0.0114 low talent  low effect\n10    10  60    -6.96 0.0110 low talent  low effect\n# ℹ 1,990 more rows\n\n\nThe bind_rows function allows you to combine the four datasets in one big dataset data_exp. This will help us to plot the data in Figure Figure 3 with the more complicated but also more flexible ggplot function. In the function, we first define the data we want to use. Within the aes() specification, we define the variables that are going to be plotted. We tell ggplot that we want the data to be plotted as the following geometric elements a point and a line.8 I make the line colour grey. The facet_grid creates different subplots depending on whether an observation has a different value for the talent_rate variable and the C variable.\n\nplot_exp &lt;- ggplot(data_exp, aes(x = size, y = wage)) +\n  geom_point() + \n  geom_line(colour = \"gray\") +\n  facet_grid(talent_rate ~ C)\nprint(plot_exp)\n\n\n\n\nFigure 3: Four simulated fake datasets on the relation between firm size and CEO compensation.\n\n\n\n\nIt looks like our theory is much more sensitive to changes in the talent rate than in changes to the scaling effect."
  },
  {
    "objectID": "auxilary/matching_theory.html#why-should-you-simulate-data",
    "href": "auxilary/matching_theory.html#why-should-you-simulate-data",
    "title": "Matching theory",
    "section": "Why should you simulate data?",
    "text": "Why should you simulate data?\nIn these notes, I will come back to the idea of simulation over and over. For three big reasons:\n\nSimulating data from a theory and visualising the theory helps you sharpen your intuition for your theory and for which values are reasonable and which ones are not. If you know upfront which values are not reasonable that will help you to interpret your findings in your statistical analysis. If you estimate parameters in your model that are too big or too small, it might be an indication that something went wrong with your analysis.\nWhen you simulate data you can simulate variables that you cannot observe (e.g. CEO talent). Sometimes these variables need to be included in your statistical analysis to get unbiased parameter estimates. With simulated datasets, you can run the analysis with and without the unobservable variables to investigate the impact of including and excluding the variable.\n\nIf there are different statistical tests you can use to investigate your research question, you do not want to test the different tests on your real data. If you pick the statistical test that gives you the “right” answer, you are likely to delude yourself. The right way to compare different statistical tests is to see whether they can estimate parameters in simulated data where you know what the right value of the parameter is.\n\nIn short, the advantage of being able to generete simulated data is that it sharpens your understanding of your theory and of your statistical test."
  },
  {
    "objectID": "auxilary/matching_theory.html#footnotes",
    "href": "auxilary/matching_theory.html#footnotes",
    "title": "Matching theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe scaling factor is not crucial to the theory. If performance is measured in millions of dollars or in billions of dollars, the scaling factor will differ but that is not telling us anything about the economic mechanism.↩︎\nIf a CEO is good at managing people, the impact of the CEO will be better if they are managing more people.↩︎\nThis is not necessarily a fudge. The theory does not say anything about whether we should measure firm size in USD, in AUD or in CNY. So the scale we use is arbitrary.↩︎\nThe rep function creates a vector of obs repititions of NA↩︎\nAgain scale and C just scale some of the values so that they are easier to interpret. These parameters are less important for the economic intuition.↩︎\nqplot is a funtion to make quick plots and it is part of the ggplot package which is part of the tidyverse.↩︎\nI call the function create_fake_data because I want to emphasise that there is nothing special about simulated data. Some people prefer more dignified names. If you prefer that, you can use any other name for the function. Just make sure that it is clear what your function is doing.↩︎\nObviously, you do not need both. This is just to show you what you can do.↩︎"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "",
    "text": "This setup code loads a package, here, that helps to navigate the folder structure in which I will create files. The tidyverse package is the main package, we will use to manipulate datasets. There are other ways to program in R. I think that to start of the tidyverse way of looking at data is quite intuitive and it is very well supported. The intuition and how quickly you can do meaningful things will hopefully be clear by the end of this document.\nlibrary(here) \ni_am(\"auxilary/introduction_to_rstudio_for_accfin.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(tidyverse)\nlibrary(dbplyr)"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#with-the-tidyverse",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#with-the-tidyverse",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "With the tidyverse",
    "text": "With the tidyverse\n\nus_comp &lt;- readRDS(here(\"data\", \"us-compensation-new.RDS\"))\nglimpse(us_comp)\n\nRows: 23,096\nColumns: 22\n$ year                    &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018…\n$ gvkey                   &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004\", \"00100…\n$ cusip                   &lt;chr&gt; \"00036110\", \"00036110\", \"00036110\", \"00036110\"…\n$ exec_fullname           &lt;chr&gt; \"David P. Storch\", \"David P. Storch\", \"David P…\n$ coname                  &lt;chr&gt; \"AAR CORP\", \"AAR CORP\", \"AAR CORP\", \"AAR CORP\"…\n$ ceoann                  &lt;chr&gt; \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO…\n$ execid                  &lt;chr&gt; \"09249\", \"09249\", \"09249\", \"09249\", \"09249\", \"…\n$ bonus                   &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.00…\n$ salary                  &lt;dbl&gt; 867.000, 877.838, 906.449, 906.449, 755.250, 8…\n$ stock_awards_fv         &lt;dbl&gt; 2664.745, 619.200, 1342.704, 1695.200, 1150.50…\n$ stock_unvest_val        &lt;dbl&gt; 4227.273, 6018.000, 4244.165, 4103.283, 1334.0…\n$ eip_unearn_num          &lt;dbl&gt; 32.681, 56.681, 66.929, 83.415, 91.969, 157.96…\n$ eip_unearn_val          &lt;dbl&gt; 393.806, 1137.021, 1626.375, 2464.079, 2244.96…\n$ option_awards           &lt;dbl&gt; 578.460, 695.520, 1622.016, 0.000, 1150.500, 1…\n$ option_awards_blk_value &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ option_awards_num       &lt;dbl&gt; 49.022, 144.000, 158.400, 0.000, 153.810, 225.…\n$ tdc1                    &lt;dbl&gt; 5786.400, 4182.832, 5247.779, 5234.648, 4674.4…\n$ tdc2                    &lt;dbl&gt; 6105.117, 3487.312, 3809.626, 10428.375, 3523.…\n$ shrown_tot_pct          &lt;dbl&gt; 2.964, 2.893, 3.444, 3.877, 4.597, 5.417, 3.71…\n$ becameceo               &lt;date&gt; 1996-10-09, 1996-10-09, 1996-10-09, 1996-10-0…\n$ joined_co               &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ reason                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nfilter(us_comp, gvkey == \"001004\")\n\n# A tibble: 11 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2011 001004 00036110 David P. Storch       AAR C… CEO    09249      0   867 \n 2  2012 001004 00036110 David P. Storch       AAR C… CEO    09249      0   878.\n 3  2013 001004 00036110 David P. Storch       AAR C… CEO    09249      0   906.\n 4  2014 001004 00036110 David P. Storch       AAR C… CEO    09249      0   906.\n 5  2015 001004 00036110 David P. Storch       AAR C… CEO    09249      0   755.\n 6  2016 001004 00036110 David P. Storch       AAR C… CEO    09249      0   835 \n 7  2017 001004 00036110 David P. Storch       AAR C… CEO    09249      0   941 \n 8  2018 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   750 \n 9  2019 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   801.\n10  2020 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   781.\n11  2021 001004 00036110 John McClain Holmes,… AAR C… CEO    48195      0   925 \n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;\n\n\n\nfilter(us_comp, bonus == 0, year == 2012)\n\n# A tibble: 1,678 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2012 001004 00036110 David P. Storch       AAR C… CEO    09249      0   878.\n 2  2012 001045 02376R10 Thomas W. Horton      AMERI… CEO    26059      0   618.\n 3  2012 001075 72348410 Donald E. Brandt, CPA PINNA… CEO    05835      0  1146 \n 4  2012 001076 74319R10 Ronald W. Allen       PROG … CEO    00283      0   850 \n 5  2012 001078 00282410 Miles D. White, M.B.… ABBOT… CEO    14300      0  1900 \n 6  2012 001094 00444610 Albert L. Eilender    ACETO… CEO    46204      0   626.\n 7  2012 001161 00790310 Rory P. Read          ADVAN… CEO    42390      0  1000.\n 8  2012 001177 00817Y10 Mark Thomas Bertolini AETNA… CEO    31029      0   977.\n 9  2012 001209 00915810 John E. McGlade       AIR P… CEO    27315      0  1200 \n10  2012 001230 01165910 Bradley D. Tilden     ALASK… CEO    21308      0   420.\n# ℹ 1,668 more rows\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#a-gamestop-to-introduce-the-pipe",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#a-gamestop-to-introduce-the-pipe",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "A Gamestop to introduce the pipe",
    "text": "A Gamestop to introduce the pipe\n\nLook for any company where “gamestop” is in the name of the company.\nLook for the gvkey of Gamestop. Then use filter with exactly that key.\n\n\nfilter(us_comp, str_detect(tolower(coname),\n                           \"gamestop\"))\n\n# A tibble: 11 × 22\n    year gvkey  cusip    exec_fullname     coname     ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2011 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  2254.  1028.\n 2  2012 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  1515   1050.\n 3  2013 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719   975   1059.\n 4  2014 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1201.\n 5  2015 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1247.\n 6  2016 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1285.\n 7  2017 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1293.\n 8  2018 145049 36467W10 Shane S. Kim      GAMESTOP … CEO    56940    25    963.\n 9  2019 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080   150    846.\n10  2020 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080     0   1005.\n11  2021 145049 36467W10 Matthew Furlong   GAMESTOP … CEO    61979  1595.   115.\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;\n\nfilter(us_comp, gvkey == \"145049\")\n\n# A tibble: 11 × 22\n    year gvkey  cusip    exec_fullname     coname     ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2011 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  2254.  1028.\n 2  2012 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719  1515   1050.\n 3  2013 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719   975   1059.\n 4  2014 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1201.\n 5  2015 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1247.\n 6  2016 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1285.\n 7  2017 145049 36467W10 J. Paul Raines    GAMESTOP … CEO    35719     0   1293.\n 8  2018 145049 36467W10 Shane S. Kim      GAMESTOP … CEO    56940    25    963.\n 9  2019 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080   150    846.\n10  2020 145049 36467W10 George E. Sherman GAMESTOP … CEO    46080     0   1005.\n11  2021 145049 36467W10 Matthew Furlong   GAMESTOP … CEO    61979  1595.   115.\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#moving-on",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#moving-on",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "Moving on",
    "text": "Moving on\n\nselect(us_comp, year, coname, bonus, salary, total = tdc1)\n\n# A tibble: 23,096 × 5\n    year coname   bonus salary total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  2011 AAR CORP     0   867  5786.\n 2  2012 AAR CORP     0   878. 4183.\n 3  2013 AAR CORP     0   906. 5248.\n 4  2014 AAR CORP     0   906. 5235.\n 5  2015 AAR CORP     0   755. 4674.\n 6  2016 AAR CORP     0   835  6073.\n 7  2017 AAR CORP     0   941  6284.\n 8  2018 AAR CORP     0   750  3344.\n 9  2019 AAR CORP     0   801. 4736.\n10  2020 AAR CORP     0   781. 4123.\n# ℹ 23,086 more rows\n\n\nWe first build up the dataset with select and mutate and the pipe %&gt;%. When we are satisfied with the result, we can save the dataset as an R object with the name us_comp_small.\n\nus_comp_small &lt;-\n  select(us_comp, year, coname, bonus,\n       salary, total = tdc1) %&gt;%\n    mutate(salary_percentage = salary/total)\nprint(us_comp_small)\n\n# A tibble: 23,096 × 6\n    year coname   bonus salary total salary_percentage\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;\n 1  2011 AAR CORP     0   867  5786.             0.150\n 2  2012 AAR CORP     0   878. 4183.             0.210\n 3  2013 AAR CORP     0   906. 5248.             0.173\n 4  2014 AAR CORP     0   906. 5235.             0.173\n 5  2015 AAR CORP     0   755. 4674.             0.162\n 6  2016 AAR CORP     0   835  6073.             0.138\n 7  2017 AAR CORP     0   941  6284.             0.150\n 8  2018 AAR CORP     0   750  3344.             0.224\n 9  2019 AAR CORP     0   801. 4736.             0.169\n10  2020 AAR CORP     0   781. 4123.             0.189\n# ℹ 23,086 more rows"
  },
  {
    "objectID": "auxilary/introduction_to_rstudio_for_accfin.html#quick-descriptive-statistics",
    "href": "auxilary/introduction_to_rstudio_for_accfin.html#quick-descriptive-statistics",
    "title": "Introduction to Rstudio for Accounting and Finance",
    "section": "Quick descriptive statistics",
    "text": "Quick descriptive statistics\n\ngroup_by(us_comp, gvkey) %&gt;%\n    summarise(N = n(), N_CEO = n_distinct(execid),\n              average = mean(salary),\n              sd = sd(salary),\n              med = median(salary),\n              min = min(salary),\n              max = max(salary)) %&gt;%\n    ungroup() %&gt;%\n  filter(med &gt; 1000)\n\n# A tibble: 599 × 8\n   gvkey      N N_CEO average    sd   med   min   max\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 001075    12     2   1219. 111.  1222. 1091  1395 \n 2 001078    12     2   1788. 224.  1900  1298. 1973.\n 3 001161    12     3    961. 151.  1000.  566. 1149.\n 4 001209    12     2   1239. 131.  1200   905. 1402.\n 5 001274    11     1   1082. 101.  1030  1000  1250 \n 6 001300    12     2   1714. 152.  1750  1415. 1890 \n 7 001380    12     1   1500    0   1500  1500  1500 \n 8 001440    12     2   1339. 176.  1350.  903. 1522.\n 9 001447    12     2   1794. 262.  2000  1488. 2038.\n10 001449    12     1   1438.  12.1 1441. 1399. 1441.\n# ℹ 589 more rows\n\n\n\nsummarise(us_comp, N = n(), N_CEO = n_distinct(execid),\n              average = mean(salary),\n              sd = sd(salary),\n              med = median(salary),\n              min = min(salary),\n              max = max(salary),\n          .by = gvkey) %&gt;%\n  filter(med &gt; 1000)\n\n# A tibble: 599 × 8\n   gvkey      N N_CEO average    sd   med   min   max\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 001075    12     2   1219. 111.  1222. 1091  1395 \n 2 001078    12     2   1788. 224.  1900  1298. 1973.\n 3 001161    12     3    961. 151.  1000.  566. 1149.\n 4 001209    12     2   1239. 131.  1200   905. 1402.\n 5 001274    11     1   1082. 101.  1030  1000  1250 \n 6 001300    12     2   1714. 152.  1750  1415. 1890 \n 7 001380    12     1   1500    0   1500  1500  1500 \n 8 001440    12     2   1339. 176.  1350.  903. 1522.\n 9 001447    12     2   1794. 262.  2000  1488. 2038.\n10 001449    12     1   1438.  12.1 1441. 1399. 1441.\n# ℹ 589 more rows"
  },
  {
    "objectID": "method_package.html",
    "href": "method_package.html",
    "title": "Content",
    "section": "",
    "text": "This section contains a written introduction to R and Rstudio with the goal of getting you up and running with the main tools for data analysis and reporting results for the unit. The script I use for this introduction is also available with some explanations. The coding is more advanced and you should see it more as a useful resource than something you should be able to do at the start of the semester."
  },
  {
    "objectID": "method_package.html#introduction",
    "href": "method_package.html#introduction",
    "title": "Content",
    "section": "",
    "text": "This section contains a written introduction to R and Rstudio with the goal of getting you up and running with the main tools for data analysis and reporting results for the unit. The script I use for this introduction is also available with some explanations. The coding is more advanced and you should see it more as a useful resource than something you should be able to do at the start of the semester."
  },
  {
    "objectID": "method_package.html#slides",
    "href": "method_package.html#slides",
    "title": "Content",
    "section": "Slides",
    "text": "Slides\nThe slides contain the lecture slides for the first half of the semester.\nThe first set of slides introduce a number of practical issues around the structure, goals,and assignments for the unit. I also demonstrate what research looks like with an example from executive compensation. You can find the data on LMS.1\nThe second set of slides introduces the notion of simulations and simulated data. Simulations are a way to make an abstract theory more concrete and to test our intuition of statistical tests. This is exactly what we are going to do in this lecture where the theory is a matching theory of firms and CEOs. Finally, we will test whether there is a relation between pay-for-performance and the size of the firm.\nAs background reading, I have also made a more detailed explanation of the matching theory of CEO compensation. It reinforces the value of knowing how to simulate some data from a theory.\nThe third set of slides looks at the issue of when and how to control (and sometimes not control) for additional effects. It’s complicated!\nThe fourth and fifth set of slides basically give up on trying to control for everything. The goal is to focus on the research design, i.e. find a situation where we can be reasonable sure that our research question is answerable. I will focus my attention on event studies and its bigger (but slower) brother difference-in-difference and instrumental variables."
  },
  {
    "objectID": "method_package.html#freaky-friday-or-friday-earnings-announcements-are-weird",
    "href": "method_package.html#freaky-friday-or-friday-earnings-announcements-are-weird",
    "title": "Content",
    "section": "Freaky Friday or Friday Earnings Announcements Are Weird",
    "text": "Freaky Friday or Friday Earnings Announcements Are Weird\nFor the remainder of the unit, we will change the mode of teaching. The remaining parts are best seen as case studies into some specific topics. The first case study is an attempt at replicating the main results of Dellavigna and Pollet (2009) from scratch. The goal is twofold. First, the study is an event study of the market reaction to the release of news which is the workhorse study design for many finance studies. Second, the study requires different sources of data and it is a good exercise to demonstrate how to manage data in a larger project. Start from the introduction to the replication and go from there."
  },
  {
    "objectID": "method_package.html#machine-learning",
    "href": "method_package.html#machine-learning",
    "title": "Content",
    "section": "Machine Learning",
    "text": "Machine Learning\nThis is also a two parter. The main goal is to make you aware when machine learning tools are useful. The other part is a long R implementation of one of the machine learning techniques that is most related to linear regression. The last part is mainly to give you the code as a starting point."
  },
  {
    "objectID": "method_package.html#generalised-linear-models",
    "href": "method_package.html#generalised-linear-models",
    "title": "Content",
    "section": "Generalised Linear Models",
    "text": "Generalised Linear Models\nHere I introduce the use of GLM models for discrete outcomes. As before, I first emphasise why the regulare linear model works reasonably well for a lot of typical applications and when the GLM models are more appropriate. Maybe surprisingly, there is a clear link with the machine learning arguments."
  },
  {
    "objectID": "method_package.html#generated-variables",
    "href": "method_package.html#generated-variables",
    "title": "Content",
    "section": "Generated Variables",
    "text": "Generated Variables\nThis section looks at a diverse set of methods that all have one thing in common. They are often a combination of two are more regression steps. I show how that might effect the uncertainty estimates and how the bootstrap can help to get the uncertainty estimates correct."
  },
  {
    "objectID": "method_package.html#footnotes",
    "href": "method_package.html#footnotes",
    "title": "Content",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr download it yourself with the introduction script.↩︎"
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "Our main tool for data analysis is the R statistical software. R is free software specifically aimed at statistical analysis and graphical representation. It is a fully developed programming language that allows to extend it with new packages with new capabilities. We are going to heavily rely on some of those packages.1\nWe are going to interact with the R software through Rstudio.2 Rstudio is an integrated development environment for R. It allows you to easily write, test, and run R code, and integrate the results with explanations. One advantage of R and Rstudio is that you can install them on as many computers as you want. You do not need a special license. This means that you do not have to be affiliated with the university to use the sofware.\nIn Rstudio, you write code and ask the R software to execute that code. You can save the different steps in your analysis in an R script which is just a plain text file with the .R-extenstion. The advantage of plain text file is that they are easy to share across different operating systems. The code in your script allows you and anyone else to run the same analysis at a later point in time which is very important to check for mistakes and for teaching. We will make use of a special type of plain text files: Quarto (.qmd) files. 3.\nQuarto is an extension of the markdown format. The idea of markdown is to write plain text documents which can later be exported to other formats such as html, pdf, or Word. The beauty of RMarkdown is that it lets you combine both R scripts and markdown into one document. This allows you to integrate your analysis and your description of your analysis in one document. When you go back to your analysis after a couple of weeks, you will be happy that you have more than the raw code to look at.\nFinally, I am going to introduce you to a specific dialect of the R language which is informally known as the tidyverse. If you want to be a good R programmer this might not be the best entry point into R. However, I assume that you want to quickly pick up some tools to facilitate your research project and for that purpose I believe the tidyverse will be excellent. I will introduce the most important bits and pieces throughout my lectures however I don’t have the time to go into detail. Now and then, you will have to experiment on your own to get your code working. Some excellent resources are the free online books R for datascience and Data Visualization: A practical introduction. You can also buy reasonably priced physical copies if you your are interested in developing your R skills further.\nFor larger projects, you should use a project in its own separate folder.4. Once you need multiple scripts, you want to make sure that you can reliably point to the results or functions in different scripts. Projects help you manage all the parts of a larger piece of work. You can start a You start a new project by clicking File &gt; New Project ... You can start a new file by clicking File &gt; New File &gt; R Script.\n\n\n\nIn Rstudio, you can use the R console directly. I mainly use the R console to test the code I have written in my scripts and I would advice you to do the same thing. If you want to understand how R works. You can experiment by typing the following lines in the console.5\n\nx &lt;- 1\nx\n\n[1] 1\n\nx &lt;- x + 1\nx\n\n[1] 2\n\n\nThe little code above already shows you two things.\n\nYou can assign (and overwrite) a numerical value to an object x\nThe right-hand side is assigned to the left-hand side. You can’t just switch them around.\n\nWe will see further that we can assign almost anything to an object x. When x is a vector or a data set with multiple elements, this will allow us to perform the same function on each element of x. That is the basic advantage of programming. You can tell your computer how to do a thing and than the computer can do it over and over for you.\n\n\n\n\ninstall.packages(\"tidyverse\")\nlibrary(\"tidyverse\")\n\nR packages are additions to R that give R extra functionality. One of the selling points of R is that it has a good way of integrating this extra functionality and a lot of people are highly motivated to add this extra functionality. We will heavily rely on one meta-packages. tidyverse is a package that helps with data transformations and working with tabular data in a tidy fashion. One of the packages in the tidyverse is ggplot2 which provides tools to make pretty plots. The code above shows you how to install packages and use them. Normally, you will only have to install a package once. However, when you want to use a package in your script or code, you will have to load the package with the library() function. You only have to do load a package once at the start of your R session or at the start of your script. 6\ndplyr is another part of the tidyverse we will use extensively. With 5 verbs 7 and one pipe operator to glue them together helps you to explore the data.filter() let’s you filter out a subset of the observations. select() selects a subset of the variables. mutate() changes variables and creates new ones. group_by() and summarise() group subsets of observations and summarise them. %&gt;% is the pipe operator and joins together the verbs to create compound statements where you for instance filter a subset of the observations, create a new variable, create groups, and summarise the groups based on the average for the new variable.\n\n\n\nLet’s run some code. First, we have to tell R to use the packages in the tidyverse.\n\nlibrary(tidyverse)\n\nNext, we have a look at the data of the compensation of CEOs of S&P500 companies. First, we have to assign the file with the data to an object, us_comp in this case. 8 The glimpse function gives an overview of the variables in the data, the type of the variables, and a couple of examples of the values for that variable.\n\nus_comp &lt;- readRDS(here(\"data\", \"us-compensation-new.RDS\"))\nglimpse(us_comp)\n\nRows: 24,294\nColumns: 22\n$ year                    &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018…\n$ gvkey                   &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004\", \"00100…\n$ cusip                   &lt;chr&gt; \"00036110\", \"00036110\", \"00036110\", \"00036110\"…\n$ exec_fullname           &lt;chr&gt; \"David P. Storch\", \"David P. Storch\", \"David P…\n$ coname                  &lt;chr&gt; \"AAR CORP\", \"AAR CORP\", \"AAR CORP\", \"AAR CORP\"…\n$ ceoann                  &lt;chr&gt; \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO…\n$ execid                  &lt;chr&gt; \"09249\", \"09249\", \"09249\", \"09249\", \"09249\", \"…\n$ bonus                   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…\n$ salary                  &lt;dbl&gt; 867.000, 877.838, 906.449, 906.449, 755.250, 8…\n$ stock_awards_fv         &lt;dbl&gt; 2664.745, 619.200, 1342.704, 1695.200, 1150.50…\n$ stock_unvest_val        &lt;dbl&gt; 4227.273, 6018.000, 4244.165, 4103.283, 1334.0…\n$ eip_unearn_num          &lt;dbl&gt; 32.681, 56.681, 66.929, 83.415, 91.969, 157.96…\n$ eip_unearn_val          &lt;dbl&gt; 393.806, 1137.021, 1626.375, 2464.079, 2244.96…\n$ option_awards           &lt;dbl&gt; 578.460, 695.520, 1622.016, 0.000, 1150.500, 1…\n$ option_awards_blk_value &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ option_awards_num       &lt;dbl&gt; 49.022, 144.000, 158.400, 0.000, 153.810, 225.…\n$ tdc1                    &lt;dbl&gt; 5786.400, 4182.832, 5247.779, 5234.648, 4674.4…\n$ tdc2                    &lt;dbl&gt; 6105.117, 3487.312, 3809.626, 10428.375, 3523.…\n$ shrown_tot_pct          &lt;dbl&gt; 2.964, 2.893, 3.444, 3.877, 4.597, 5.417, 3.71…\n$ becameceo               &lt;date&gt; 1996-10-09, 1996-10-09, 1996-10-09, 1996-10-0…\n$ joined_co               &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ reason                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWe can also look at a subset of the observations, i.e. we filter the observations from the company that has the key 001045.\n\nfilter(us_comp, gvkey == \"001045\")\n\n# A tibble: 12 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2011 001045 02376R10 Gerard J. Arpey       AMERI… CEO    14591      0   614.\n 2  2012 001045 02376R10 Thomas W. Horton      AMERI… CEO    26059      0   618.\n 3  2013 001045 02376R10 Thomas W. Horton      AMERI… CEO    26059      0   592.\n 4  2014 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0   688.\n 5  2015 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0   232.\n 6  2016 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n 7  2017 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n 8  2018 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n 9  2019 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n10  2020 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n11  2021 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n12  2022 001045 02376R10 Robert D. Isom, Jr.   AMERI… CEO    46193      0  1162.\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;\n\n\nYou can see that the company, American Airlines, has had four CEOs since 2011. I used the gvkey to select a company and not its name. You will often use an id or a database key especially if you are working with multiple datasets and you want to link observations from one dataset to observations in the other dataset.\nWe can also filter data based on other variables. For instance, the below filters the observations from 2014 where the CEO had a salary over $1,000,000.\n\nfilter(us_comp, salary &gt; 1000, year == 2014)\n\n# A tibble: 494 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2014 001075 72348410 Donald E. Brandt, CPA PINNA… CEO    05835      0  1240 \n 2  2014 001078 00282410 Miles D. White, M.B.… ABBOT… CEO    14300      0  1973.\n 3  2014 001161 00790310 Rory P. Read          ADVAN… CEO    42390      0  1000.\n 4  2014 001300 43851610 David M. Cote         HONEY… CEO    20931   5500  1866.\n 5  2014 001380 42809H10 John B. Hess          HESS … CEO    02132      0  1500 \n 6  2014 001440 02553710 Nicholas K. Akins     AMERI… CEO    40778      0  1241.\n 7  2014 001447 02581610 Kenneth I. Chenault   AMERI… CEO    02157   4500  2000 \n 8  2014 001449 00105510 Daniel Paul Amos      AFLAC… CEO    00013      0  1441.\n 9  2014 001468 02637510 Jeffrey M. Weiss      AMERI… CEO    11710   3640  1012.\n10  2014 001487 02687478 Robert Herman Benmos… AMERI… CEO    20972      0  1385.\n# ℹ 484 more rows\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;\n\n\nYou can also select some variables if you are only interested in those. If a variable has an undescriptive name, you can use select to rename the variable. For instance, tdc1 is the total compensation of a the CEO. A more descriptive name will help you to remember what the variable actually means.\n\nselect(us_comp, year, coname, bonus, salary, total = tdc1)\n\n# A tibble: 24,294 × 5\n    year coname   bonus salary total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  2011 AAR CORP     0   867  5786.\n 2  2012 AAR CORP     0   878. 4183.\n 3  2013 AAR CORP     0   906. 5248.\n 4  2014 AAR CORP     0   906. 5235.\n 5  2015 AAR CORP     0   755. 4674.\n 6  2016 AAR CORP     0   835  6073.\n 7  2017 AAR CORP     0   941  6284.\n 8  2018 AAR CORP     0   750  3344.\n 9  2019 AAR CORP     0   801. 4736.\n10  2020 AAR CORP     0   781. 4123.\n# ℹ 24,284 more rows\n\n\nYou can also create new variables with the mutate function. I created a variable that calculates what percentage of total compensation is the CEO’s salary.\n\nselect(us_comp, year, coname, bonus, salary, total = tdc1) %&gt;%\n  mutate(salary_percentage = salary / total)  \n\n# A tibble: 24,294 × 6\n    year coname   bonus salary total salary_percentage\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;\n 1  2011 AAR CORP     0   867  5786.             0.150\n 2  2012 AAR CORP     0   878. 4183.             0.210\n 3  2013 AAR CORP     0   906. 5248.             0.173\n 4  2014 AAR CORP     0   906. 5235.             0.173\n 5  2015 AAR CORP     0   755. 4674.             0.162\n 6  2016 AAR CORP     0   835  6073.             0.138\n 7  2017 AAR CORP     0   941  6284.             0.150\n 8  2018 AAR CORP     0   750  3344.             0.224\n 9  2019 AAR CORP     0   801. 4736.             0.169\n10  2020 AAR CORP     0   781. 4123.             0.189\n# ℹ 24,284 more rows\n\n\nRemark how the select statement from before is chained together with the mutate statement through the pipe operator (%&gt;%). This operator pipes the results from the first statemetn to the second statement. The second statement implicitly uses the result from the first statement as the data it is going to work with.\nYou can use pipe statement to make your own descriptive statistics table. I created a table with some statistics for each firm. I first group the observations based on the gvkey and then summarise each group with the same key by defining a number of new variables.\n\ngroup_by(us_comp, gvkey) %&gt;%\n  summarise(N = n(), N_CEO = n_distinct(execid), \n            average = mean(salary), sd = sd(salary),\n            med = median(salary), minimum = min(salary), \n            maximum = max(salary)) %&gt;%\n  ungroup()\n\n# A tibble: 2,571 × 8\n   gvkey      N N_CEO average    sd   med minimum maximum\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 001004    12     2    862.  78.9  872.    750    1000 \n 2 001045    12     4    325. 395.   116.      0    1162.\n 3 001072     8     2    677. 219.   652.    450     967.\n 4 001075    12     2   1219. 111.  1222.   1091    1395 \n 5 001076    12     4    758. 119.   743.    567.    950 \n 6 001078    12     2   1788. 224.  1900    1298.   1973.\n 7 001094     8     3    563.  66.2  581.    437.    626.\n 8 001161    12     3    961. 151.  1000.    566.   1149.\n 9 001177     7     1   1049.  86.4 1000     977.   1200 \n10 001209    13     2   1247. 129.  1200     905.   1402.\n# ℹ 2,561 more rows\n\n\nRecent updates allow you to write the same code in a shorter manner.\n\nsummarise(us_comp, N = n(), N_CEO = n_distinct(execid), \n          average = mean(salary), sd = sd(salary),\n          med = median(salary), minimum = min(salary), \n          maximum = max(salary),\n          .by = gvkey) \n\n# A tibble: 2,571 × 8\n   gvkey      N N_CEO average    sd   med minimum maximum\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 001004    12     2    862.  78.9  872.    750    1000 \n 2 001045    12     4    325. 395.   116.      0    1162.\n 3 001072     8     2    677. 219.   652.    450     967.\n 4 001075    12     2   1219. 111.  1222.   1091    1395 \n 5 001076    12     4    758. 119.   743.    567.    950 \n 6 001078    12     2   1788. 224.  1900    1298.   1973.\n 7 001094     8     3    563.  66.2  581.    437.    626.\n 8 001161    12     3    961. 151.  1000.    566.   1149.\n 9 001177     7     1   1049.  86.4 1000     977.   1200 \n10 001209    13     2   1247. 129.  1200     905.   1402.\n# ℹ 2,561 more rows\n\n\n\n\n\nProgramming is hard work. You will make mistakes. You will get error messages. An important programming skill is to efficiently debug your code and find out what is going wrong.\n\nThe most important technique is trial and error. Change one thing in your code and see what the output is. Do you get an error message? What does the error message tell you? If you are carefull and do not change everything at once, this should at least help you to find out which part of the code does not work. This is one reason why scripts are so important. Because a script contains your entire analyis, you can always go back and (let your computer) redo the analysis from the start.\nIf you are not sure how you should use a certain function in R, you can read its help files in the Rstudio Help window. You can search for more information what a function is doing.\nR is a very popular language. If you have a problem it is likely that someone else had the same problem before you. If you Google the error message, there is a decent chance you will find a solution to your problem.\nA specific website where a lot of these questions are asked is StackOverflow. You can often directly search for your error on the website and find multiple solutions. You can improve your chances of finding a related answer to your problem by adding the [r], [tidyverse], [dplyr], [ggplot] tags to your error message.\n\n\n\n\nOne of the big advantages of the R world is that you can easily combine explanations with your analysis in .qmd files. The assignements can all be completed in Quarto. You can find a lot of good resources on the Quarto website. In short, markdown is a simple markup language 9 that lets you include R code.\n\n\nYou can have titles in markdown.\n# Title\n## Subtitle\n### Lower level titles.\nYou can emphasise some words.\n*Italic*, **bold**\nAdd links to pages and include pictures.\n[weblink](https://www.google.com)\n![pictures](https://rstudio.com/wp-content/uploads/2015/10/r-packages.png)\nYou can write enumerations and lists.\n1. Item 1\n2. Item 2 \n3. Item 3 \n\n- one\n- two \n- three \nYou can also write tables. You will rarely have to use tables. Typically, you can directly create the tables from R without the need to type in the results of your analysis.\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell\n\n\n\nThe largest advantage of Rmarkdown files is that we can include pieces of R code. 10 Code chuncks go between three backticks and we tell Rmarkdown that the language we are using is {r}. The example below creates a code chuck where we create a random vector x with then elements where the elements are drawn from a random distribution with mean 4 and standard deviation 3.\n```{r}\nx &lt;- rnorm(n = 10, mean = 4, sd = 3)\n```"
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#data-analysis-tools",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#data-analysis-tools",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "Our main tool for data analysis is the R statistical software. R is free software specifically aimed at statistical analysis and graphical representation. It is a fully developed programming language that allows to extend it with new packages with new capabilities. We are going to heavily rely on some of those packages.1\nWe are going to interact with the R software through Rstudio.2 Rstudio is an integrated development environment for R. It allows you to easily write, test, and run R code, and integrate the results with explanations. One advantage of R and Rstudio is that you can install them on as many computers as you want. You do not need a special license. This means that you do not have to be affiliated with the university to use the sofware.\nIn Rstudio, you write code and ask the R software to execute that code. You can save the different steps in your analysis in an R script which is just a plain text file with the .R-extenstion. The advantage of plain text file is that they are easy to share across different operating systems. The code in your script allows you and anyone else to run the same analysis at a later point in time which is very important to check for mistakes and for teaching. We will make use of a special type of plain text files: Quarto (.qmd) files. 3.\nQuarto is an extension of the markdown format. The idea of markdown is to write plain text documents which can later be exported to other formats such as html, pdf, or Word. The beauty of RMarkdown is that it lets you combine both R scripts and markdown into one document. This allows you to integrate your analysis and your description of your analysis in one document. When you go back to your analysis after a couple of weeks, you will be happy that you have more than the raw code to look at.\nFinally, I am going to introduce you to a specific dialect of the R language which is informally known as the tidyverse. If you want to be a good R programmer this might not be the best entry point into R. However, I assume that you want to quickly pick up some tools to facilitate your research project and for that purpose I believe the tidyverse will be excellent. I will introduce the most important bits and pieces throughout my lectures however I don’t have the time to go into detail. Now and then, you will have to experiment on your own to get your code working. Some excellent resources are the free online books R for datascience and Data Visualization: A practical introduction. You can also buy reasonably priced physical copies if you your are interested in developing your R skills further.\nFor larger projects, you should use a project in its own separate folder.4. Once you need multiple scripts, you want to make sure that you can reliably point to the results or functions in different scripts. Projects help you manage all the parts of a larger piece of work. You can start a You start a new project by clicking File &gt; New Project ... You can start a new file by clicking File &gt; New File &gt; R Script."
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#the-r-console",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#the-r-console",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "In Rstudio, you can use the R console directly. I mainly use the R console to test the code I have written in my scripts and I would advice you to do the same thing. If you want to understand how R works. You can experiment by typing the following lines in the console.5\n\nx &lt;- 1\nx\n\n[1] 1\n\nx &lt;- x + 1\nx\n\n[1] 2\n\n\nThe little code above already shows you two things.\n\nYou can assign (and overwrite) a numerical value to an object x\nThe right-hand side is assigned to the left-hand side. You can’t just switch them around.\n\nWe will see further that we can assign almost anything to an object x. When x is a vector or a data set with multiple elements, this will allow us to perform the same function on each element of x. That is the basic advantage of programming. You can tell your computer how to do a thing and than the computer can do it over and over for you."
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#r-packages",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#r-packages",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "install.packages(\"tidyverse\")\nlibrary(\"tidyverse\")\n\nR packages are additions to R that give R extra functionality. One of the selling points of R is that it has a good way of integrating this extra functionality and a lot of people are highly motivated to add this extra functionality. We will heavily rely on one meta-packages. tidyverse is a package that helps with data transformations and working with tabular data in a tidy fashion. One of the packages in the tidyverse is ggplot2 which provides tools to make pretty plots. The code above shows you how to install packages and use them. Normally, you will only have to install a package once. However, when you want to use a package in your script or code, you will have to load the package with the library() function. You only have to do load a package once at the start of your R session or at the start of your script. 6\ndplyr is another part of the tidyverse we will use extensively. With 5 verbs 7 and one pipe operator to glue them together helps you to explore the data.filter() let’s you filter out a subset of the observations. select() selects a subset of the variables. mutate() changes variables and creates new ones. group_by() and summarise() group subsets of observations and summarise them. %&gt;% is the pipe operator and joins together the verbs to create compound statements where you for instance filter a subset of the observations, create a new variable, create groups, and summarise the groups based on the average for the new variable."
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#useful-code",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#useful-code",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "Let’s run some code. First, we have to tell R to use the packages in the tidyverse.\n\nlibrary(tidyverse)\n\nNext, we have a look at the data of the compensation of CEOs of S&P500 companies. First, we have to assign the file with the data to an object, us_comp in this case. 8 The glimpse function gives an overview of the variables in the data, the type of the variables, and a couple of examples of the values for that variable.\n\nus_comp &lt;- readRDS(here(\"data\", \"us-compensation-new.RDS\"))\nglimpse(us_comp)\n\nRows: 24,294\nColumns: 22\n$ year                    &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018…\n$ gvkey                   &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004\", \"00100…\n$ cusip                   &lt;chr&gt; \"00036110\", \"00036110\", \"00036110\", \"00036110\"…\n$ exec_fullname           &lt;chr&gt; \"David P. Storch\", \"David P. Storch\", \"David P…\n$ coname                  &lt;chr&gt; \"AAR CORP\", \"AAR CORP\", \"AAR CORP\", \"AAR CORP\"…\n$ ceoann                  &lt;chr&gt; \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO\", \"CEO…\n$ execid                  &lt;chr&gt; \"09249\", \"09249\", \"09249\", \"09249\", \"09249\", \"…\n$ bonus                   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…\n$ salary                  &lt;dbl&gt; 867.000, 877.838, 906.449, 906.449, 755.250, 8…\n$ stock_awards_fv         &lt;dbl&gt; 2664.745, 619.200, 1342.704, 1695.200, 1150.50…\n$ stock_unvest_val        &lt;dbl&gt; 4227.273, 6018.000, 4244.165, 4103.283, 1334.0…\n$ eip_unearn_num          &lt;dbl&gt; 32.681, 56.681, 66.929, 83.415, 91.969, 157.96…\n$ eip_unearn_val          &lt;dbl&gt; 393.806, 1137.021, 1626.375, 2464.079, 2244.96…\n$ option_awards           &lt;dbl&gt; 578.460, 695.520, 1622.016, 0.000, 1150.500, 1…\n$ option_awards_blk_value &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ option_awards_num       &lt;dbl&gt; 49.022, 144.000, 158.400, 0.000, 153.810, 225.…\n$ tdc1                    &lt;dbl&gt; 5786.400, 4182.832, 5247.779, 5234.648, 4674.4…\n$ tdc2                    &lt;dbl&gt; 6105.117, 3487.312, 3809.626, 10428.375, 3523.…\n$ shrown_tot_pct          &lt;dbl&gt; 2.964, 2.893, 3.444, 3.877, 4.597, 5.417, 3.71…\n$ becameceo               &lt;date&gt; 1996-10-09, 1996-10-09, 1996-10-09, 1996-10-0…\n$ joined_co               &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ reason                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWe can also look at a subset of the observations, i.e. we filter the observations from the company that has the key 001045.\n\nfilter(us_comp, gvkey == \"001045\")\n\n# A tibble: 12 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2011 001045 02376R10 Gerard J. Arpey       AMERI… CEO    14591      0   614.\n 2  2012 001045 02376R10 Thomas W. Horton      AMERI… CEO    26059      0   618.\n 3  2013 001045 02376R10 Thomas W. Horton      AMERI… CEO    26059      0   592.\n 4  2014 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0   688.\n 5  2015 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0   232.\n 6  2016 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n 7  2017 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n 8  2018 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n 9  2019 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n10  2020 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n11  2021 001045 02376R10 William Douglas Park… AMERI… CEO    46191      0     0 \n12  2022 001045 02376R10 Robert D. Isom, Jr.   AMERI… CEO    46193      0  1162.\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;\n\n\nYou can see that the company, American Airlines, has had four CEOs since 2011. I used the gvkey to select a company and not its name. You will often use an id or a database key especially if you are working with multiple datasets and you want to link observations from one dataset to observations in the other dataset.\nWe can also filter data based on other variables. For instance, the below filters the observations from 2014 where the CEO had a salary over $1,000,000.\n\nfilter(us_comp, salary &gt; 1000, year == 2014)\n\n# A tibble: 494 × 22\n    year gvkey  cusip    exec_fullname         coname ceoann execid bonus salary\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  2014 001075 72348410 Donald E. Brandt, CPA PINNA… CEO    05835      0  1240 \n 2  2014 001078 00282410 Miles D. White, M.B.… ABBOT… CEO    14300      0  1973.\n 3  2014 001161 00790310 Rory P. Read          ADVAN… CEO    42390      0  1000.\n 4  2014 001300 43851610 David M. Cote         HONEY… CEO    20931   5500  1866.\n 5  2014 001380 42809H10 John B. Hess          HESS … CEO    02132      0  1500 \n 6  2014 001440 02553710 Nicholas K. Akins     AMERI… CEO    40778      0  1241.\n 7  2014 001447 02581610 Kenneth I. Chenault   AMERI… CEO    02157   4500  2000 \n 8  2014 001449 00105510 Daniel Paul Amos      AFLAC… CEO    00013      0  1441.\n 9  2014 001468 02637510 Jeffrey M. Weiss      AMERI… CEO    11710   3640  1012.\n10  2014 001487 02687478 Robert Herman Benmos… AMERI… CEO    20972      0  1385.\n# ℹ 484 more rows\n# ℹ 13 more variables: stock_awards_fv &lt;dbl&gt;, stock_unvest_val &lt;dbl&gt;,\n#   eip_unearn_num &lt;dbl&gt;, eip_unearn_val &lt;dbl&gt;, option_awards &lt;dbl&gt;,\n#   option_awards_blk_value &lt;dbl&gt;, option_awards_num &lt;dbl&gt;, tdc1 &lt;dbl&gt;,\n#   tdc2 &lt;dbl&gt;, shrown_tot_pct &lt;dbl&gt;, becameceo &lt;date&gt;, joined_co &lt;date&gt;,\n#   reason &lt;chr&gt;\n\n\nYou can also select some variables if you are only interested in those. If a variable has an undescriptive name, you can use select to rename the variable. For instance, tdc1 is the total compensation of a the CEO. A more descriptive name will help you to remember what the variable actually means.\n\nselect(us_comp, year, coname, bonus, salary, total = tdc1)\n\n# A tibble: 24,294 × 5\n    year coname   bonus salary total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  2011 AAR CORP     0   867  5786.\n 2  2012 AAR CORP     0   878. 4183.\n 3  2013 AAR CORP     0   906. 5248.\n 4  2014 AAR CORP     0   906. 5235.\n 5  2015 AAR CORP     0   755. 4674.\n 6  2016 AAR CORP     0   835  6073.\n 7  2017 AAR CORP     0   941  6284.\n 8  2018 AAR CORP     0   750  3344.\n 9  2019 AAR CORP     0   801. 4736.\n10  2020 AAR CORP     0   781. 4123.\n# ℹ 24,284 more rows\n\n\nYou can also create new variables with the mutate function. I created a variable that calculates what percentage of total compensation is the CEO’s salary.\n\nselect(us_comp, year, coname, bonus, salary, total = tdc1) %&gt;%\n  mutate(salary_percentage = salary / total)  \n\n# A tibble: 24,294 × 6\n    year coname   bonus salary total salary_percentage\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;\n 1  2011 AAR CORP     0   867  5786.             0.150\n 2  2012 AAR CORP     0   878. 4183.             0.210\n 3  2013 AAR CORP     0   906. 5248.             0.173\n 4  2014 AAR CORP     0   906. 5235.             0.173\n 5  2015 AAR CORP     0   755. 4674.             0.162\n 6  2016 AAR CORP     0   835  6073.             0.138\n 7  2017 AAR CORP     0   941  6284.             0.150\n 8  2018 AAR CORP     0   750  3344.             0.224\n 9  2019 AAR CORP     0   801. 4736.             0.169\n10  2020 AAR CORP     0   781. 4123.             0.189\n# ℹ 24,284 more rows\n\n\nRemark how the select statement from before is chained together with the mutate statement through the pipe operator (%&gt;%). This operator pipes the results from the first statemetn to the second statement. The second statement implicitly uses the result from the first statement as the data it is going to work with.\nYou can use pipe statement to make your own descriptive statistics table. I created a table with some statistics for each firm. I first group the observations based on the gvkey and then summarise each group with the same key by defining a number of new variables.\n\ngroup_by(us_comp, gvkey) %&gt;%\n  summarise(N = n(), N_CEO = n_distinct(execid), \n            average = mean(salary), sd = sd(salary),\n            med = median(salary), minimum = min(salary), \n            maximum = max(salary)) %&gt;%\n  ungroup()\n\n# A tibble: 2,571 × 8\n   gvkey      N N_CEO average    sd   med minimum maximum\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 001004    12     2    862.  78.9  872.    750    1000 \n 2 001045    12     4    325. 395.   116.      0    1162.\n 3 001072     8     2    677. 219.   652.    450     967.\n 4 001075    12     2   1219. 111.  1222.   1091    1395 \n 5 001076    12     4    758. 119.   743.    567.    950 \n 6 001078    12     2   1788. 224.  1900    1298.   1973.\n 7 001094     8     3    563.  66.2  581.    437.    626.\n 8 001161    12     3    961. 151.  1000.    566.   1149.\n 9 001177     7     1   1049.  86.4 1000     977.   1200 \n10 001209    13     2   1247. 129.  1200     905.   1402.\n# ℹ 2,561 more rows\n\n\nRecent updates allow you to write the same code in a shorter manner.\n\nsummarise(us_comp, N = n(), N_CEO = n_distinct(execid), \n          average = mean(salary), sd = sd(salary),\n          med = median(salary), minimum = min(salary), \n          maximum = max(salary),\n          .by = gvkey) \n\n# A tibble: 2,571 × 8\n   gvkey      N N_CEO average    sd   med minimum maximum\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 001004    12     2    862.  78.9  872.    750    1000 \n 2 001045    12     4    325. 395.   116.      0    1162.\n 3 001072     8     2    677. 219.   652.    450     967.\n 4 001075    12     2   1219. 111.  1222.   1091    1395 \n 5 001076    12     4    758. 119.   743.    567.    950 \n 6 001078    12     2   1788. 224.  1900    1298.   1973.\n 7 001094     8     3    563.  66.2  581.    437.    626.\n 8 001161    12     3    961. 151.  1000.    566.   1149.\n 9 001177     7     1   1049.  86.4 1000     977.   1200 \n10 001209    13     2   1247. 129.  1200     905.   1402.\n# ℹ 2,561 more rows"
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#looking-for-help",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#looking-for-help",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "Programming is hard work. You will make mistakes. You will get error messages. An important programming skill is to efficiently debug your code and find out what is going wrong.\n\nThe most important technique is trial and error. Change one thing in your code and see what the output is. Do you get an error message? What does the error message tell you? If you are carefull and do not change everything at once, this should at least help you to find out which part of the code does not work. This is one reason why scripts are so important. Because a script contains your entire analyis, you can always go back and (let your computer) redo the analysis from the start.\nIf you are not sure how you should use a certain function in R, you can read its help files in the Rstudio Help window. You can search for more information what a function is doing.\nR is a very popular language. If you have a problem it is likely that someone else had the same problem before you. If you Google the error message, there is a decent chance you will find a solution to your problem.\nA specific website where a lot of these questions are asked is StackOverflow. You can often directly search for your error on the website and find multiple solutions. You can improve your chances of finding a related answer to your problem by adding the [r], [tidyverse], [dplyr], [ggplot] tags to your error message."
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#quarto-markdown-r-a-bunch-of-other-programming-languages",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#quarto-markdown-r-a-bunch-of-other-programming-languages",
    "title": "Rstudio installation and first code",
    "section": "",
    "text": "One of the big advantages of the R world is that you can easily combine explanations with your analysis in .qmd files. The assignements can all be completed in Quarto. You can find a lot of good resources on the Quarto website. In short, markdown is a simple markup language 9 that lets you include R code.\n\n\nYou can have titles in markdown.\n# Title\n## Subtitle\n### Lower level titles.\nYou can emphasise some words.\n*Italic*, **bold**\nAdd links to pages and include pictures.\n[weblink](https://www.google.com)\n![pictures](https://rstudio.com/wp-content/uploads/2015/10/r-packages.png)\nYou can write enumerations and lists.\n1. Item 1\n2. Item 2 \n3. Item 3 \n\n- one\n- two \n- three \nYou can also write tables. You will rarely have to use tables. Typically, you can directly create the tables from R without the need to type in the results of your analysis.\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell\n\n\n\nThe largest advantage of Rmarkdown files is that we can include pieces of R code. 10 Code chuncks go between three backticks and we tell Rmarkdown that the language we are using is {r}. The example below creates a code chuck where we create a random vector x with then elements where the elements are drawn from a random distribution with mean 4 and standard deviation 3.\n```{r}\nx &lt;- rnorm(n = 10, mean = 4, sd = 3)\n```"
  },
  {
    "objectID": "auxilary/introduction_to_r_rstudio_tidyverse.html#footnotes",
    "href": "auxilary/introduction_to_r_rstudio_tidyverse.html#footnotes",
    "title": "Rstudio installation and first code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can download R from this CRAN server↩︎\nYou can download from the Rstudio website↩︎\nThese notes and slides are enteriely written in Quarto files↩︎\nSee the Project Chapter of Hadley Wickham’s book↩︎\nIt is tempting to copy and paste the code but retyping the code is a surprising powerful way of learning to code. I would advise you to try to type as much of the code as possible when you are trying to figure out how R works.↩︎\nInstalling packages is another reason to use the R console.↩︎\ni.e. functions to do something with a dataset.↩︎\nThe data is available on LMS or through the code in the detailed introduction file↩︎\nThis means that you can add little bits of code to your text that indicate how the text should look↩︎\n Again, all these notes are written in Rmarkdown.↩︎"
  },
  {
    "objectID": "slides/slides1.html#how-to-do-empirical-research",
    "href": "slides/slides1.html#how-to-do-empirical-research",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "How to do empirical research",
    "text": "How to do empirical research\n\nThe connection between theory and the observed data\nThe connection between practical knowledge and what you can investigate\nThe appropriate statistical tests and code\n\n\nEmpirical just means that some data in the broadest possible be sense will be collected or generated. The emphasis on the units will be on the practical and statistical issues of the data analysis part of a thesis. The influence will not be on the accounting and finance part of your thesis. The goal is to be relevant to everyone in the unit. This would be a good time to figure out whether students do a study with a more qualitative approach."
  },
  {
    "objectID": "slides/slides1.html#different-modules",
    "href": "slides/slides1.html#different-modules",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Different Modules",
    "text": "Different Modules\n\nThe research process and basic data skills in R (week 1 - 3)\nResearch design (week 4 - 6)\nData Management and Event Study (week 7 - 8)\nSpecial Topics (9 - 12)\n\nMachine Learning\nDiscrete Dependent Variables\nGenerated Variables\n\n\n\nAll modules work as stand-alone units and aim to cover a wide range of topics. Not all of them will in the end be relevant for everyone. However, it is probably a good idea to get to know the different statistical methods, their advantages, and disadvantages. Bringing a new methodology to an old topic can be a valuable contribution. Some problems have already been solved in other research streams."
  },
  {
    "objectID": "slides/slides1.html#assessment",
    "href": "slides/slides1.html#assessment",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Assessment",
    "text": "Assessment\n\nNo Exam\n(Almost weekly) Assignments (70%)\n\n\n- Homework: Practical Issues (0%) (1 March)\n- Assignment 1: Theory and Regressions (10%) (8 March)\n- Assignment 2: Regression and control variables (10%) (17 March)\n- Assignment 3: Research Design (10%) (12 April)\n- Assignment 4: Event Study (10%) (26 April)\n- Assignment 5: Machine Learning (10%) (3 May)\n- Assignment 6: Simulation Research Design (20%) (24 May)\n\n\nProposal and presentation (30%)\n\n\n- Pitch (10%) (29 March)\n- Proposal (10%) (12 May)\n- Presentation (10%) (Probably Thursday 18 July)\n\n\nWe want you to (1) do some data analysis and (2) be well prepared to undertake (the data analysis part of) a research project. So we are going to evaluate you by letting you (1) analyse data and (2) prepare your honours thesis."
  },
  {
    "objectID": "slides/slides1.html#the-first-two-weeks",
    "href": "slides/slides1.html#the-first-two-weeks",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "The first two weeks",
    "text": "The first two weeks\nCEO compensation\n\nThis is not my area of expertise!\nI am not a specialist in the topic nor in this type of data analysis. CEO compensation is something that people in finance, accounting, economics, and outside of academia are interested in. The topic is probably the one with the most commonality. I am comforable with these type of economic theories and I am going to stress the role of theory in data analysis a lot. Some of you will have a topic that is at first sight less theory driven or rely more strongly on very specific knowledge about your setting. I am going to try to convince you that it is going to be useful to think about the underlying story that you are testing."
  },
  {
    "objectID": "slides/slides1.html#topic",
    "href": "slides/slides1.html#topic",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Topic",
    "text": "Topic\nCompletely based on Edmans and Gabaix (2016) in Journal of Economic Literature.\n\nThe level of CEO compensation\nCEO incentives\n\n\nI am going to focus on two topics. 1. How high can we expect the total compensation of a CEO to be (compared to other CEOs) based on some simple economic assumptions. Too high CEO compensation is sometimes seen as a signal of bad corporate goverance. To measure what ‘too high’ means, we first need to establish a baseline of normal levels of compensation. 2. How should CEOs be incentivised: equity or options? How schould we measure whether CEOs have appropriate incentives: $ for $ increases, % for % increases? Incentives are a big topic in Accounting and Finance."
  },
  {
    "objectID": "slides/slides1.html#firm-production-function",
    "href": "slides/slides1.html#firm-production-function",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Firm production function",
    "text": "Firm production function\n\\[\nV = T^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n                 \\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\]\n\\[\n\\alpha_T + \\alpha_K + \\alpha_L = 1\n\\]\n\n\\(V =\\) The value of the firm\n\\(K =\\) Capital of the firm\n\\(L =\\) Labour of the firm\n\\(T =\\) CEO talent/skills/ability/experience\n\n\nWe assume that there is nothing in the structure of the production function that favours a particular firm size, i.e. constant returns to scale."
  },
  {
    "objectID": "slides/slides1.html#ceo-decision",
    "href": "slides/slides1.html#ceo-decision",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "CEO decision",
    "text": "CEO decision\n\\[\n\\max_{K, L} W_T = V - w_L L - rK\n\\]\n\n\\(W_T =\\) wage for CEO with talent T\n\\(w_L =\\) labour unit costs\n\\(r =\\) cost of capital (or return on capital for investors)\n\n\nThe CEO maximises their income \\(W_T\\) by attracting capital at a cost, \\(r\\), and and hiring labour at a wage, \\(w_L\\). The model assumes that the CEO takes the ultimate decision. As it turns out when you assume competitive labour and financial markets, that assumptions does not really matter a lot.\nThis model is too simple to capture reality perfectly. However, that is not the goal of the model and of this exercise. The idea is to see whether we can find a reasonable baseline for CEO compensation that we can test against the data."
  },
  {
    "objectID": "slides/slides1.html#relation-between-size-and-ceo-wage",
    "href": "slides/slides1.html#relation-between-size-and-ceo-wage",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Relation between size and CEO wage",
    "text": "Relation between size and CEO wage\n\\[\nW_T = \\alpha_T V\n\\]\n\nIn this model, the driving force is that more talented CEOs grow the business to a bigger size and they earn more money when they create more value.\n\n\nFirst find the optimal level of capital …\n\\[\n\\begin{aligned}\n\\frac{\\partial W_T}{\\partial K} &=  T^{\\alpha_T}  \n\\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K - 1}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L} - r = 0\n\\\\\nT^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\frac{\\alpha_K}{K}\n&= r\n\\\\\n\\frac{V}{r} &= \\frac{K}{\\alpha_K}\n\\end{aligned}\n\\]\n… and labour\n\\[\n\\begin{aligned}\n\\frac{\\partial W_T}{\\partial K} &=  T^{\\alpha_T}  \n\\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L - 1} - w_L\n\\\\\nT^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\frac{\\alpha_L}{L} &= w_L\n\\\\\n\\frac{V}{w_L} &= \\frac{L}{\\alpha_L}\n\\end{aligned}\n\\]\nNow we can plugin \\(L\\) and \\(K\\) in \\(V\\) …\n\\[\n\\begin{align}\nV = T^{\\alpha_T} \\Bigl( \\frac{V}{r} \\Bigl) ^{\\alpha_K}\n\\Bigl( \\frac{V}{w_L} \\Bigl) ^{\\alpha_L}\n\\\\\nV^{1 - \\alpha_K - \\alpha_L} = \\frac{T^{\\alpha_T}}\n{r^{\\alpha_K} w_L^{\\alpha_L}}\n\\\\\nV^{\\alpha_T} = \\frac{T^{\\alpha_T}}\n{r^{\\alpha_K} w_L^{\\alpha_L}}\n\\\\\nV = \\frac{T}\n{r^{\\frac{\\alpha_K}{\\alpha_T}} w_L^{\\frac{\\alpha_L}{\\alpha_T}}}\n\\end{align}\n\\]\n… and in \\(W_T\\).\n\\[\n\\begin{align}\nW_T = V - V \\alpha_K - V\\alpha_L = (1 - \\alpha_K - \\alpha_L) V\n= \\alpha_T V\n\\end{align}\n\\]\nI like the basic intuition and deriviation of the model. The derivation is straightforward and (some of) the implicit assumptions are relatively easy to accept. The effect of the CEO depends on the size of the firm (\\(V\\)). When there is more capital and labour available a more talented CEO will have a bigger impact. The model also predicts a clear quantitative relationship between firm size, \\(V\\), and CEO compensation, \\(W_T\\), i.e. that relationship should be linear. This is a nice result that we can test with data. In contrast to the linear relationship between firm size and CEO talent. We can measure \\(V\\) but not \\(T\\)."
  },
  {
    "objectID": "slides/slides1.html#data-compensation-value",
    "href": "slides/slides1.html#data-compensation-value",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Data: Compensation-Value",
    "text": "Data: Compensation-Value\n\n\nThe data is downloaded from Compustat and Execucomp. A lot of you will use these are similar databases in your research project. I did not clean or check the data for this exercise. In your own project, you should show a better understanding of how the data are gathered and what they include than what I am displaying here.\n\nCEO compensation is fairly complete. It includes changes in the value of equity and options.\nMarket value also includes all outstanding financial instruments on the company.\n\nThe qualitative relationship holds quite well. Bigger companies have CEOs with higher compensation. However, the relationship is far from linear and looks more like a power function. Clearly there are other effects at play. In this sample, the power coefficient is 0.31. Prior studies have found a coefficient more closely to 0.33 (Baker, Jensen, and Murphy 1988). Remember that in our setup the CEO can grow the firm at will by attracting more capital and more labour. That assumption is probably too strong."
  },
  {
    "objectID": "slides/slides1.html#the-research-process",
    "href": "slides/slides1.html#the-research-process",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "The research process",
    "text": "The research process\n\n\n\n\n\n\nSummary\n\n\n\nMake assumptions\nDerive relationship between measurable quantities\nCompare the theory and the data\n\n\n\n\n\n\nNote what we have just done. We started with some assumptions about the production function of a company and competitive markets to find the theoretical relation between firm size and CEO compensation. We followed up by testing this theory to data from S&P500 firms. These are the steps that you should be following."
  },
  {
    "objectID": "slides/slides1.html#literature-search",
    "href": "slides/slides1.html#literature-search",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Literature search",
    "text": "Literature search\n\nGoogle Scholar\nUWA One Search\nWeb of Knowledge\nEBSCOhost Research Databases\nSocial Science Research Network\nNational Bureau of Economic Research\n\n\nIn the CEO compensation case above, we derived the theoretical prediction. Normally, you will build on prior theoretical and empirical research to build predictions. The\nIn most cases (ssrn is the exception), you will have to be on the university’s network if you want to actually read the full paper.\n\nGoogle Scholar is probably the most comprehensive repository. This search engine work very similar to regular Google search. There are some additional tricks you can use “author:lastname-firstname” will help you to narrow down papers from a specific author. “intitle:keyword” let’s you search for keywords in the title of papers. You can also narrow down your search based on year of publication. The advanced search features hidden in the left side bar give you additional options such as searching for certain journals. If you are on the university network, Google Scholar will tell you for every paper\nOnesearch is the university search engine. It’s the best way to figure out whether there is an easily accessible version of the paper.\nWebofknowledge and EBSCOhost are two publisher driven initiatives. They work pretty well. Each with their own quirks.\nSSRN (Social Science Research Network) and NBER (National Bureau of Economic Research) both provide access to their own not-yet-peer-reviewed paper repositories. Here you go to find cutting edge research."
  },
  {
    "objectID": "slides/slides1.html#start-of-literature-search",
    "href": "slides/slides1.html#start-of-literature-search",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Start of literature search",
    "text": "Start of literature search\n\nDon’t start too narrow!\nReview articles and journals\n\nJournal of Economic Literature\nJournal of Accounting Literature\n\nA*/A journals\n\nAccounting\nFinance\n\n\n\nMy favourite way to start a research project now is to find one or two excellent overview or review papers. A (systematic) review paper provides a state of a research field and identifies interesting new research questions. I am not sure whether my strategy will work for you. I find that a good review paper gives a good list of papers you can build on and they often already compare the most important papers in a field. The trick is to be not too picky. You probably will not find a review for your exact reserch problem but it is unlikely that you will not find a partly relevant overview paper. You can search for review papers by adding “intitle:review” or “intitle:overview” to your Google Scholar search.\nTo find other papers relevant to your topic, you can build on the review paper by (1) looking up the papers referred to in the review paper and (2) search for papers that cite the review paper. You can do the latter via Google Scholar and Webofknowledge.\nTo find good reviews, I think you should start your search in the better journals. Some journals are dedicated to these literature reviews for instance Journal of Economic Literature and Journal of Accounting Literature. I am not aware of a similar journal in finance but I will happily add it if you let me know.\nWhen you start your literature search, you don’t want to narrow. You are not going to find an overview paper about “CEO compensation in Australian mining companies after the GFC”. However, you can start with an overview paper about CEO compensation. Like the one I found: “Executive Compensation: A Modern Primer” by Alex Edmans and Xavier Gabraix in Journal of Economic Literature."
  },
  {
    "objectID": "slides/slides1.html#signs-of-bad-workload-management",
    "href": "slides/slides1.html#signs-of-bad-workload-management",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Signs of bad workload management",
    "text": "Signs of bad workload management\n\nIrregular sleeping habits\nLoss of motivation\nPostponing difficult tasks"
  },
  {
    "objectID": "slides/slides1.html#working-with-a-supervisor",
    "href": "slides/slides1.html#working-with-a-supervisor",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Working with a supervisor",
    "text": "Working with a supervisor\n\nThe role of the supervisor\n\n\nGuide you towards a feasible research project\nHelp you finish the dissertation\n\n\nWork process\n\n\nSchedule weekly or fortnightly meetings\nSubmit writing or data analysis before every meeting.\n\n\nAdd a tl;dr section.\n\n\nYour supervisor is not your copy-editor, let them know when you submit an “early” draft.\nTell your supervisor what has changed\nClarify the sample and the main variables in tables\nTell your supervisor what the main table or figure is"
  },
  {
    "objectID": "slides/slides1.html#managing-the-workload-40-hours-per-week",
    "href": "slides/slides1.html#managing-the-workload-40-hours-per-week",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Managing the workload (40 hours per week)",
    "text": "Managing the workload (40 hours per week)\n\nPlan ahead (with your supervisor) towards major deadlines\nIt’s okay to submit partial assignments, as long as you make progress. (Especially for programming exercises)\nKeep writing!\nReach out when you need help with planning or when you feel overwhelmed.\n\nstijn.masschelein@uwa.edu.au\nUWA Counselling services\n\n\n\nThe plan can be an excel sheet with deadlines and milestones. Breaking down 5000 words into 10 weeks of 500 words is a lot less daunting."
  },
  {
    "objectID": "slides/slides1.html#questions",
    "href": "slides/slides1.html#questions",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Questions",
    "text": "Questions\n\nAnswer in Quarto (.qmd) format. File &gt; New File &gt; Quarto Document ... &gt;\nYou can use the code examples that I used in the video. I have uploaded the file to LMS. Use a different level 2 header for each question. Use R chunks to\n\nLoad the CEO compensation data from LMS so that you can work with it.\nPrint the dataset with only the CEOs without a cash bonus in 2013. You do not need to print the whole dataset. The default number of lines is sufficient.\nCalculate the number of observations, and the average and median bonus per year for the entire dataset.\n\nClick the Render button and upload the qmd and html version to LMS.\n\n\nThere is going to be some trial-and-error and debugging. That is fine. Carefully read the errors you get and use the resouces for help. Don’t be afraid to ask me or each other for help.\n\nGive a name to your document and enter your name\nSee the examples\nSee the render button in RStudio"
  },
  {
    "objectID": "slides/slides3.html#an-example-of-a-causal-graph",
    "href": "slides/slides3.html#an-example-of-a-causal-graph",
    "title": "Control Variables",
    "section": "An Example of a Causal Graph",
    "text": "An Example of a Causal Graph\n\n\n\n\n\n\n\nbrand_capital\n\n  \n\nInfoEnvironment\n\n InfoEnvironment   \n\nCreditRating\n\n CreditRating   \n\nInfoEnvironment-&gt;CreditRating\n\n    \n\nFutureCashFlow\n\n FutureCashFlow   \n\nFutureCashFlow-&gt;CreditRating\n\n    \n\nBrandCapital\n\n BrandCapital   \n\nBrandCapital-&gt;InfoEnvironment\n\n    \n\nBrandCapital-&gt;FutureCashFlow"
  },
  {
    "objectID": "slides/slides3.html#difference-with-equilibrium-models",
    "href": "slides/slides3.html#difference-with-equilibrium-models",
    "title": "Control Variables",
    "section": "Difference with equilibrium models",
    "text": "Difference with equilibrium models\n\n\n\n\n\n\nDifferences\n\n\n\nAll the qualitative information about causal relations is in the graph.\nThe equilibrium model directly gives the relation between the variables of interest.\n\ne.g.: Signaling model\n\n\n\n\n\n\n\n\n\n\ndonations\n\n  \n\nPerformance\n\n Performance   \n\nDonation\n\n Donation   \n\nPerformance-&gt;Donation\n\n    \n\nReturn\n\n Return   \n\nDonation-&gt;Return\n\n   \n\n\n\n\n\n\n\nIs there a relation between Performance and Return?\nThere is a parallel with voluntary disclosure of information that the company does not have.\nWhat happens if firms are no longer allowed to donate? What happens if firms are all forced to donate?"
  },
  {
    "objectID": "slides/slides3.html#assignment-csr-report",
    "href": "slides/slides3.html#assignment-csr-report",
    "title": "Control Variables",
    "section": "Assignment: CSR report",
    "text": "Assignment: CSR report\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nPerformance\n\n Performance   \n\nCSR_report\n\n CSR_report   \n\nPerformance-&gt;CSR_report\n\n    \n\nScandals\n\n Scandals   \n\nPerformance-&gt;Scandals\n\n    \n\nReturn\n\n Return   \n\nCSR_report-&gt;Return\n\n    \n\nObserved_Report\n\n Observed_Report   \n\nCSR_report-&gt;Observed_Report\n\n   \n\n\n\n\n\n\n\nWhat is the effect of an increase in scandals?\nWhat happens if we keep the number of scandals constant?"
  },
  {
    "objectID": "slides/slides3.html#causal-graph",
    "href": "slides/slides3.html#causal-graph",
    "title": "Control Variables",
    "section": "Causal Graph",
    "text": "Causal Graph\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nTechIndustry\n\n TechIndustry   \n\nTechIndustry-&gt;Performance\n\n   \n\n\n\n\n\n\nAssume that we are interested in the role of the GC on performance."
  },
  {
    "objectID": "slides/slides3.html#simulation",
    "href": "slides/slides3.html#simulation",
    "title": "Control Variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nset.seed(230383)\nN &lt;- 1000\nds &lt;- tibble(CG = runif(N, 0, 10),\n             TI = rbinom(N, 1, .25)) %&gt;%\n  mutate(Performance =\n           rnorm(N, CG * .15 + TI * 10, 5))\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nlm2 &lt;- lm(Performance ~ CG + TI, data = ds)\n\n\n\ngof_omit &lt;- \"Adj|IC|Log|Pseudo|RMSE\"\nstars &lt;- c('*' = .1, '**' = .05, '***' = .01)\nmsummary(list(lm1, lm2), stars = stars,\n         gof_omit = gof_omit, output = \"markdown\")\n\n\n\n\n\n(1)\n(2)\n\n\n\n\n(Intercept)\n2.976***\n0.036\n\n\n\n(0.408)\n(0.310)\n\n\nCG\n0.081\n0.130**\n\n\n\n(0.073)\n(0.052)\n\n\nTI\n\n10.433***\n\n\n\n\n(0.344)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.001\n0.480\n\n\n\nNote: ^^ * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\nMeasurement error typically decreases the effect and this is also what happened in the assignment. For instance, the difference between the donation and no donation should be 3 but it is less."
  },
  {
    "objectID": "slides/slides3.html#causal-graph-1",
    "href": "slides/slides3.html#causal-graph-1",
    "title": "Control Variables",
    "section": "Causal Graph",
    "text": "Causal Graph\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nTechIndustry\n\n TechIndustry   \n\nTechIndustry-&gt;CorporateGovernance\n\n    \n\nTechIndustry-&gt;Performance\n\n   \n\n\n\n\n\n\nThere is only 1 difference between this causal graph and the previous one."
  },
  {
    "objectID": "slides/slides3.html#simulation-1",
    "href": "slides/slides3.html#simulation-1",
    "title": "Control Variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nN &lt;- 1000\nds &lt;- tibble(TI = rbinom(N, 1, .25)) %&gt;%\n  mutate(CG = rnorm(N, .5 - TI, .2),\n         Performance = rnorm(N, TI + 0 * CG, 1))\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nlm2 &lt;- lm(Performance ~ CG + TI, data = ds)\n\n\n\nmsummary(list(lm1, lm2), stars = stars,\n         gof_omit = gof_omit, output = \"markdown\")\n\n\n\n\n\n(1)\n(2)\n\n\n\n\n(Intercept)\n0.473***\n0.002\n\n\n\n(0.036)\n(0.087)\n\n\nCG\n-0.945***\n-0.090\n\n\n\n(0.067)\n(0.159)\n\n\nTI\n\n1.018***\n\n\n\n\n(0.172)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.165\n0.193\n\n\n\nNote: ^^ * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\nIn the simulation, we set the effect of GC equal to 0 i.e. there is not effect. The reason to do that is to show why it’s necessary to adjust for TI."
  },
  {
    "objectID": "slides/slides3.html#fixed-effects-as-a-special-case",
    "href": "slides/slides3.html#fixed-effects-as-a-special-case",
    "title": "Control Variables",
    "section": "Fixed effects as a special case",
    "text": "Fixed effects as a special case\n\n\n\n\n\n\nDefinition\n\n\nEffects that are the same for every industry, year, firm, or individual can be adjusted for by using fixed effects.\n\n\n\n\n\n\n\n\n\nBenefits\n\n\nWe do not need to measure the specific variables and can just use indicators variables for each category (e.g. for each different industry).\n\n\n\nSee more in chapter 16 of Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides3.html#fixed-effects-for-industry",
    "href": "slides/slides3.html#fixed-effects-for-industry",
    "title": "Control Variables",
    "section": "Fixed effects (for industry)",
    "text": "Fixed effects (for industry)\n\nCausal DiagramSimulationRegressionsSimulation with correlated fixed effectsRegressions with correlated fixed effects\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nIndustry\n\n Industry   \n\nIndustry-&gt;CorporateGovernance\n\n    \n\nIndustry-&gt;Performance\n\n   \n\n\n\n\n\n\n\n\nNind &lt;- 20\nN &lt;- 5000\ndi &lt;- tibble(\n  ind_number = 1:Nind,\n  ind_CG = rnorm(Nind, 0, 1),\n  ind_performance = rnorm(Nind, 0, 1)\n)\nds &lt;- tibble(\n    ind_number = sample(1:Nind, N, replace = TRUE)) %&gt;%\n  left_join(\n    di, by = \"ind_number\") %&gt;%\n  mutate(\n    CG = rnorm(N, .5 + ind_CG, .2),\n    Performance = rnorm(N, 0 * CG + ind_performance, 1)\n  )\n\n\n\n\nglimpse(di, width = 50)\n\nRows: 20\nColumns: 3\n$ ind_number      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, …\n$ ind_CG          &lt;dbl&gt; 0.23567083, -0.34180999,…\n$ ind_performance &lt;dbl&gt; 1.1335103, 1.2873377, 0.…\n\n\n\n\nglimpse(ds, width = 50)\n\nRows: 5,000\nColumns: 5\n$ ind_number      &lt;int&gt; 9, 12, 5, 7, 6, 8, 15, 1…\n$ ind_CG          &lt;dbl&gt; 1.91243572, 0.16031769, …\n$ ind_performance &lt;dbl&gt; 0.1773941, -0.1250858, 0…\n$ CG              &lt;dbl&gt; 2.3076069, 0.6604549, 1.…\n$ Performance     &lt;dbl&gt; 0.09219279, -0.37244401,…\n\n\n\n\n\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nlm2 &lt;- lm(Performance ~ CG + factor(ind_number), data = ds)\nlibrary(fixest)\nfe &lt;- feols(Performance ~ CG | ind_number, data = ds)\n\n\nmsummary(list(lm1, lm2, fe), gof_omit = gof_omit, stars = stars, output = \"markdown\")\n\n\n\n\n\n(1)\n(2)\n(3)\n\n\n\n\n(Intercept)\n0.490***\n1.054***\n\n\n\n\n(0.019)\n(0.080)\n\n\n\nCG\n-0.076***\n0.030\n0.030\n\n\n\n(0.018)\n(0.071)\n(0.061)\n\n\nfactor(ind_number)2\n\n0.207**\n\n\n\n\n\n(0.097)\n\n\n\nfactor(ind_number)3\n\n-0.490***\n\n\n\n\n\n(0.090)\n\n\n\nfactor(ind_number)4\n\n-0.372**\n\n\n\n\n\n(0.161)\n\n\n\nfactor(ind_number)5\n\n-0.410***\n\n\n\n\n\n(0.099)\n\n\n\nfactor(ind_number)6\n\n-1.907***\n\n\n\n\n\n(0.169)\n\n\n\nfactor(ind_number)7\n\n-0.083\n\n\n\n\n\n(0.134)\n\n\n\nfactor(ind_number)8\n\n-1.191***\n\n\n\n\n\n(0.178)\n\n\n\nfactor(ind_number)9\n\n-0.935***\n\n\n\n\n\n(0.148)\n\n\n\nfactor(ind_number)10\n\n-1.712***\n\n\n\n\n\n(0.087)\n\n\n\nfactor(ind_number)11\n\n-1.162***\n\n\n\n\n\n(0.122)\n\n\n\nfactor(ind_number)12\n\n-1.118***\n\n\n\n\n\n(0.090)\n\n\n\nfactor(ind_number)13\n\n0.765***\n\n\n\n\n\n(0.091)\n\n\n\nfactor(ind_number)14\n\n-0.564***\n\n\n\n\n\n(0.104)\n\n\n\nfactor(ind_number)15\n\n0.730***\n\n\n\n\n\n(0.129)\n\n\n\nfactor(ind_number)16\n\n0.689***\n\n\n\n\n\n(0.158)\n\n\n\nfactor(ind_number)17\n\n-2.056***\n\n\n\n\n\n(0.098)\n\n\n\nfactor(ind_number)18\n\n-0.503***\n\n\n\n\n\n(0.091)\n\n\n\nfactor(ind_number)19\n\n-1.907***\n\n\n\n\n\n(0.093)\n\n\n\nfactor(ind_number)20\n\n0.608***\n\n\n\n\n\n(0.086)\n\n\n\nNum.Obs.\n5000\n5000\n5000\n\n\nR2\n0.003\n0.443\n0.443\n\n\nR2 Within\n\n\n0.000\n\n\nStd.Errors\n\n\nby: ind_number\n\n\nFE: ind_number\n\n\nX\n\n\n\nNote: ^^ * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\nWhy do we need the factor(ind_number) formulation?\n\n\n\n\n\nNind &lt;- 20\nN &lt;- 5000\ncorrel &lt;- -0.5\ndi &lt;- tibble(\n    ind_number = 1:Nind,\n    ind_CG = rnorm(Nind, 0, 1)) %&gt;%\n  mutate(\n    ind_performance = sqrt(1 - correl^2) * rnorm(Nind, 0, 1) + correl * ind_CG)\nds &lt;- tibble(\n    ind_number = sample(1:Nind, N, replace = TRUE)) %&gt;%\n  left_join(\n    di, by = \"ind_number\") %&gt;%\n  mutate(\n    CG = rnorm(N, .5 + ind_CG, .2),\n    Performance = rnorm(N, 0 * CG + ind_performance, 1)\n  )\n\n\n\n\nglimpse(di, width = 50)\n\nRows: 20\nColumns: 3\n$ ind_number      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, …\n$ ind_CG          &lt;dbl&gt; -0.82999044, 0.44908313,…\n$ ind_performance &lt;dbl&gt; -1.12284290, -0.57326559…\n\n\n\n\nglimpse(ds, width = 50)\n\nRows: 5,000\nColumns: 5\n$ ind_number      &lt;int&gt; 20, 17, 2, 1, 9, 20, 5, …\n$ ind_CG          &lt;dbl&gt; -1.1358960, 0.4833522, 0…\n$ ind_performance &lt;dbl&gt; 1.0252155, -0.6131181, -…\n$ CG              &lt;dbl&gt; -0.79273388, 1.44367931,…\n$ Performance     &lt;dbl&gt; 1.75927497, -1.39745179,…\n\n\n\n\n\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nfe &lt;- feols(Performance ~ CG | ind_number, data = ds)\n\n\nmsummary(list(lm1, fe), gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n−0.033\n\n\n\n\n(0.022)\n\n\n\nCG\n−0.279***\n−0.012\n\n\n\n(0.015)\n(0.046)\n\n\nNum.Obs.\n5000\n5000\n\n\nR2\n0.067\n0.321\n\n\nR2 Within\n\n0.000\n\n\nRMSE\n1.17\n1.00\n\n\nStd.Errors\n\nby: ind_number\n\n\nFE: ind_number\n\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the factor(ind_number) formulation?\nThe trick with the correlated ind_CG and ind_performance"
  },
  {
    "objectID": "slides/slides3.html#what-do-fixed-effects-do",
    "href": "slides/slides3.html#what-do-fixed-effects-do",
    "title": "Control Variables",
    "section": "What do fixed effects do?",
    "text": "What do fixed effects do?\n\nFull SampleHighlight IndustryRemove Industry Effects\n\n\n\n\nCode\nfe_plot &lt;-\n  ggplot(ds, aes(y = Performance, x = CG)) +\n  geom_point()\nplot(fe_plot)\n\n\n\n\n\n\n\n\n\nCode\nfe_colour &lt;-\n  ggplot(ds, aes(y = Performance, x = CG,\n                colour = factor(ind_number))) +\n  geom_point() + theme(legend.position=\"none\") \nplot(fe_colour)\n\n\n\n\n\n\n\n\n\nCode\nfe_demean &lt;- group_by(ds, ind_number) %&gt;%\n  mutate(Performance2 = Performance - mean(Performance),\n         CG2 = CG - mean(CG)) %&gt;%\n  ggplot(aes(y = Performance2, x = CG2,\n             colour = factor(ind_number))) +\n  geom_point() + theme(legend.position=\"none\") \nplot(fe_demean)"
  },
  {
    "objectID": "slides/slides3.html#speedboat-racing-example-booth2017",
    "href": "slides/slides3.html#speedboat-racing-example-booth2017",
    "title": "Control Variables",
    "section": "Speedboat Racing Example (Booth and Yamamura 2017)",
    "text": "Speedboat Racing Example (Booth and Yamamura 2017)\n\n\n\nMixed-sex and single-sex races determined by lottery (Randomisation)\n7 race courses\nMultiple races in the same month and location\n\n\n\n\n\n\n\n\n\nspeedboat\n\n  \n\nave_ability\n\n ave_ability   \n\nltime\n\n ltime   \n\nave_ability-&gt;ltime\n\n    \n\nmixed_race\n\n mixed_race   \n\nmixed_race-&gt;ltime\n\n    \n\nfemale\n\n female   \n\nfemale-&gt;ave_ability\n\n    \n\nfemale-&gt;ltime\n\n    \n\ncourse\n\n course   \n\ncircumstances\n\n circumstances   \n\ncourse-&gt;circumstances\n\n    \n\nmonth_location\n\n month_location   \n\nmonth_location-&gt;circumstances\n\n    \n\ncircumstances-&gt;ltime\n\n    \n\ncircumstances-&gt;female"
  },
  {
    "objectID": "slides/slides3.html#results-of-speedboat-races",
    "href": "slides/slides3.html#results-of-speedboat-races",
    "title": "Control Variables",
    "section": "Results of Speedboat Races",
    "text": "Results of Speedboat Races\n\n\nCode\nload(here(\"data\", \"booth_yamamura.Rdata\"))\ntable &lt;- as_tibble(table) %&gt;%\n  select(p_id, women_dat, time, ltime, mix_ra, course,\n         race_id, yrmt_locid)\ntable_clean &lt;- filter(table, complete.cases(table)) %&gt;%\n  select(ltime, women_dat, mix_ra, course, p_id, race_id,\n         yrmt_locid)\nltime_reg &lt;- feols(ltime ~ women_dat : mix_ra + mix_ra\n                   | course + p_id + yrmt_locid,\n                   cluster = \"race_id\",\n                   data = table_clean)\nmsummary(ltime_reg, gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n\n (1)\n\n\n\n\nmix_ra\n−0.002***\n\n\n\n(0.000)\n\n\nwomen_dat × mix_ra\n0.007***\n\n\n\n(0.001)\n\n\nNum.Obs.\n142346\n\n\nR2\n0.361\n\n\nR2 Within\n0.001\n\n\nStd.Errors\nby: race_id\n\n\nFE: course\nX\n\n\nFE: p_id\nX\n\n\nFE: yrmt_locid\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\nThis requires an explanation of interactions. Luckily, it’s relatively simple with two discrete variables.\n| ltime | man    | woman  |\n|-------|:------:|:------:|\n| same  | 0      | 0      |\n| mixed | -0.002 | 0.005  |"
  },
  {
    "objectID": "slides/slides3.html#bad-controls-survival-bias-selection-bias-self-selection-bias",
    "href": "slides/slides3.html#bad-controls-survival-bias-selection-bias-self-selection-bias",
    "title": "Control Variables",
    "section": "Bad Controls, Survival Bias, Selection Bias, Self-Selection Bias",
    "text": "Bad Controls, Survival Bias, Selection Bias, Self-Selection Bias\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nStockPrice\n\n StockPrice   \n\nPerformance-&gt;StockPrice\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nSurvival\n\n Survival   \n\nPerformance-&gt;Survival\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nSP500\n\n SP500   \n\nPerformance-&gt;SP500\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nIPO\n\n IPO   \n\nPerformance-&gt;IPO"
  },
  {
    "objectID": "slides/slides3.html#example-in-the-assignment",
    "href": "slides/slides3.html#example-in-the-assignment",
    "title": "Control Variables",
    "section": "Example in the assignment",
    "text": "Example in the assignment\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nPerformance\n\n Performance   \n\nCSR_report\n\n CSR_report   \n\nPerformance-&gt;CSR_report\n\n    \n\nScandals\n\n Scandals   \n\nPerformance-&gt;Scandals\n\n    \n\nReturn\n\n Return   \n\nCSR_report-&gt;Return\n\n    \n\nObserved_Report\n\n Observed_Report   \n\nCSR_report-&gt;Observed_Report"
  },
  {
    "objectID": "slides/slides3.html#simulation-bad-control",
    "href": "slides/slides3.html#simulation-bad-control",
    "title": "Control Variables",
    "section": "Simulation Bad Control",
    "text": "Simulation Bad Control\n\nCausal GraphSimulateResults\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nMarketReturn\n\n MarketReturn   \n\nCorporateGovernance-&gt;MarketReturn\n\n    \n\nPerformance-&gt;MarketReturn\n\n   \n\n\n\n\n\n\n\n\nd &lt;- tibble(corp_gov = rnorm(N, 0, 1)) %&gt;%\n  mutate(acc_profit = rnorm(N, corp_gov, sd = 3),\n         market_return = rnorm(N, 2 * corp_gov + acc_profit,\n                               sd = 3))\nlm1 &lt;- lm(acc_profit ~ corp_gov, data = d)\nlm2 &lt;- lm(acc_profit ~ corp_gov + market_return, data = d)\n\n\n\n\nmsummary(list(lm1, lm2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n0.039\n0.012\n\n\n\n(0.042)\n(0.030)\n\n\ncorp_gov\n1.000***\n−0.489***\n\n\n\n(0.042)\n(0.037)\n\n\nmarket_return\n\n0.498***\n\n\n\n\n(0.007)\n\n\nNum.Obs.\n5000\n5000\n\n\nR2\n0.101\n0.551\n\n\nRMSE\n2.98\n2.11\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides3.html#survival-bias",
    "href": "slides/slides3.html#survival-bias",
    "title": "Control Variables",
    "section": "Survival Bias",
    "text": "Survival Bias\n\nSimulateResults\n\n\n\nd &lt;- mutate(d, survival = if_else(market_return &gt; 5, 1, 0))\n\n\n\n\nlm1 &lt;- lm(acc_profit ~ corp_gov, data = filter(d, survival == 1))\nlm2 &lt;- lm(acc_profit ~ corp_gov * survival, data = d)\nmsummary(list(lm1, lm2), gof_omit = gof_omit, stars = stars, output = \"markdown\")\n\n\n\n\n\n(1)\n(2)\n\n\n\n\n(Intercept)\n3.518***\n-0.549***\n\n\n\n(0.115)\n(0.043)\n\n\ncorp_gov\n-0.137\n0.606***\n\n\n\n(0.095)\n(0.045)\n\n\nsurvival\n\n4.067***\n\n\n\n\n(0.135)\n\n\ncorp_gov × survival\n\n-0.743***\n\n\n\n\n(0.115)\n\n\nNum.Obs.\n853\n5000\n\n\nR2\n0.002\n0.262\n\n\n\nNote: ^^ * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\nOne interpretation is that a firm can survive by being lucky (and having high returns) and by having good corporate governance (which translates in high returns). The survivors all are more likely to be having good corporate governance and there is little that can be explained further."
  },
  {
    "objectID": "slides/slides3.html#visualisation-of-colliders-and-interactions",
    "href": "slides/slides3.html#visualisation-of-colliders-and-interactions",
    "title": "Control Variables",
    "section": "Visualisation of Colliders (and Interactions)",
    "text": "Visualisation of Colliders (and Interactions)\n\nFull SampleSurvival HighlightedSurvival Only"
  },
  {
    "objectID": "slides/slides3.html#pitching-format",
    "href": "slides/slides3.html#pitching-format",
    "title": "Control Variables",
    "section": "Pitching Format",
    "text": "Pitching Format\n\n\n\nDescription (Important)\n\nTitle\nResearch Question\nKey Paper\nMotivation\n\nTHREE (IDioT) (Important)\n\nIdea\nData\nTools"
  },
  {
    "objectID": "slides/slides3.html#pitching-format-1",
    "href": "slides/slides3.html#pitching-format-1",
    "title": "Control Variables",
    "section": "Pitching Format",
    "text": "Pitching Format\n\n\n\nDescription (Important)\n\nTitle\nResearch Question\nKey Paper\nMotivation\n\nTHREE (IDioT) (Important)\n\nIdea\nData\nTools\n\n\n\n\nTWO\n\nWhat’s new?\nSo what?\n\nONE contribution\nOther considerations."
  },
  {
    "objectID": "slides/slides5.html#a-basic-before---after-comparison",
    "href": "slides/slides5.html#a-basic-before---after-comparison",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A basic before - after comparison",
    "text": "A basic before - after comparison\n\n\n\n\n\n\n\nevent\n\n  \n\nEvent Happened\n\n Event Happened   \n\nTreatment\n\n Treatment   \n\nEvent Happened-&gt;Treatment\n\n    \n\nOutcome\n\n Outcome   \n\nTreatment-&gt;Outcome\n\n    \n\nTime\n\n Time   \n\nTime-&gt;Event Happened\n\n    \n\nTime-&gt;Outcome\n\n   \n\n\n\n\n\nChapter 17 Event Studies in Huntington-Klein (2021)\n\nWhere Time could be standing in for a lot of other annoying things that might happen."
  },
  {
    "objectID": "slides/slides5.html#use-data-before-the-event-to-infer-the-counterfactual-outcome-in-yellow",
    "href": "slides/slides5.html#use-data-before-the-event-to-infer-the-counterfactual-outcome-in-yellow",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)",
    "text": "Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not observe the yellow/gold returns. We have to estimate them or convince the reader that there are no trends to be expected for theoretical/institutional reasons. I will not go into the details of the estimation here but I will in week 7."
  },
  {
    "objectID": "slides/slides5.html#front-running-information-leaking-and-anticipation-are-all-annoying.",
    "href": "slides/slides5.html#front-running-information-leaking-and-anticipation-are-all-annoying.",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Front-running, information leaking, and anticipation are all annoying.",
    "text": "Front-running, information leaking, and anticipation are all annoying.\n\n\n\n\n\n\n\nevent\n\n  \n\nBefore Block Trade\n\n Before Block Trade   \n\nLarge Block Trade\n\n Large Block Trade   \n\nBefore Block Trade-&gt;Large Block Trade\n\n    \n\nPrice Impact\n\n Price Impact   \n\nLarge Block Trade-&gt;Price Impact\n\n    \n\nTime\n\n Time   \n\nTime-&gt;Before Block Trade\n\n    \n\nTime-&gt;Price Impact\n\n   \n\n\n\n\n\n\n\n\nGo also back to Assignment 2 where we modeled the same problem when investors anticipate a donation.\n\n\n\nTo gauge demand from buyers and potentially gin up interest from sellers, bankers send out lists of shares with upcoming lockup expirations, according to market participants. (Money Stuff, Matt Levine)\nSometimes, bankers also engage in hypothetical conversations with buyers before they have a mandate. Asking prospective buyers whether they might be interested in certain stocks is one thing. But if there are indeed plans afoot for block sales, such conversations, even phrased hypothetically, can tip off savvy money managers. (Money Stuff, Matt Levine)"
  },
  {
    "objectID": "slides/slides5.html#what-if-we-had-an-additional-control-group-to-estimate-the-counterfactual",
    "href": "slides/slides5.html#what-if-we-had-an-additional-control-group-to-estimate-the-counterfactual",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "What if we had an additional control group to estimate the counterfactual?",
    "text": "What if we had an additional control group to estimate the counterfactual?"
  },
  {
    "objectID": "slides/slides5.html#a-simulated-cheap-talk-example-voluntary-disclosure-in-time-2",
    "href": "slides/slides5.html#a-simulated-cheap-talk-example-voluntary-disclosure-in-time-2",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2",
    "text": "A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2\n\nN &lt;- 500\nT &lt;- 2\ntime_effect &lt;- c(3.5, 0)\nrd_did_firm &lt;- tibble(\n  firm = 1:N,\n  performance = runif(N, 1, 10),\n  firm_effect = rnorm(N, 0, 2) + ifelse(performance &lt; 3, 3, 0)\n)\nrd_did_panel &lt;- tibble(\n  firm = rep(1:N, each = T),\n  time = rep(1:T, times = N)) %&gt;%\n  left_join(rd_did_firm, by = \"firm\") %&gt;%\n  mutate(\n    report = ifelse(time == 2, ifelse(performance &gt; 3, 1, 0), 0),\n    noise = rnorm(N*T, 0, 3),\n    profit_report = 6.5 + time_effect[time] + firm_effect + noise,\n    profit_no_report = 1.5 + time_effect[time] + firm_effect + noise,\n    actual_profit = ifelse(report == 1, profit_report, profit_no_report))\n\n\nThe idea is that we have firms who perform well (performance &gt; 3) and firms that perform bad (performance &lt; 3). The firms that perform well will voluntarily disclose a report in time 2. We can see the effect as the difference between time 1 and time 2 for disclosers and non-disclosers.\nImportant: the cost of misreporting is not in calculated in the profit. The reasoning would be that this might be a litigation cost that would only emerge later on."
  },
  {
    "objectID": "slides/slides5.html#the-causal-effects-in-our-simulation",
    "href": "slides/slides5.html#the-causal-effects-in-our-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Causal Effects in Our Simulation",
    "text": "The Causal Effects in Our Simulation\n\nrd_did_panel %&gt;%\n  mutate(causal_effect = profit_report - profit_no_report) %&gt;%\n  group_by(time, report2 = performance &gt; 3) %&gt;%\n  summarise(profit_report = mean(profit_report),\n            profit_no_report = mean(profit_no_report),\n            causal_effect = mean(causal_effect)) %&gt;%\n  kable(digits = 1)\n\n\n\n\ntime\nreport2\nprofit_report\nprofit_no_report\ncausal_effect\n\n\n\n\n1\nFALSE\n13.1\n8.1\n5\n\n\n1\nTRUE\n10.2\n5.2\n5\n\n\n2\nFALSE\n9.4\n4.4\n5\n\n\n2\nTRUE\n6.6\n1.6\n5"
  },
  {
    "objectID": "slides/slides5.html#a-summary-of-the-actual-profits",
    "href": "slides/slides5.html#a-summary-of-the-actual-profits",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A Summary of The Actual Profits",
    "text": "A Summary of The Actual Profits\n\nrd_did_panel %&gt;%\n  group_by(time, report2 = performance &gt; 3) %&gt;%\n  summarise(actual_profit = mean(actual_profit)) %&gt;%\n  pivot_wider(names_from = time, values_from = actual_profit) %&gt;%\n  kable(digits = 1)\n\n\n\n\nreport2\n1\n2\n\n\n\n\nFALSE\n8.1\n4.4\n\n\nTRUE\n5.2\n6.6"
  },
  {
    "objectID": "slides/slides5.html#regressions",
    "href": "slides/slides5.html#regressions",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Regressions",
    "text": "Regressions\n\ndid_lm &lt;- feols(actual_profit ~ report, data = rd_did_panel)\ndid_sub &lt;- feols(actual_profit ~ report, data = filter(rd_did_panel, time == 2))\ndid_fixed &lt;- feols(actual_profit ~ report | firm, data = rd_did_panel)\ndid_did &lt;- feols(actual_profit ~ report | firm + time, data = rd_did_panel)\nmsummary(list(simple = did_lm, \"time 2\" = did_sub, \"firm FE\" = did_fixed, \"two-way FE\" = did_did),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\nsimple\n time 2\nfirm FE\ntwo-way FE\n\n\n\n\n(Intercept)\n5.580***\n4.380***\n\n\n\n\n\n(0.144)\n(0.308)\n\n\n\n\nreport\n1.005***\n2.206***\n1.403***\n5.091***\n\n\n\n(0.233)\n(0.352)\n(0.208)\n(0.428)\n\n\nNum.Obs.\n1000\n500\n1000\n1000\n\n\nR2\n0.018\n0.073\n0.624\n0.685\n\n\nR2 Within\n\n\n0.071\n0.222\n\n\nRMSE\n3.58\n3.34\n2.22\n2.03\n\n\nStd.Errors\nIID\nIID\nby: firm\nby: firm\n\n\nFE: firm\n\n\nX\nX\n\n\nFE: time\n\n\n\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides5.html#what-if-we-have-three-periods",
    "href": "slides/slides5.html#what-if-we-have-three-periods",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "What if we have three periods?",
    "text": "What if we have three periods?\n\n\n\n\n\n\nNote\n\n\nWe assume that over time investors and regulators get better at detecting when firms exaggerate in their report.\n\n\n\n\nTime 1: Reports are not believable, nobody reports\nTime 2: The biggest exaggerations will be caught, only well performing firms will report and communicate that they are doing excellent.\nTime 3: More subtle exaggerations will be caught. The worst performers will not report at all, the moderate performers will report and say that they will do well, the good performers will report that they are doing excellent.\n\n\n\nSee the Appendix of the assignment for the derivation of the exact parameters."
  },
  {
    "objectID": "slides/slides5.html#setup-of-three-period-simulation",
    "href": "slides/slides5.html#setup-of-three-period-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Setup of three period simulation",
    "text": "Setup of three period simulation\n\nN &lt;- 1000\nT &lt;- 3\ncutoff2 &lt;- 3 # performance cutoff to report for time 1\ncutoff3 &lt;- c(4/3, 4 + 2/3) # performance cutoff to report for time 2\nprofit1 &lt;- 5\nprofit2 &lt;- c(1.5, 6.5) #Profits for time 2 depending on report\nprofit3 &lt;- c(2/3, 3, 7 + 1/3) #Profits for time 2 depending on report\nrd_did3_firm &lt;- tibble(\n  firm = 1:N,\n  performance = runif(N, 0, 10),\n  firm_effect = rnorm(N, 0, 2) + ifelse(performance &lt; cutoff2, 3, 0)\n)"
  },
  {
    "objectID": "slides/slides5.html#three-period-simulation",
    "href": "slides/slides5.html#three-period-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Three period simulation",
    "text": "Three period simulation\n\nrd_did3_panel &lt;- tibble(\n  firm = rep(1:N, each = T),\n  time = rep(1:T, times = N)) %&gt;%\n  left_join(rd_did3_firm, by = \"firm\") %&gt;%\n  mutate(\n    # When will firms report?\n    report = case_when(\n      time == 1 ~ 0,\n      time == 2 & performance &lt; cutoff2 ~ 0,\n      time == 3 & performance &lt; cutoff3[1] ~ 0,\n      TRUE ~ 1),\n    noise = rnorm(T*N, 0, 5),\n    profit_no_report = firm_effect + noise +\n      case_when(\n        time == 1 ~ profit1,\n        time == 2 ~ profit2[1],\n        time == 3 ~ profit3[1]\n    ),\n    profit_report = firm_effect + noise +\n      case_when(\n        time == 1 ~ profit1,\n        time == 2 ~ profit2[2],\n        time == 3 & performance &lt; cutoff3[2] ~ profit3[2],\n        TRUE ~ profit3[3]\n      ),\n    actual_profit = ifelse(report == 1, profit_report, profit_no_report)\n  )"
  },
  {
    "objectID": "slides/slides5.html#overview-of-4-groups-of-firms",
    "href": "slides/slides5.html#overview-of-4-groups-of-firms",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Overview of 4 groups of firms",
    "text": "Overview of 4 groups of firms\n\nNever reporters\nReporters in year 3\nReporters in year 2 and 3 (Medium)\nReporters in year 2 and 3 (High)\n\n\ncausal_effects &lt;- rd_did3_panel %&gt;%\n  mutate(causal_effect = profit_report - profit_no_report,\n         group = case_when(\n           performance &lt; cutoff3[1] ~ 1,\n           performance &lt; cutoff2 ~ 2,\n           performance &lt; cutoff3[2] ~ 3,\n           TRUE ~ 4\n         )) %&gt;%\n  group_by(time, group) %&gt;%\n  summarise(report = mean(report),\n            N = n(),\n            M_report = mean(profit_report),\n            M_no_report = mean(profit_no_report),\n            M_causal_effect = mean(causal_effect))"
  },
  {
    "objectID": "slides/slides5.html#overview-of-4-groups-of-firms-1",
    "href": "slides/slides5.html#overview-of-4-groups-of-firms-1",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Overview of 4 groups of firms",
    "text": "Overview of 4 groups of firms\n\n\n\n\n\ntime\ngroup\nreport\nN\nM_report\nM_no_report\nM_causal_effect\n\n\n\n\n1\n1\n0\n141\n7.4\n7.4\n0.0\n\n\n1\n2\n0\n159\n7.5\n7.5\n0.0\n\n\n1\n3\n0\n168\n4.9\n4.9\n0.0\n\n\n1\n4\n0\n532\n5.0\n5.0\n0.0\n\n\n2\n1\n0\n141\n9.3\n4.3\n5.0\n\n\n2\n2\n0\n159\n9.3\n4.3\n5.0\n\n\n2\n3\n1\n168\n6.7\n1.7\n5.0\n\n\n2\n4\n1\n532\n6.7\n1.7\n5.0\n\n\n3\n1\n0\n141\n4.7\n2.3\n2.3\n\n\n3\n2\n1\n159\n5.4\n3.1\n2.3\n\n\n3\n3\n1\n168\n2.9\n0.6\n2.3\n\n\n3\n4\n1\n532\n7.2\n0.5\n6.7"
  },
  {
    "objectID": "slides/slides5.html#two-way-fixed-effects",
    "href": "slides/slides5.html#two-way-fixed-effects",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Two-way Fixed Effects",
    "text": "Two-way Fixed Effects\n\ntwoway12 &lt;- feols(actual_profit ~ report | firm + time,\n                  data = filter(rd_did3_panel, time != 3))\ntwoway13 &lt;- feols(actual_profit ~ report | firm + time,\n                  data = filter(rd_did3_panel, time != 2))\ntwoway123 &lt;- feols(actual_profit ~ report | firm + time,\n                  data = rd_did3_panel)"
  },
  {
    "objectID": "slides/slides5.html#separate-2-by-2-effects-are-larger-than-the-total-sample-effect",
    "href": "slides/slides5.html#separate-2-by-2-effects-are-larger-than-the-total-sample-effect",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Separate 2 by 2 effects are larger than the total sample effect",
    "text": "Separate 2 by 2 effects are larger than the total sample effect\n\nmsummary(list(\"time 1 and 2\" = twoway12, \"time 1 and 3\" = twoway13,\n              \"time 1, 2 and 3\" = twoway123), gof_omit = gof_omit,\n         stars = c(\"*\" = .1, \"**\" = .05, \"***\" = .01))\n\n\n\n\n\n time 1 and 2\n time 1 and 3\n time 1, 2 and 3\n\n\n\n\nreport\n4.882***\n5.671***\n4.219***\n\n\n\n(0.488)\n(0.647)\n(0.409)\n\n\nNum.Obs.\n2000\n2000\n3000\n\n\nR2\n0.578\n0.565\n0.437\n\n\nR2 Within\n0.093\n0.066\n0.051\n\n\nRMSE\n3.49\n3.71\n4.15\n\n\nStd.Errors\nby: firm\nby: firm\nby: firm\n\n\nFE: firm\nX\nX\nX\n\n\nFE: time\nX\nX\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not what I would expect. Why would the full sample lead to a smaller effect than all the subsamples?"
  },
  {
    "objectID": "slides/slides5.html#problem-statement",
    "href": "slides/slides5.html#problem-statement",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nFinally, when research settings combine staggered timing of treatment effects and treatment effect heterogeneity across firms or over time, staggered DiD estimates are likely to be biased. In fact, these estimates can produce the wrong sign altogether compared to the true average treatment effects."
  },
  {
    "objectID": "slides/slides5.html#solution",
    "href": "slides/slides5.html#solution",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Solution",
    "text": "Solution\n\nWhile the literature has not settled on a standard, the proposed solutions all deal with the biases arising from the “bad comparisons” problem inherent in TWFE DiD regressions by modifying the set of effective comparison units in the treatment effect estimation process. For example, each alternative estimator ensures that firms receiving treatment are not compared to those that previously received it.\n\n\nAgain, the solution to all our problems is to make sure that we make the right comparison."
  },
  {
    "objectID": "slides/slides5.html#simulation-setup---the-true-average-treatment-effect-of-three-groups",
    "href": "slides/slides5.html#simulation-setup---the-true-average-treatment-effect-of-three-groups",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Simulation Setup - The True Average Treatment Effect of Three Groups",
    "text": "Simulation Setup - The True Average Treatment Effect of Three Groups\n\n\nIt’s clear that the average treatment effect should be positive. It’s positive for every group."
  },
  {
    "objectID": "slides/slides5.html#the-estimated-effect-by-twoway-fixed-effects-of-500-simulations",
    "href": "slides/slides5.html#the-estimated-effect-by-twoway-fixed-effects-of-500-simulations",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Estimated Effect by Twoway Fixed Effects of 500 Simulations",
    "text": "The Estimated Effect by Twoway Fixed Effects of 500 Simulations"
  },
  {
    "objectID": "slides/slides5.html#the-sun2021-solution---restrict-the-sample",
    "href": "slides/slides5.html#the-sun2021-solution---restrict-the-sample",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Sun and Abraham (2021) Solution - Restrict The Sample",
    "text": "The Sun and Abraham (2021) Solution - Restrict The Sample"
  },
  {
    "objectID": "slides/slides5.html#the-estimated-effect-with-the-sun-and-abraham-solution",
    "href": "slides/slides5.html#the-estimated-effect-with-the-sun-and-abraham-solution",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Estimated Effect with the Sun and Abraham Solution",
    "text": "The Estimated Effect with the Sun and Abraham Solution"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham-in-practice",
    "href": "slides/slides5.html#sun-and-abraham-in-practice",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham in Practice",
    "text": "Sun and Abraham in Practice\n\nsa_new &lt;- readRDS(here(\"data\", \"sa_new.RDS\"))\nsa_fe &lt;- feols(roa ~ 1 + sunab(treatment_group, year) | firm + year,\n               cluster = \"state\", data = sa_new)\nsa_fe_att &lt;- summary(sa_fe, agg = \"ATT\")\nsa_fe_group &lt;- summary(sa_fe, agg = \"cohort\")\n\n\ntreatment_group: first year of treatment\nyear: calendar year\n\n\nnames(sa_fe$coefficients)\n\n [1] \"year::-18:cohort::1998\" \"year::-17:cohort::1998\" \"year::-16:cohort::1998\"\n [4] \"year::-15:cohort::1998\" \"year::-14:cohort::1998\" \"year::-13:cohort::1998\"\n [7] \"year::-12:cohort::1998\" \"year::-11:cohort::1998\" \"year::-10:cohort::1998\"\n[10] \"year::-9:cohort::1989\"  \"year::-9:cohort::1998\"  \"year::-8:cohort::1989\" \n[13] \"year::-8:cohort::1998\"  \"year::-7:cohort::1989\"  \"year::-7:cohort::1998\" \n[16] \"year::-6:cohort::1989\"  \"year::-6:cohort::1998\"  \"year::-5:cohort::1989\" \n[19] \"year::-5:cohort::1998\"  \"year::-4:cohort::1989\"  \"year::-4:cohort::1998\" \n[22] \"year::-3:cohort::1989\"  \"year::-3:cohort::1998\"  \"year::-2:cohort::1989\" \n[25] \"year::-2:cohort::1998\"  \"year::0:cohort::1989\"   \"year::0:cohort::1998\"  \n[28] \"year::1:cohort::1989\"   \"year::1:cohort::1998\"   \"year::2:cohort::1989\"  \n[31] \"year::2:cohort::1998\"   \"year::3:cohort::1989\"   \"year::3:cohort::1998\"  \n[34] \"year::4:cohort::1989\"   \"year::4:cohort::1998\"   \"year::5:cohort::1989\"  \n[37] \"year::5:cohort::1998\""
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---relative-year",
    "href": "slides/slides5.html#sun-and-abraham---relative-year",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Relative Year",
    "text": "Sun and Abraham - Relative Year\n\nmsummary(sa_fe, gof_omit = gof_omit, stars = stars, statistic = NULL,\n         estimate = \"{estimate} ({std.error}) {stars}\", coef_omit = \"-1\")\n\n\n\n\n\n (1)\n\n\n\n\nyear = -9\n−0.003 (0.006)\n\n\nyear = -8\n0.001 (0.005)\n\n\nyear = -7\n−0.001 (0.006)\n\n\nyear = -6\n−0.002 (0.005)\n\n\nyear = -5\n0.005 (0.005)\n\n\nyear = -4\n0.003 (0.005)\n\n\nyear = -3\n0.004 (0.004)\n\n\nyear = -2\n0.010 (0.006)\n\n\nyear = 0\n0.011 (0.005) *\n\n\nyear = 1\n0.025 (0.006) ***\n\n\nyear = 2\n0.042 (0.006) ***\n\n\nyear = 3\n0.055 (0.005) ***\n\n\nyear = 4\n0.062 (0.005) ***\n\n\nyear = 5\n0.082 (0.006) ***\n\n\nNum.Obs.\n119996\n\n\nR2\n0.727\n\n\nR2 Within\n0.005\n\n\nRMSE\n0.17\n\n\nStd.Errors\nby: state\n\n\nFE: firm\nX\n\n\nFE: year\nX"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---relative-year-1",
    "href": "slides/slides5.html#sun-and-abraham---relative-year-1",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Relative Year",
    "text": "Sun and Abraham - Relative Year\n\niplot(sa_fe)"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---att",
    "href": "slides/slides5.html#sun-and-abraham---att",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - ATT",
    "text": "Sun and Abraham - ATT\n\nmsummary(sa_fe_att, gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n\n\n\n\nATT\n0.046***\n\n\n\n(0.009)\n\n\nNum.Obs.\n119996\n\n\nR2\n0.727\n\n\nR2 Within\n0.005\n\n\nRMSE\n0.17\n\n\nStd.Errors\nby: state\n\n\nFE: firm\nX\n\n\nFE: year\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---cohort-effects",
    "href": "slides/slides5.html#sun-and-abraham---cohort-effects",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Cohort Effects",
    "text": "Sun and Abraham - Cohort Effects\n\nmsummary(sa_fe_group, gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n\n\n\n\ncohort = 1989\n0.058***\n\n\n\n(0.006)\n\n\ncohort = 1998\n0.034***\n\n\n\n(0.005)\n\n\nNum.Obs.\n119996\n\n\nR2\n0.727\n\n\nR2 Within\n0.005\n\n\nRMSE\n0.17\n\n\nStd.Errors\nby: state\n\n\nFE: firm\nX\n\n\nFE: year\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides5.html#take-away-lessons",
    "href": "slides/slides5.html#take-away-lessons",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Take-away Lessons",
    "text": "Take-away Lessons\n\n\n\n\n\n\nNote\n\n\n\nSimulations are good!\nEverything is a regression (Ok, not really)\nNot all the data should go in the regression"
  },
  {
    "objectID": "slides/slides5.html#abadie2017",
    "href": "slides/slides5.html#abadie2017",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Abadie et al. (2017)",
    "text": "Abadie et al. (2017)\n\n\n\n\n\n\nNote\n\n\nWhat is the level of the treatment variable? What is the comparison?\n\n\n\n\nMixed race or same-sex race\nState legislation\nCountry legislation\nFirm corporate governance changes"
  },
  {
    "objectID": "freaky_friday/index.html",
    "href": "freaky_friday/index.html",
    "title": "Research Design",
    "section": "",
    "text": "This website is an attempt to replicate the main results in Dellavigna and Pollet (2009) from scratch. The paper is a good example because (1) it has an explicit theoretical model, (2) provides excellent descriptions on how the different measures are constructed, (3) uses the canonical finance design, an event study. I will focus most of my attention on (2) and (3) but (1) is important because it provides guidance to readers of the paper why the measures and the design is important.\nThe basic argument of the paper is that firms will bury earnings announcements on Fridays if the earnings are bad because the market pays less attention to news on Fridays."
  },
  {
    "objectID": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "href": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "title": "Research Design",
    "section": "The Unexpected Component of Earnings",
    "text": "The Unexpected Component of Earnings\n\\[ s_{t,k} = \\frac{e_{t,k} - \\hat{e}_{t,k}}{P_{t,k}} \\]\nIn this equation, \\(s_{t,k}\\) is the surprise (i.e. the unexpected component) in earnings of company \\(k\\) at time \\(t\\). It is calculated by the actual earnings per share, \\(e_{t,k}\\), minus the median expected earnings by analysts, \\(\\hat{e}_{t,k}\\), divided by the price of the stock 5 days before the earnings release, \\(P_{t,k}\\). You will see over and over that empirical researchers are wary that the day(s) just before an announcement might be special, i.e. the news might have already leaked out, for good and less good reasons. So, instead of using the price the day before the earnings release, the paper picks a couple of days earlier 2.\nThe most important part is that we try to filter out all the information in the earnings announcement that is already known to the market by subtracting the earnings estimates of analysts. The implicit assumption is that these earnings estimates are a good measure of the market’s information on the company just before the earnings are announced."
  },
  {
    "objectID": "freaky_friday/index.html#the-market-reaction",
    "href": "freaky_friday/index.html#the-market-reaction",
    "title": "Research Design",
    "section": "The Market Reaction",
    "text": "The Market Reaction\nThe market reaction is calculated as the abnormal return from day \\(h\\) to day \\(H\\). 3\n\\[ R_{t,k}^{(h, H)} = [\\Pi_{j=h}^H (1 + R_{j,k})] - 1\n- \\hat{\\beta}_{t,k} [\\Pi_{j=h}^H (1 + R_{j,m}) - 1]\\]\nThis looks complicated but it is quite simple. The first part is the raw return of the stock over the period that we are interested in 4. The second part is the return of the market times the sensitivity of the stock to the market. The latter, \\(\\hat{\\beta}_{t,k}\\) is estimated with data from before the announcement. The goal of this approach is to filter out other reasons that the stock price might go up or down because of general economic or financial events that affect the stock market as a whole.\nSpecifically, we estimate the following regression model with data from days, \\(u\\), with \\(u\\) between 46 and 300 days before the earnings announcements. This is a regression of the market return on the firm return.\n\\[ R_{u,k} = \\alpha_{t,k} + \\beta_{t,k} R_{u,m}\\]\nAgain, we are using data from long before the earnings announcement so that our estimate is not contaminated by the earnings announcement 5.\nThere are lot of different approaches in the literature to estimate these abnormal returns but they all have the same flavour of trying to filter out other reasons why the stock price might be moving. In a pure regression framework, we would include additional variables as control variables. Constructing variables like the abnormal returns and the earnings surprise like this serves exactly the same function. There are good reasons to use the approach of first constructing the measures as precise as possible in an event study design like this but there are some problem with applying this same logic in different research designs Chen, Hribar, and Melessa (n.d.). See also the section on generated variables."
  },
  {
    "objectID": "freaky_friday/index.html#footnotes",
    "href": "freaky_friday/index.html#footnotes",
    "title": "Research Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSort of. It can look like time travel to the empirical researcher because the earnings are released after the market reaction to it but ofcourse the economic facts happened before the market reacted. I am merely pointing out the implicit assumption that when news is officially announced is not necessarily when the market receives the information.↩︎\nThis is one of these forms of time travel. Remark that the if there is information leaked before the announcement, the market reaction after the announcement will be smaller than when that would not have happened. In a way, we are underestimating the effect of the earnings on stock price returns if we only look at what happened after the announcement. Most empirical researchers are ok with this because it means that they are less likely to find an effect. So they are not biasing their method towards finding something.↩︎\nIn comparison with the paper I am simplifying the formula a little bit by setting \\(\\tau = 0\\). That is, the day of the announcement is day 0.↩︎\nRecall that if a stock goes up with 10% and goes down with 10% we can write the total return as \\(1.10 \\times 0.90 - 1 = -0.01\\).↩︎\nBut it will be affected by the two previous earnings announcements …↩︎"
  },
  {
    "objectID": "freaky_friday/linking.html",
    "href": "freaky_friday/linking.html",
    "title": "Combining databases",
    "section": "",
    "text": "The packages are the same as before.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/linking.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "freaky_friday/linking.html#linking-ibes",
    "href": "freaky_friday/linking.html#linking-ibes",
    "title": "Combining databases",
    "section": "Linking I/B/E/S",
    "text": "Linking I/B/E/S\n\nlinking_table %&gt;%\n  select(gvkey, ticker) %&gt;%\n  distinct() %&gt;%\n  summarise(N = n(), .by = ticker) %&gt;%\n  filter(N &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: ticker &lt;chr&gt;, N &lt;int&gt;\n\n\nThere are no tickers linked with multiple gvkeys. This means that left_join from I/B/E/S is the way to start the joining process. That way, there will be no duplicate matches from Compustat."
  },
  {
    "objectID": "freaky_friday/linking.html#footnotes",
    "href": "freaky_friday/linking.html#footnotes",
    "title": "Combining databases",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept for some checks on the linking table.↩︎"
  },
  {
    "objectID": "freaky_friday/download_stocks.html",
    "href": "freaky_friday/download_stocks.html",
    "title": "Stock price data",
    "section": "",
    "text": "On this page, we download the stock price data so that we can later calculate the abnormal return after the earnings announcements.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(RPostgres)\nlibrary(dbplyr)\ni_am(\"freaky_friday/download_stocks.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nearn_ann &lt;- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\")) \n\n\nwrds &lt;- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  user='stimas',\n                  sslmode='require')\n\nThis section sets the parameters that we will need to limit the download. Following Dellavigna and Pollet (2009), the beginning date is 300 days before the first earnings announcement and the end date is 75 days after the last earnings announcement. I could have done this for every earnings announcement specificially but Finally, I keep the permno identifiers because these are the only stocks we want the data from.\n\ncrsp_input &lt;- earn_ann %&gt;%\n  summarise(begin = min(anndat) - 300, end = max(anndat) + 75, .by = permno) %&gt;%\n  glimpse()\n\nRows: 8,754\nColumns: 3\n$ permno &lt;dbl&gt; 10560, 10656, 88784, 10659, 87832, 84606, 80585, 88836, 87771, …\n$ begin  &lt;date&gt; 2003-12-26, 2002-07-12, 2001-01-17, 2001-11-09, 2003-05-06, 20…\n$ end    &lt;date&gt; 2006-07-10, 2006-07-19, 2006-07-18, 2006-05-23, 2005-01-16, 20…\n\npermnos &lt;- crsp_input$permno\nbegin_date &lt;- min(crsp_input$begin)\nend_date &lt;- max(crsp_input$end)\n\nI use the same syntax as before to call the WRDS databases as before with sql interspersed with the R parameters created in the previous code block. We get the daily volume, return, price, shares outstanding, cumulative factor to adjust price, and cumulative factor to adjust shares. The latter two are adjustment factors for stock splits and dividends which we probably will not need but if we do we have them.\nThis is by far the largest download from WRDS and this is why it has it’s own page. We do not want to rerun this more than strictly necessary.\n\ncrsp_query &lt;- tbl(wrds, in_schema(\"crsp_a_stock\", \"dsf\")) %&gt;%\n  filter(permno %in% permnos, date &gt;= begin_date, date &lt;= end_date) %&gt;%\n  select(permno, date, vol, ret, prc, shrout, cfacpr, cfacshr)\nall_stocks &lt;- collect(crsp_query)\nsaveRDS(all_stocks, here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nprint(all_stocks)\n\n# A tibble: 14,967,132 × 8\n   permno date         vol      ret   prc shrout cfacpr cfacshr\n    &lt;int&gt; &lt;date&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  10002 1994-03-10   700 -0.00952  13     2999    1.5     1.5\n 2  10002 1994-03-11   200  0.0577   13.8   2999    1.5     1.5\n 3  10002 1994-03-14     0 -0.0455  -13.1   2999    1.5     1.5\n 4  10002 1994-03-15     0  0       -13.1   2999    1.5     1.5\n 5  10002 1994-03-16  1700  0.00952  13.2   2999    1.5     1.5\n 6  10002 1994-03-17     0 -0.00943 -13.1   2999    1.5     1.5\n 7  10002 1994-03-18     0  0       -13.1   2999    1.5     1.5\n 8  10002 1994-03-21     0  0.00457 -13.1   2999    1.5     1.5\n 9  10002 1994-03-22  2000  0.0190   13.4   2999    1.5     1.5\n10  10002 1994-03-23     0 -0.00935 -13.2   2999    1.5     1.5\n# ℹ 14,967,122 more rows\n\n\nOne important footnote is that the price is negative on days where there were no trades. This might be important going forward.\n\n\n\n\nReferences\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x."
  },
  {
    "objectID": "freaky_friday/descriptive.html",
    "href": "freaky_friday/descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"freaky_friday/descriptive.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nmain &lt;- readRDS(here(\"data\", \"freaky_friday\", \"main.RDS\")) %&gt;%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\nglimpse(main)\n\nRows: 130,759\nColumns: 23\n$ ticker       &lt;chr&gt; \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       &lt;dbl&gt; -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        &lt;chr&gt; \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       &lt;dbl&gt; 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        &lt;chr&gt; \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            &lt;int&gt; 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 4, 2, …\n$ median       &lt;dbl&gt; -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         &lt;dbl&gt; -0.0896500, -0.0680000, -0.0370000, -0.1005500, -0.076580…\n$ mean_days    &lt;dbl&gt; 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    &lt;dbl&gt; 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     &lt;dbl&gt; -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          &lt;dbl&gt; 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value &lt;dbl&gt; 2592630.7, 1993992.8, 2200875.0, 1434970.5, 1769503.5, 24…\n$ surprise     &lt;dbl&gt; -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n$ weekday      &lt;ord&gt; Wed, Wed, Wed, Wed, Wed, Thu, Tue, Tue, Tue, Wed, Tue, We…\n$ group        &lt;chr&gt; \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         &lt;dbl&gt; 2005, 2005, 2005, 2005, 2006, 2004, 2001, 2002, 2002, 200…"
  },
  {
    "objectID": "freaky_friday/descriptive.html#quantiles",
    "href": "freaky_friday/descriptive.html#quantiles",
    "title": "Descriptive statistics",
    "section": "Quantiles",
    "text": "Quantiles\n\nquantiles &lt;- main %&gt;%\n  mutate(sign = case_when(surprise &gt; 0 ~ \"positive\",\n                          surprise &lt; 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %&gt;%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %&gt;%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %&gt;%\n  glimpse()\n\nRows: 130,759\nColumns: 26\n$ ticker       &lt;chr&gt; \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       &lt;dbl&gt; -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        &lt;chr&gt; \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       &lt;dbl&gt; 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        &lt;chr&gt; \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            &lt;int&gt; 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 4, 2, …\n$ median       &lt;dbl&gt; -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         &lt;dbl&gt; -0.0896500, -0.0680000, -0.0370000, -0.1005500, -0.076580…\n$ mean_days    &lt;dbl&gt; 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    &lt;dbl&gt; 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     &lt;dbl&gt; -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          &lt;dbl&gt; 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value &lt;dbl&gt; 2592630.7, 1993992.8, 2200875.0, 1434970.5, 1769503.5, 24…\n$ surprise     &lt;dbl&gt; -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n$ weekday      &lt;ord&gt; Wed, Wed, Wed, Wed, Wed, Thu, Tue, Tue, Tue, Wed, Tue, We…\n$ group        &lt;chr&gt; \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         &lt;dbl&gt; 2005, 2005, 2005, 2005, 2006, 2004, 2001, 2002, 2002, 200…\n$ sign         &lt;chr&gt; \"negative\", \"negative\", \"negative\", \"positive\", \"negative…\n$ quintile     &lt;int&gt; 2, 1, 3, 5, 2, 5, 2, 4, 4, 4, 1, 3, 3, 2, 1, 2, 4, 3, 1, …\n$ quantile     &lt;dbl&gt; 2, 1, 3, 11, 2, 11, 2, 10, 10, 4, 6, 9, 9, 2, 6, 8, 4, 9,…\n\n\nThis is a quick version of Figure 1a. It can be further cleaned up with better axis labels. It shows the main results from Dellavigna and Pollet (2009) that the market reaction is subdued on Fridays.\n\nggplot(quantiles,\n       aes(y = car_short, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey()\n\n\n\n\nThis is how I would program Figure 1a and b together. It’s a good example of how using pivot_longer can make your life easier. In this case, if we need to plot multiple similar variables.\n\nquantiles %&gt;%\n  pivot_longer(c(car_short, car_long), values_to = \"car\", names_to = \"window\") %&gt;%\n  mutate(fig_name = if_else(window == \"car_short\", \"Figure 1a\", \"Figure 1b\")) %&gt;%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ fig_name, nrow = 2)"
  },
  {
    "objectID": "freaky_friday/assignment.html",
    "href": "freaky_friday/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "For the assignment, you can just copy the code in the assignment and make the necessary changes where indicated. Do not forget to copy all the code. Not just the one where you change something.\n\nSetup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/assignment.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nLet’s restrict the data to two years. So that the computations do not take too long.\n\nbegin &lt;- ymd(\"2003-01-01\")\nend &lt;- ymd(\"2005-12-31\")\n\nI keep the full stock price data for now. This code is a slightly adapted version of the Abnormal returns page on the website. There are two changes: (1) we have to filter only the announcements between begin and end and (2) we add more factors from the French data. I will not go into the theoretical details here. The basic idea is that stock return are not only sensitive to one set of economic factors which are captured in the market return, but also to more specific factors such as the size (smb) and whether the company is value or growth company (hml). In what follows, we will treat the returns to these factors exactly the same as the market return, i.e. as predictors for the returns of the company that we are interested in.\n\nearn_ann &lt;- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\")) %&gt;%\n  filter(anndat &gt;= begin, anndat &lt;= end)\nanalyst &lt;- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\")) %&gt;%\n  filter(anndat &gt;= begin, anndat &lt;= end)\nall_stocks &lt;- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench &lt;- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  mutate_if(is.numeric, ~ . / 100) %&gt;%\n  mutate(mkt = mkt_rf + rf) %&gt;%\n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\n\nThree-Factor model\nThe following code is almost identical to the Abnormal returns page. However, for the three-factor model we need to run the following regression for each announcement.\n\\[\nR_{u,k} = \\alpha_{t,k} + \\beta^{mkt}_{t,k} R_{u,m}\n+ \\beta^{size}_{t,k} smb_{u,m} + \\beta^{value}_{t,k} hlm_{u,m} + \\epsilon_{u,k}\n\\]\nThat is, we include two more factors (smb, and hlm) in the regression model that we run for each announcement. The three-factor model typically works with the stock return and market return adjusted for the risk free rate (i.e. ret - rf and mkt_rf respectively). The code will give us a column coefs with the 4 estimated parameters alpha, beta_mkt, beta_size, beta_value.\nYou need to change the code below in two parts as indicated by the comments in the code.\nThe code first creates a function creat_coefs_3factor for n earnings announcements where the default value for n is 6. You can test whether your function works by creating the test object which should be a data set with 6 rows. One for each of the 6 earnings announcements at the top of the earn_ann\n\ncreate_coefs_3factor &lt;- function(n = 6){\n  earn_ann %&gt;% head(n = n) %&gt;%\n    distinct(permno, anndat) %&gt;%\n    mutate(start = anndat - 300, end = anndat - 46) %&gt;%\n    left_join(select(all_stocks, permno, date, ret),\n              by = join_by(permno == permno, start &lt;= date, end &gt;= date)) %&gt;%\n    # Changes are necessary in the following lines\n    # ... needs to be changed\n    left_join(select(famafrench, ...),\n              by = join_by(date == date)) %&gt;%\n    filter(!is.na(ret), !is.infinite(ret)) %&gt;%\n    # Changes are necesary in the following lines:\n    # .x, .y., .z need to be changed\n    summarise(y = list(cbind(ret - rf)),\n              X = list(cbind(alpha = 1, beta_mkt = .x,\n                             beta_size = .y, beta_value = .z)),\n              .by = c(permno, anndat)) %&gt;%\n    mutate(coefs = pmap(list(X, y), ~ lm.fit(..1, ..2) %&gt;% coef()),\n           .by = c(permno, anndat)) %&gt;%\n    select(-y, -X)\n}\n\ntest &lt;- create_coefs_3factor()\n\nmicrobenchmark::microbenchmark(\n                  create_coefs_3factor(1000),\n                  times = 10)\n\nIf it takes on average (noticeably) more than 10 seconds to run the three-factor model on 1000 observations, it is ok to limit the sample to 1 year. You can change the start and end date above and rerun the code.\n\n\nAbnormal Returns\nIn the first parts of the next code, we will calculate the coefficients for all announcements in the 2 year dataset. This is computationally the most intensive step and I could take up to two minutes to finish. When finished, we have an object results that contains the alphas and betas, we need to calculate the abnormal returns.\nIn the next step, we need to calculate the abnormal returns for every announcement in our data based on the three factors (mkt, smb, hlm). That is, we have to subtract the actual return on a day, \\(h\\), after the announcement from the expected return based on the three-factor model.\n\\[ R_{t,k} ^ {h} = R_{h, k} - \\alpha_{t,k} - \\beta^{mkt}_{t,k} R_{h,m}\n- \\beta^{size}_{t,k} smb_{h,m} - \\beta^{value}_{t,k} hlm_{h,m}\\]\n\\(R_{t,k}^h\\) is the abnormal return on day \\(h\\) after the announcement, \\(R_{h,m}\\) is the market return on day \\(h\\), \\(smb_{h,m}\\) is the size factor and \\(hlm_{h,m}\\) is the value factor.\nIn the last change, you need to add up the abnormal returns for the days 0 and 1 for the short window after the announcement and the for the days 2-75 after the announcement. That is you need to calculate car_sum\nI also give the accumulation with the product which is more correct but the literature typically works with the sum as far as I could discern. You will also see further in the code that it does not really matter for our results even in the restricted dataset. You can use car_prod as an example for car_sum where car_sum is simpler.\n\nN &lt;- nrow(earn_ann)\nresults &lt;- create_coefs_3factor(N)\n\nabnormal &lt;- results %&gt;%\n  unnest_wider(coefs) %&gt;%\n  mutate(date75 = anndat + 75) %&gt;%\n  left_join(select(all_stocks, permno, date, ret),\n            by = join_by(permno == permno,\n                         date75 &gt;= date, anndat &lt;= date)) %&gt;% print %&gt;%\n  # Changes are necessary in the following lines\n  # ... needs to be changed\n  left_join(select(famafrench, ...),\n            by = join_by(date == date)) %&gt;%\n  # Changes are necessary in the following lines\n  # ... needs to be changed\n  mutate(ar = ...,\n         time_frame = if_else(date - anndat &lt;= 1, \"short\", \"long\")) %&gt;%\n  # Changes are necessary in the following lines\n  # .x needs to be changed\n  summarise(car_sum = ...,\n            car_prod = prod(1 + ar) - 1,\n            .by = c(permno, anndat, time_frame)) %&gt;%\n  filter(!is.na(car_sum), !is.na(car_prod)) %&gt;%\n  pivot_wider(values_from = c(car_sum, car_prod), names_from = time_frame)\nglimpse(abnormal)\n\n\n\nPutting it all together\nThe next two code blocks gather the necessary market price and market value information. Next we combine the earning announcement data with the analyst expectations data, the abnormal returns, and the stock price data. Finally, we calculate the earnings surprise scaled by the stock price.\nYou can just include this code as is.\n\nclean_prices &lt;- all_stocks %&gt;%\n  filter(prc &gt; 0) %&gt;%\n  select(permno, date, prc, shrout) %&gt;%\n  mutate(market_value = prc * shrout) %&gt;%\n  select(-shrout) %&gt;%\n  print()\n\n# A tibble: 14,333,348 × 4\n   permno date         prc market_value\n    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1  10002 1994-03-10  13         38987 \n 2  10002 1994-03-11  13.8       41236.\n 3  10002 1994-03-16  13.2       39737.\n 4  10002 1994-03-22  13.4       40112.\n 5  10002 1994-03-24  13.1       39362.\n 6  10002 1994-04-07  12.5       37488.\n 7  10002 1994-04-15  14         41986 \n 8  10002 1994-04-18  14         41986 \n 9  10002 1994-04-22  12.5       37488.\n10  10002 1994-04-28  14         41986 \n# ℹ 14,333,338 more rows\n\n\n\nsurprise &lt;- earn_ann %&gt;%\n  left_join(analyst,\n            by = join_by(ticker, anndat, actual, pdf)) %&gt;%\n  left_join(abnormal,\n            by = join_by(permno, anndat)) %&gt;%\n  mutate(date_minus5 = anndat - 5) %&gt;%\n  left_join(clean_prices,\n            by = join_by(permno, closest(date_minus5 &gt;= date))) %&gt;%\n  mutate(surprise = (actual - median) / prc)\nglimpse(surprise)\n\nThe last code block here further cleans the data following the rules set in the paper and as used by me in the Abnormal returns page. You can just include the code as is.\n\nwinsorise &lt;- 5/10000\nmain &lt;- surprise %&gt;%\n  filter(!is.na(surprise)) %&gt;%\n  filter(abs(median) &lt; prc, abs(actual) &lt; prc) %&gt;%\n  filter(prc &gt; 2) %&gt;%\n  mutate(weekday = wday(anndat, label = TRUE)) %&gt;%\n  filter(! weekday %in% c(\"Sat\", \"Sun\")) %&gt;%\n  filter(percent_rank(car_sum_long) &gt;= winsorise,\n         percent_rank(car_sum_long) &lt;= 1 - winsorise,\n         percent_rank(car_sum_short) &gt;= winsorise,\n         percent_rank(car_sum_short) &lt;= 1 - winsorise) %&gt;%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\n\n\n\nDescriptive Plot\nThe next code block creates the quantiles in the new main dataset so that we can recreate one of the figures.\n\nquantiles &lt;- main %&gt;%\n  mutate(sign = case_when(surprise &gt; 0 ~ \"positive\",\n                          surprise &lt; 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %&gt;%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %&gt;%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %&gt;%\n  glimpse()\n\nThe last part you need to do is to recreate the quick version of Figure 1a in the descriptives page\n\nggplot(quantiles,\n       aes(...)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey()\n\nThis is some free code just to demonstrate how easy it is to combine pivot_longer and ggplot to create similar plots for a bunch of similar variables where the variables are originally in different columns.\n\nquantiles %&gt;%\n  pivot_longer(c(car_sum_short, car_prod_short, car_sum_long, car_prod_long),\n               values_to = \"car\", names_to = \"window\") %&gt;%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ window, nrow = 2)"
  },
  {
    "objectID": "machine_learning/assignment.html",
    "href": "machine_learning/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Answer the following two questions in no more than 1 A4 page in total. My expectations are 5 to 10 sentences per question depending on your writing style. You can submit your answer on LMS in word, qmd, pdf, or html version."
  },
  {
    "objectID": "machine_learning/assignment.html#instructions",
    "href": "machine_learning/assignment.html#instructions",
    "title": "Assignment",
    "section": "",
    "text": "Answer the following two questions in no more than 1 A4 page in total. My expectations are 5 to 10 sentences per question depending on your writing style. You can submit your answer on LMS in word, qmd, pdf, or html version."
  },
  {
    "objectID": "machine_learning/assignment.html#questions",
    "href": "machine_learning/assignment.html#questions",
    "title": "Assignment",
    "section": "Questions",
    "text": "Questions\n\nSearch for an article in the same research area as your thesis that uses any machine learning technique. Give a citation to the paper and based on your reading of the abstract of the paper and skimming the methodology section, argue whether the machine learning improves the researchers’ ability to answer their research question. When you look for a paper do not restrict your search too much. I primarily want you to find a paper where you are comfortable with the main research question. For instance, I used google scholar and searched for \"voluntary disclosure\" \"machine learning\" source:accounting to find an article with a machine learning technique on voluntary disclosure in an accounting journal. If you cannot find an article in your research area, I would also accept a paper in any area that you have discussed in FINA4481, FINA4491, or ACCT4471. Do not spend too much time on finding the right paper!\nHow could a machine learning technique improve the method for your thesis? You can ignore the cost of data collection and can assume that you can use all the data sources that have been used in your research field.\n\n\n\n\n\n\n\nNote\n\n\n\nFor answering the questions, you will have to ask yourself the question whether the main research question involves a prediction task 1. If not and the research question involves estimating a causal 2 or descriptive 3 estimate, can better out-of-sample prediction help with this research question. Ask yourself the question whether traditional linear models or time series models are sufficient or whether more advanced machine learning methods are required."
  },
  {
    "objectID": "machine_learning/assignment.html#footnotes",
    "href": "machine_learning/assignment.html#footnotes",
    "title": "Assignment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nE.g. what is the future stock return based on current market conditions?↩︎\nE.g.: what is the effect of voluntary disclosure on the cost of capital?↩︎\nE.g.: what percentage of senior accountants supports changing the marginal tax rate?↩︎"
  },
  {
    "objectID": "machine_learning/theory.html",
    "href": "machine_learning/theory.html",
    "title": "Theory",
    "section": "",
    "text": "In this section, I will show how you can code a prediction model for the return of a company’s stock price based on the returns of a number of peer companies. I loosely follow the same approach as in Baker and Gelbach (2020). In that paper, peer firms are identified based on the SIC code in the Compustat data which indicates what the industry is of the firm’s primary product.\nWe are using the tidymodel package for the application and I will follow the introduction to tidymodels. The advantage of the tidymodels approach is that you can follow a very similar workflow for other machine learning methods than the one I will be showing here. The code itself is not that important. The goal is more to give you a starting point.\nTo understand the machine learning approach that we are using, I need to start of with some theory. The fundamental idea is that we want to avoid overfitting in the data that we have, so that when we use the model to predict on new data we still have good predictions. This means that when we estimate our model we do not want to have a model that fits the current data as good as possible. We want to regularize the parameters in the model so that we do not get a perfect fit in the current sample and a better out-of-sample predictions. For instance, if we use 200 trading days and have 200 peer firms, we can perfectly predict within the sample of 200 days 1 but there are no guarantees that we will get good predictions from that model out-of-sample (i.e. after the earnings announcement).\nFor the linear model to predict stock returns based on peers, we will use a the elastic net regularizer to bias the estimates within the sample data to make it more likely that the linear model will give good predictions out-of-sample. One way to think about the linear model with peers as predictors is that we are creating a bespoke market index for each firm as a weighted average of its peers.\nA regular linear model estimates the \\(\\beta\\)s by minimising the following equation where we minimise the sum of the squared difference between the outcome (\\(y_i\\)) and prediction (\\(X_i \\beta\\)) for each observation i.\n\\[\n\\sum^N_{i=1} (y_i - X_i \\beta)^2\n\\]\nThat is, we want to find estimates that give the best possible fit in the data. The regulariser puts a penalty on bigger absolute values for the \\(\\beta\\)s to limit overfitting to the in-sample data. The estimates will now be chosen to minimise the following equation.\n\\[\n\\sum^N_{i=1} (y_i - X_i \\beta)^2 + \\lambda \\left ( \\alpha \\sum^p_j \\beta^2_j + (1 - \\alpha) \\sum^p_j |\\beta_j| \\right)\n\\]\nThe size of the penalty is given by the parameters \\(\\lambda &gt; 0\\) and consists of two parts: the sum of the squared \\(\\beta\\)s and the sum of the absolute values of the \\(\\beta\\)s. The first term is the ridge reguliser and the second one is the LASSO regulariser. They both have been shown to have useful properties as regulisers and thus they are often used together with the weight \\(1 \\geq \\alpha \\geq 0\\). With \\(\\alpha = 1\\), we only use ridge regression and with \\(\\alpha = 0\\), we only use the LASSO.\nThe final step is that we need to choose the right values for \\(\\lambda\\) and \\(\\alpha\\). A common approach is to use cross validation where we split the data that we have in roughly equal sized partitions or folds. For instance, you have 10 folds with each 10% of the data. With cross validation, we will use the 90% of the remaining data, use a number of different values for \\(\\lambda\\) and \\(\\alpha\\) to estimate \\(\\beta\\)s and predict the 10% fold that we did not use for estimation. In other words, we use the data that we have to do a prediction task with the advantage that we can evaluate which \\(\\lambda\\) and \\(\\alpha\\) gives us the best predictions.\n\n\n\n\n\n\nImportant\n\n\n\nThe key insight is that if we care about a prediction task, we can use some of the data and pretend it is data that we have never seen when we estimate our prediction model. We can then test which model is actually good at predicting on data that it has never seen.\n\n\nWe do need a measure to evaluate the quality of the predictions. A common choice is the Root Mean Squared Error (RMSE) which is defined as the square root of the squared difference between the actual value of the outcome and the predictions for the outcome.\n\\[\n\\sqrt{ \\frac{\\sum^N_{i=1} (y_i - \\hat{y}_i)^2}{N} }\n\\]\nWe will use the RMSE to evaluate the predictions out-of-sample. The RMSE is similar to the first equation where we choose the \\(\\beta\\)s to minimise the squared difference between the outcome and the fitted data in the in-sample data."
  },
  {
    "objectID": "machine_learning/theory.html#footnotes",
    "href": "machine_learning/theory.html#footnotes",
    "title": "Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a system of linear equations with 200 unknown parameters (the \\(\\beta\\)s) and 200 observations (the trading days).↩︎"
  },
  {
    "objectID": "generalised/introduction.html",
    "href": "generalised/introduction.html",
    "title": "Introduction and Theory",
    "section": "",
    "text": "The question in this section is how we should deal with outcome variables that are not continuous or restricted to be positive. I will limit the applications to the most common examples: (1) binary outcome variables and (2) positive outcome variables. Examples of binary outcome variables are the decision to disclose information, or merge a company. Positive outcome variables are variables such as employee salaries or the market value of a company.\nThis is a fairly contentious topic where different literature streams have different expectations and norms for what is appropriate. This is by no means a complete overview of the topic. My aim is to give you my perspective and to give you enough tools to be able to decide what is most appropriate for your research and to explain to assessors why you made those choices.\nThe basic problem is the following. In our typical linear model, the outcome variable can take on any value between \\(-\\infty\\) and \\(+\\infty\\).\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nIf we want to make sure that \\(y_i\\) is restricted to a certain domain, for instance between 0 and 1, we can use a transformation function on the linear model. These transformed linear models are often called generalised linear models.\n\\[\ny_i = g(\\alpha + \\beta x_i + \\epsilon_i)\n\\]\nYou can also think of the inverse of the transformation function as transforming the outcome so that it can take all positive and negative values. This is implicitly what we have done earlier when we used the logarithmic transformation in our earlier discussion on what the appropriate measure is for pay-performance sensitivity. In the second week, we discussed the importance of theory to determine what the right measure is according to our research question. In this week, we are asking the question whether we are interested in \\(y_i\\) or in the transformation \\(g^{-1}(y_i)\\).\n\\[\ng^{-1}(y_i) = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nThe discussion should also remind you of the difference between prediction and parameter estimation at the start of the machine learning chapter. The issue of the mismatch between the domain of the linear model and the outcome variable is less of an issue if we are really interested in estimating the parameter \\(\\beta\\). It really is a problem if we are trying to predict the outcome variable \\(y\\) because we know that we are going to get predictions that are impossible. In the reminder of this chapter I will focus on parameter estimation because that is a more common research question in finance and accounting research. That does not mean that prediction of restricted outcome variables is not important. Indeed, regularised versions of generalised linear models for binary outcome variables are popular classification models. They are fairly easy to implement in the tidymodels framework we have used in the machine learning application."
  },
  {
    "objectID": "generalised/introduction.html#introduction",
    "href": "generalised/introduction.html#introduction",
    "title": "Introduction and Theory",
    "section": "",
    "text": "The question in this section is how we should deal with outcome variables that are not continuous or restricted to be positive. I will limit the applications to the most common examples: (1) binary outcome variables and (2) positive outcome variables. Examples of binary outcome variables are the decision to disclose information, or merge a company. Positive outcome variables are variables such as employee salaries or the market value of a company.\nThis is a fairly contentious topic where different literature streams have different expectations and norms for what is appropriate. This is by no means a complete overview of the topic. My aim is to give you my perspective and to give you enough tools to be able to decide what is most appropriate for your research and to explain to assessors why you made those choices.\nThe basic problem is the following. In our typical linear model, the outcome variable can take on any value between \\(-\\infty\\) and \\(+\\infty\\).\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nIf we want to make sure that \\(y_i\\) is restricted to a certain domain, for instance between 0 and 1, we can use a transformation function on the linear model. These transformed linear models are often called generalised linear models.\n\\[\ny_i = g(\\alpha + \\beta x_i + \\epsilon_i)\n\\]\nYou can also think of the inverse of the transformation function as transforming the outcome so that it can take all positive and negative values. This is implicitly what we have done earlier when we used the logarithmic transformation in our earlier discussion on what the appropriate measure is for pay-performance sensitivity. In the second week, we discussed the importance of theory to determine what the right measure is according to our research question. In this week, we are asking the question whether we are interested in \\(y_i\\) or in the transformation \\(g^{-1}(y_i)\\).\n\\[\ng^{-1}(y_i) = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nThe discussion should also remind you of the difference between prediction and parameter estimation at the start of the machine learning chapter. The issue of the mismatch between the domain of the linear model and the outcome variable is less of an issue if we are really interested in estimating the parameter \\(\\beta\\). It really is a problem if we are trying to predict the outcome variable \\(y\\) because we know that we are going to get predictions that are impossible. In the reminder of this chapter I will focus on parameter estimation because that is a more common research question in finance and accounting research. That does not mean that prediction of restricted outcome variables is not important. Indeed, regularised versions of generalised linear models for binary outcome variables are popular classification models. They are fairly easy to implement in the tidymodels framework we have used in the machine learning application."
  },
  {
    "objectID": "generalised/introduction.html#setup",
    "href": "generalised/introduction.html#setup",
    "title": "Introduction and Theory",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "generalised/introduction.html#transformation",
    "href": "generalised/introduction.html#transformation",
    "title": "Introduction and Theory",
    "section": "Transformation",
    "text": "Transformation\nIn this section, I will visualise the most common transformation functions in the accounting and finance literature. I start with the logistic and probit transformations which are commonly used to model binary variables. We can think of those variables as having two values either \\(y_i = 1\\) or \\(y_i = 0\\). In this case, we can model the probability \\(P(y_i = 1) = g(\\alpha + \\beta x_i + \\epsilon_i)\\) and then use a binomial distribution to predict 1s and 0s. The logistic and probit transformation are two possible options for the function \\(g\\).\n\nLogistic Transformation\nThe logistic transformation transforms the linear scale, \\(z_i = \\alpha + \\beta x_i + \\epsilon_i\\), to \\(\\frac{e^{z_i}}{1+ e^{z_i}}\\). R has an inbuilt function plogis that does the transformation for us but you can obviously just write the transformation yourself. The code below shows that you get identical results.\n\nN &lt;- 1001\nlogit &lt;-\n  tibble(\n    linear_scale = seq(from = -10, to = 10, length.out = N)) %&gt;%\n  mutate(\n    logit_probability1 = exp(linear_scale)/(1 + exp(linear_scale)),\n    logit_probability2 = plogis(linear_scale),\n  )\nglimpse(logit)\n\nRows: 1,001\nColumns: 3\n$ linear_scale       &lt;dbl&gt; -10.00, -9.98, -9.96, -9.94, -9.92, -9.90, -9.88, -…\n$ logit_probability1 &lt;dbl&gt; 4.539787e-05, 4.631492e-05, 4.725050e-05, 4.820498e…\n$ logit_probability2 &lt;dbl&gt; 4.539787e-05, 4.631492e-05, 4.725050e-05, 4.820498e…\n\nggplot(logit, aes(y = logit_probability1, x = linear_scale)) +\n  geom_line() +\n  xlab(\"z\") + ylab(\"logistic(z)\")\n\n\n\n\nIn the figure, you can see how the transformed value stays between 0 and 1, exactly as we would want from a probability. Also notice how the impact of an increase of 1 on the linear scale \\(z\\) varies depending on the starting value of \\(z\\). This is because the logistic transformation is an non-linear transformation and it makes the interpretation of the the coefficients in our model more difficult.\n\n\nCumulative Normal or Probit\nThere is another function that is commonly used to transform the linear scale to the probability scale, the cumulative normal or probit function. We need to take a short detour to the normal distribution to explain the cumulative normal distribution to explain this function. You can see a normal distribution below where the lower 90% of the distribution is filled in yellow while the upper 10% is filled in blue. The value for \\(x\\) associated with the 90% is approximately 1.28. That means that we can associate a probability with each value of \\(x\\) which tells us what the probability is that a randomly generated normal value is smaller than \\(x\\). This function is the probit function.\n\ntibble(x = seq(from = -3, to = 3, length.out = N)) %&gt;%\n  mutate(dnorm = dnorm(x),\n         fill = if_else(x &lt; qnorm(0.90), \"below\", \"above\")) %&gt;%\n  ggplot(aes(y = dnorm, x = x, fill = fill)) +\n  geom_area() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Normal Distribution\")\n\n\n\n\nIn the next code, I show the cumulative probability function for a normal distribution with mean 0 and standard deviation 1. You can see that it look quite similar to the logistic transformation. In the next, section we will compare the two transformations directly.\n\ncumul_norm  &lt;- logit %&gt;%\n  mutate(cumul_norm_prob = pnorm(linear_scale))\nggplot(cumul_norm, aes(y = cumul_norm_prob, x = linear_scale)) +\n  geom_line() +\n  xlab(\"z\") + ylab(\"cumulative_normal(z)\")\n\n\n\n\n\n\nComparison\nThe main reason to show the comparison is to highlight that when we divide \\(z\\) by 1.6 the probit transformation is very similar to the logistic transformation. This probit transformation is also equivalent to the cumulative normal probability for a normal distribution with mean 0 and standard deviation 1.6. In other words, the difference between the logistic or probit regression is often inconsequential.\n\ncombination &lt;- cumul_norm %&gt;%\n  mutate(cumul_norm_adj = pnorm(linear_scale/1.6)) %&gt;%\n  select(-logit_probability2) %&gt;%\n  pivot_longer(c(logit_probability1, cumul_norm_prob, cumul_norm_adj))\nggplot(combination, aes(y = value, x = linear_scale,\n                        colour = name)) +\n  scale_color_viridis_d() +\n  geom_line() +\n  xlab(\"z\") + xlab(\"g(z)\") + ggtitle(\"Binary Distribution\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe choice between a logistic regression or a probit regression is often not consequential. If your research question requires you to model a binary variable, the choice between probit or logistic regression can be based on what the preferred choice is in your research area.\n\n\n\n\nPoisson\nThe poisson distribution (rpois) is originally developed to model the number of independent events with a fixed rate happening in a given amount of time. For instance, the number of acquisitions that a firm does in a year or the number of patents they obtained. It is the general workhorse model for any count variable. As, you can see in the figure below, you can think of the poisson distribution as discrete random distribution (the dots) around the the exponentiated linear model (the line). It will be important further on to think of a poisson regression as a model for\n\\[ g^{-1}(E(y_i | x_i)) = \\alpha + \\beta x_i\\]\nwhere \\(g^{-1}(z) = log(z)\\) and \\(g(z) = exp(z)\\). That, is the logarithm of the expected value of \\(y_i\\) is given by \\(\\alpha + \\beta x_i\\). Moreover, in contrast to the exponent, the poisson distribution can give 0 values1.\n\npoisson &lt;-\n  tibble(linear_scale = seq(from = -3, to = 3, length.out = N)) %&gt;%\n  mutate(exp_scale = exp(linear_scale),\n         poisson = rpois(N, exp_scale))\nggplot(poisson, aes(y = exp_scale, x = linear_scale)) +\n  geom_line() +\n  geom_point(aes(y = poisson)) +\n  xlab(\"z\") + ylab(\"g(z)\") + ggtitle(\"Poisson Distribution\")"
  },
  {
    "objectID": "generalised/introduction.html#footnotes",
    "href": "generalised/introduction.html#footnotes",
    "title": "Introduction and Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the first lecture, I transformed a variable on the logarithm scale but that lead to problems with 0 values. I used a common hack by adding 1 to the variable and then take the logarithm. On the next page, we will see that does not always give desirable results.↩︎"
  },
  {
    "objectID": "generated/introduction.html",
    "href": "generated/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This last section is going to bring everything full circle to an extent. The topic of this section is generated variables which are variables that are the result of a regression. For instance, we could use the residuals or predicted values of one regression and use them as variables in an other regression. This is the idea behind two-stage least squares with an instrumental variable, models of abnormal returns, models of abnormal accruals, and a lot of matching approaches and many more in the accounting and finance literature.\nThe issue is that these generated variables might violate some of the implicit or explicit assumptions in the second regression model. I will go over some of the issues that are specific to certain models such as two-stage least squares on this page and the use of generated variables on the next page. The main message of this section is that the properties of these models that exists of more than one regression are not always well understood: the estimates could be biased or the standard software packages could give the wrong standard errors. The solutions I propose in this setting is to use simulations of many datasets to understand whether an approach gives biased estimates and use bootstrapped standard errors to check whether the multi-step approach gives reasonable standard errors. I use plenty of coding examples to illustrate these points."
  },
  {
    "objectID": "generated/introduction.html#introduction",
    "href": "generated/introduction.html#introduction",
    "title": "Introduction",
    "section": "",
    "text": "This last section is going to bring everything full circle to an extent. The topic of this section is generated variables which are variables that are the result of a regression. For instance, we could use the residuals or predicted values of one regression and use them as variables in an other regression. This is the idea behind two-stage least squares with an instrumental variable, models of abnormal returns, models of abnormal accruals, and a lot of matching approaches and many more in the accounting and finance literature.\nThe issue is that these generated variables might violate some of the implicit or explicit assumptions in the second regression model. I will go over some of the issues that are specific to certain models such as two-stage least squares on this page and the use of generated variables on the next page. The main message of this section is that the properties of these models that exists of more than one regression are not always well understood: the estimates could be biased or the standard software packages could give the wrong standard errors. The solutions I propose in this setting is to use simulations of many datasets to understand whether an approach gives biased estimates and use bootstrapped standard errors to check whether the multi-step approach gives reasonable standard errors. I use plenty of coding examples to illustrate these points."
  },
  {
    "objectID": "generated/introduction.html#setup",
    "href": "generated/introduction.html#setup",
    "title": "Introduction",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(modelsummary)\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nlibrary(fixest)\nlibrary(bayesboot)\ni_am(\"generated/introduction.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nset.seed(230383)"
  },
  {
    "objectID": "generated/introduction.html#an-example-2sls",
    "href": "generated/introduction.html#an-example-2sls",
    "title": "Introduction",
    "section": "An example: 2SLS",
    "text": "An example: 2SLS\nIn this example, we are using a standard setting where an instrumental variable, iv, is used to identify the effect of a variable x on y in the presence of annoying confounding. Note that I generate the data in two steps. The initial tibble step simulates the variables that are exogenous and not affected by other parts of the model while the second mutate step simulates the endogenous variables that are causally affected by other parts of the model.\n\nN &lt;- 1e4\ndata_2sls &lt;-\n  tibble(\n    iv = rbinom(N, 1, 0.5),\n    annoying = rnorm(N)\n  ) %&gt;%\n  mutate(\n    x = rnorm(N, 2 * iv - 3 * annoying, 5),\n    y = rnorm(N, 3 * annoying + x, 5)\n  )\n\nNext, we run four different models with this data.\n\nlm1 is the confounded regression.\nlm2 controls for the annoying confounding factor and provides the best possible answer to the question. Unfortunately, in reality we often are not sure that we have all the confounding factors.\ntsls is the standard two-stage least squares estimate with iv as the instrumental variable.\ntsls_manual1 and tsls_manual2 run the two regressions of two-stage least squares manually.\n\n\nlm1 &lt;- lm(y ~ x, data = data_2sls)\nlm2 &lt;- lm(y ~ x + annoying, data = data_2sls)\ntsls &lt;- feols(y ~ 1 | 0 | x ~ iv, data = data_2sls)\ntsls_manual1 &lt;- lm(x ~ iv, data = data_2sls)\ndata_2sls &lt;- data_2sls %&gt;%\n  mutate(fit_x = fitted(tsls_manual1))\ntsls_manual2 &lt;- lm(y ~ fit_x, data = data_2sls)\ncoef_map &lt;- c('x' = 'x', 'annoying' = 'annoying',\n              'fit_x' = 'x')\nmsummary(list(confounded = lm1, ideal = lm2, tsls = tsls,\n              manual = tsls_manual2),\n         gof_map = gof_map, coef_map = coef_map)\n\n\n\n\n\nconfounded\nideal\ntsls\nmanual\n\n\n\n\nx\n0.747\n1.007\n1.040\n1.040\n\n\n\n(0.010)\n(0.010)\n(0.061)\n(0.074)\n\n\nannoying\n\n2.970\n\n\n\n\n\n\n(0.058)\n\n\n\n\nNum.Obs.\n10000\n10000\n10000\n10000\n\n\nR2\n0.378\n0.506\n0.320\n0.019\n\n\n\n\n\n\n\nIn the results, you can notice the following. The confounded regression is biased. We do not get the correct coefficient of \\(\\beta = 1\\). The ideal regression estimates the coefficient precisely with a low standard error. Despite the high number of observations (10^{4}), the two-stage least squares estimates also have an unbiased estimate but the standard error is substantially larger. The manual two-stage least squares model gets exactly the same coefficient but the standard error is larger again.\nIt turns out that the standard error is wrong in the manual version (Gelman and Hill 2006, 223). That is why we should use the fixest package to estimate the two-stage least squares model. The problem is that in our second stage regression we use fit_x as the independent variable which is the predicted value of x in the first stage regression. When lm calculates the standard error of the coefficient, it uses the standard deviation of the residuals in the regression, i.e. the difference between y and the predicted y based on x_fit. However, we know that we should actually use x to predict y. So, we need to adjust the standard errors of the coefficient and that is what the following code does manually.\nThe following code shows how to do that adjustment 1. First, we get the coefficient table from the summary which is just a matrix, where we can access the values based on the name or row/column number of the values that we need. I am extracting the coefficient and the standard error that we are interested in. We can also extract the unadjusted standard deviation of the residuals which is called sigma in the summary. Finally, we calculate the adjusted standard deviation if we use x instead of fit_x to predict y. The last line adjusts the standard error.\n\ncoef_table &lt;- summary(tsls_manual2)$coef\nintercept &lt;- coef_table[\"(Intercept)\", 1]\nbeta &lt;- coef_table[\"fit_x\", 1]\nse &lt;- coef_table[\"fit_x\", 2]\nsd_unadj &lt;- summary(tsls_manual2)$sigma\nsd_adj &lt;- data_2sls %&gt;%\n  mutate(e = y - intercept - beta * x) %&gt;%\n  summarise(sd = sd(e)) %&gt;%\n  pull(sd)\nse * sd_adj / sd_unadj\n\n[1] 0.06145269\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith two-stage least squares, statisticians and econometricians have figured out the correct adjustments to standard errors and they are build into the software packages. For more complicated models with multiple steps this might not always be possible. One possible solution is to bootstrap the standard errors."
  },
  {
    "objectID": "generated/introduction.html#bayesian-bootstrap",
    "href": "generated/introduction.html#bayesian-bootstrap",
    "title": "Introduction",
    "section": "Bayesian Bootstrap",
    "text": "Bayesian Bootstrap\nI have explained the bootstrap before. In this section, I introduce the Bayesian Bootstrap and it’s R implementation 2. In the traditional bootstrap we resample observations from the data with replacement so that we create new datasets which are similar but different from the original data. The disadvantage of the traditional approach is that the implicit weight on an observation in each resampled data is discrete and can be 0. The Bayesian bootstrap bootstrap creates explicit weights based on a Dirichlet distribution which gives a weight between for each observation so that the sum of the weights equals 1.\nWe do not need to know this to use the Bayesian bootstrap. We can just use the bayesboot package. The only thing we need to do is to create a function that returns the estimate that we are interested in based on the data and the weights. We can use the bootstrap function and tell it to start with the data_2sls, resample it R = 1000 times, calculate the beta with get_beta, and use weights in that calculation.\n\nget_beta &lt;- function(data, weights){\n  tsls_manual1 &lt;- lm(x ~ iv, data = data, weights = weights)\n  data$fit_x &lt;- fitted(tsls_manual1)\n  tsls_manual2 &lt;- lm(y ~ fit_x, data = data, weights = weights)\n  beta &lt;- coefficients(tsls_manual2)[\"fit_x\"]\n  return(beta)\n}\nget_beta(data_2sls, weights = rep(1/N, times = N))\n\n  fit_x \n1.03956 \n\nbootstrap &lt;- bayesboot(data_2sls, get_beta, R = 1000,\n                       use.weights = TRUE)\nsummary(bootstrap)\n\nBayesian bootstrap\n\nNumber of posterior draws: 1000 \n\nSummary of the posterior (with 95% Highest Density Intervals):\n statistic     mean         sd   hdi.low hdi.high\n     fit_x 1.038238 0.06325506 0.9135051 1.163498\n\nQuantiles:\n statistic     q2.5%      q25%  median     q75%   q97.5%\n     fit_x 0.9146771 0.9978153 1.03673 1.080914 1.166789\n\nCall:\n bayesboot(data = data_2sls, statistic = get_beta, R = 1000, use.weights = TRUE)"
  },
  {
    "objectID": "generated/introduction.html#the-bayesian-bootstrap-by-hand-and",
    "href": "generated/introduction.html#the-bayesian-bootstrap-by-hand-and",
    "title": "Introduction",
    "section": "The Bayesian Bootstrap by hand and",
    "text": "The Bayesian Bootstrap by hand and\nIf you want to calculate the Bayesian bootstrap by hand, I give the code below. It’s a good introduction on how to run efficient simulations with the tidyverse 3. I need to slightly change the function because we are going to put the simulated weights in the data. The next part is to actual create nsim versions of the data. The trick here is to say that we want the data data_2sls where we repeat each row nsim times.\nThe rest of the code follows more easy to understand patterns. We first create a sim variable so that we can keep track of different data simulations. In this case, the data is the same but we create new weights for each simulation. The weights are derived from a Dirichlet distribution which you can generate by an exponential distribution divided by the sum of the generated values (per simulation). We only keep the variables that are necessary in our get_beta_2 function. Then we create a dataset per simulation with the nest function and calculate the estimate in the last mutate step. And that is the Bayesian bootstrap as a simulation exercise.\n\nget_beta_2 &lt;- function(data){\n  tsls_manual1 &lt;- lm(x ~ iv, data = data, weights = weights)\n  data$fit_x &lt;- fitted(tsls_manual1)\n  tsls_manual2 &lt;- lm(y ~ fit_x, data = data, weights = weights)\n  beta &lt;- coefficients(tsls_manual2)[\"fit_x\"]\n  return(beta)\n}\nnsim &lt;- 1000\nnrows &lt;- nrow(data_2sls)\nbootstrap2 &lt;-\n  data_2sls[rep(1:nrows, times = nsim),] %&gt;%\n  mutate(sim = rep(1:nsim, each = nrows),\n         raw_weights = rexp(nsim *  nrows, 1)) %&gt;%\n  mutate(weights = raw_weights/sum(raw_weights), .by = sim) %&gt;%\n  select(sim, iv, x, y, weights) %&gt;%\n  nest(.by = sim) %&gt;%\n  mutate(beta = map_dbl(data, get_beta_2)) %&gt;%\n  print()\n\n# A tibble: 1,000 × 3\n     sim data                   beta\n   &lt;int&gt; &lt;list&gt;                &lt;dbl&gt;\n 1     1 &lt;tibble [10,000 × 4]&gt; 0.932\n 2     2 &lt;tibble [10,000 × 4]&gt; 1.15 \n 3     3 &lt;tibble [10,000 × 4]&gt; 0.994\n 4     4 &lt;tibble [10,000 × 4]&gt; 1.01 \n 5     5 &lt;tibble [10,000 × 4]&gt; 1.06 \n 6     6 &lt;tibble [10,000 × 4]&gt; 1.12 \n 7     7 &lt;tibble [10,000 × 4]&gt; 1.01 \n 8     8 &lt;tibble [10,000 × 4]&gt; 1.04 \n 9     9 &lt;tibble [10,000 × 4]&gt; 1.07 \n10    10 &lt;tibble [10,000 × 4]&gt; 1.08 \n# ℹ 990 more rows\n\nsummarise(bootstrap2,\n          estimate = mean(beta),\n          se = sd(beta))\n\n# A tibble: 1 × 2\n  estimate     se\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     1.04 0.0622"
  },
  {
    "objectID": "generated/introduction.html#footnotes",
    "href": "generated/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich you should never have to do!↩︎\nYou can also find a thorough discussion here.↩︎\nThis advice is based on the [Grant McDermott’s advice] for very efficient simulations. I did not implement everything but I did use most of the principles in the tidyverse setting.↩︎"
  },
  {
    "objectID": "generated/residual_dependent.html",
    "href": "generated/residual_dependent.html",
    "title": "Generated Independent",
    "section": "",
    "text": "This page is based on Chen, Hribar, and Melessa (2018) who show the potential problems of using a two-stage approach where in the second stage, we use the residuals from the first regression as a dependent variable. This approach is quite popular in the accounting and finance literature. The paper shows formally and with simulation that (1) these models can typically be estimated with 1 regression and that (2) the two-step procedure can be biased if they are not used carefully. The paper also re-analyses a number of accounting studies and shows that the results sometimes meaningfully change.\nI am going to illustrate the issue with a number of simulated examples. As always, we will be interested in the effect of a variable x on y where we want to control for a variable z. The bias of the two-stage procedure will often depend on unobserved correlations between those variables which I will model with the unobserved variable w.\nThe two-step approach is to: - First run a regression y ~ z and calculate the residuals. - Second use those residuals to estimate the effect of x on the residuals residuals ~ x\nThe one-step approach is to just run the regression y ~ x + z."
  },
  {
    "objectID": "generated/residual_dependent.html#introduction",
    "href": "generated/residual_dependent.html#introduction",
    "title": "Generated Independent",
    "section": "",
    "text": "This page is based on Chen, Hribar, and Melessa (2018) who show the potential problems of using a two-stage approach where in the second stage, we use the residuals from the first regression as a dependent variable. This approach is quite popular in the accounting and finance literature. The paper shows formally and with simulation that (1) these models can typically be estimated with 1 regression and that (2) the two-step procedure can be biased if they are not used carefully. The paper also re-analyses a number of accounting studies and shows that the results sometimes meaningfully change.\nI am going to illustrate the issue with a number of simulated examples. As always, we will be interested in the effect of a variable x on y where we want to control for a variable z. The bias of the two-stage procedure will often depend on unobserved correlations between those variables which I will model with the unobserved variable w.\nThe two-step approach is to: - First run a regression y ~ z and calculate the residuals. - Second use those residuals to estimate the effect of x on the residuals residuals ~ x\nThe one-step approach is to just run the regression y ~ x + z."
  },
  {
    "objectID": "generated/residual_dependent.html#setup",
    "href": "generated/residual_dependent.html#setup",
    "title": "Generated Independent",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(cowplot)\ntheme_set(theme_cowplot(font_size = 18))\nlibrary(broom)\nlibrary(here)\ni_am(\"generated/residual_dependent.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(modelsummary)\n\n\nN &lt;- 1000\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generated/residual_dependent.html#correlated-step-1-control",
    "href": "generated/residual_dependent.html#correlated-step-1-control",
    "title": "Generated Independent",
    "section": "Correlated Step 1 control",
    "text": "Correlated Step 1 control\nThe basic problem is that in the two-step approach researchers regularly include z in the first regression and not in the second regression. If z and x are correlated, the two-step procedure will be biased downwards if the second step does not control for z. This can be seen in the simulated example below. I also show that the bias can be avoided by running the one-step regression, add z as a control variable, or use z to residualise both x and y.\n\nd1 &lt;- tibble(w = rnorm(N)) %&gt;%\n  mutate(z = rnorm(N, w, 1),\n         x = rnorm(N, w, 1),\n         y = rnorm(N, x + z, 10))\nlm1 &lt;- lm(y ~ x + z, data = d1)\nlm1x &lt;- lm(y ~ z, data = d1)\nlm1y &lt;- lm(x ~ z, data = d1)\nd1 &lt;- mutate(d1,\n             resid_y = resid(lm1x),\n             resid_x = resid(lm1y))\nlm1resy &lt;- lm(resid_y ~ x, data = d1)\nlm1resyz &lt;- lm(resid_y ~ x + z, data = d1)\nlm1resyx &lt;- lm(resid_y ~ resid_x, data = d1)\nmodelsummary(list(onestep = lm1, no_control = lm1resy,\n                  with_control = lm1resyz, double_resid = lm1resyx),\n             gof_map = gof_map)\n\n\n\n\n\nonestep\nno_control\nwith_control\ndouble_resid\n\n\n\n\n(Intercept)\n−0.070\n0.001\n0.002\n0.000\n\n\n\n(0.305)\n(0.306)\n(0.305)\n(0.305)\n\n\nx\n1.294\n0.938\n1.294\n\n\n\n\n(0.255)\n(0.218)\n(0.255)\n\n\n\nz\n0.495\n\n−0.672\n\n\n\n\n(0.253)\n\n(0.253)\n\n\n\nresid_x\n\n\n\n1.294\n\n\n\n\n\n\n(0.255)\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n\n\nR2\n0.052\n0.018\n0.025\n0.025"
  },
  {
    "objectID": "generated/residual_dependent.html#extra-controls-in-step-2",
    "href": "generated/residual_dependent.html#extra-controls-in-step-2",
    "title": "Generated Independent",
    "section": "Extra controls in step 2",
    "text": "Extra controls in step 2\nIn the next step, we can include extra controls, z2, in the second step. The bias will now depend on the correlations that the extra control has with the first stage controls and x and y. More importantly, the bias can now be an overestimation or underestimation. If the additional control is not correlated to x or z1, we still have the downward bias from before.\n\nd2 &lt;- d1 %&gt;%\n  rename(z1 = z) %&gt;%\n  mutate(z2 = rnorm(N, 0, 1),\n         y = rnorm(N, x + z1 + z2, 10))\nlm2 &lt;- lm(y ~ x + z1 + z2, data = d2)\nlm2y &lt;- lm(y ~ z1, data = d2)\nd2 &lt;- d2 %&gt;%\n  mutate(resid_y = resid(lm2y))\nlm2resy &lt;- lm(resid_y ~ x + z2, data = d2)\nmodelsummary(list(onestep = lm2, twostep = lm2resy),\n             gof_map = gof_map)\n\n\n\n\n\nonestep\ntwostep\n\n\n\n\n(Intercept)\n0.123\n0.000\n\n\n\n(0.320)\n(0.321)\n\n\nx\n0.961\n0.682\n\n\n\n(0.269)\n(0.229)\n\n\nz1\n1.438\n\n\n\n\n(0.266)\n\n\n\nz2\n0.938\n0.929\n\n\n\n(0.318)\n(0.318)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.089\n0.018"
  },
  {
    "objectID": "generated/residual_dependent.html#extra-correlated-controls-in-step-2",
    "href": "generated/residual_dependent.html#extra-correlated-controls-in-step-2",
    "title": "Generated Independent",
    "section": "Extra correlated controls in step 2",
    "text": "Extra correlated controls in step 2\nHowever, when the additional control is negatively correlated to both z1 and x but positively to y, we get an upward bias.\n\nd3 &lt;- d2 %&gt;%\n  mutate(z3 = rnorm(N, - 2 * w, 1),\n         y = rnorm(N, x + z1 + z3, 10)) %&gt;%\n  mutate(resid_y = resid(lm(y ~ z1, data = .)))\nlm3 &lt;- lm(y ~ x + z1 + z3, data = d3)\nlm3resy &lt;- lm(resid_y ~ x + z3, data = d3)\nmodelsummary(list(onestep = lm3, twostep = lm3resy),\n             gof_map = gof_map)\n\n\n\n\n\nonestep\ntwostep\n\n\n\n\n(Intercept)\n0.080\n−0.012\n\n\n\n(0.321)\n(0.321)\n\n\nx\n0.929\n1.009\n\n\n\n(0.297)\n(0.291)\n\n\nz1\n0.773\n\n\n\n\n(0.306)\n\n\n\nz3\n0.850\n0.717\n\n\n\n(0.207)\n(0.180)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.021\n0.017"
  },
  {
    "objectID": "generated/residual_dependent.html#run-simulations-as-a-large-dataframe",
    "href": "generated/residual_dependent.html#run-simulations-as-a-large-dataframe",
    "title": "Generated Independent",
    "section": "Run simulations as a large dataframe",
    "text": "Run simulations as a large dataframe\nWe can use a simulation approach to see whether that bias is persistent and not just a coincedence in the simulated data. First, we create the two functions that run the two step and one step approach and return the estimate and the p-value from the coefficient table.\n\ntwo_step &lt;- function(data){\n  lm1 &lt;- lm(y ~ z1, data)\n  lm2 &lt;- lm(resid(lm1) ~ x + z3, data = data)\n  coefs &lt;- summary(lm2)$coefficients\n  result &lt;- coefs[\"x\", c(1, 4)]\n  names(result) &lt;- c(\"estimate\", \"pvalue\")\n  return(result)\n}\none_step &lt;- function(data){\n  lm &lt;- lm(y ~ x + z1 + z3, data = data)\n  coefs &lt;- summary(lm)$coefficients\n  result &lt;- coefs[\"x\", c(1, 4)]\n  names(result) &lt;- c(\"estimate\", \"pvalue\")\n  return(result)\n}\ntwo_step(d3)\n\n    estimate       pvalue \n1.0094585099 0.0005442113 \n\none_step(d3)\n\n   estimate      pvalue \n0.929023153 0.001822299 \n\n\nNext, we set up the simulation for 1000 simulations with 100 observations per data set. One trick for efficient simulations is to generate the data in one big data frame. You can see that we can still use the two step approach with tibble and mutate for exogenous and endogenous variables. We just need to be careful with the number of observations and use ntotal.\n\nN &lt;- 100\nnsim &lt;- 1000\nntotal &lt;- nsim * N\nsimdata &lt;-\n  tibble(\n    sample = rep(1:nsim, each = N),\n    w = rnorm(ntotal)) %&gt;%\n  mutate(\n    z1 = rnorm(ntotal, w, 1),\n    x = rnorm(ntotal, w, 1),\n    z3 = rnorm(ntotal, - 2 * w, 1),\n    y = rnorm(ntotal, x + z1 + z3, 10))\n\nWe can than use nest to create separate data by sample and calculate the estimate and p-value for each sample with our functions. The results are a vector with two values and we use unnest_wider to create separate columns. Finally, I calculate the difference between the two estimates.\n\nresults &lt;- simdata %&gt;%\n  nest(.by = sample) %&gt;%\n  mutate(two = map(.x = data, .f = ~ two_step(.x)),\n         one = map(.x = data, .f = ~ one_step(.x))) %&gt;%\n  select(-data) %&gt;%\n  unnest_wider(c(one, two), names_sep = \"_\") %&gt;%\n  mutate(bias = two_estimate - one_estimate)\n\nNext, we can summarise the results and we see that the two step approach is likely to overestimate the effect of x.\n\nresults %&gt;%\n  summarise(M = mean(bias), se = sd(bias)/sqrt(n()), \n            M_one = mean(one_estimate), M_two = mean(two_estimate))\n\n# A tibble: 1 × 4\n       M      se M_one M_two\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.0860 0.00427  1.02  1.10\n\n\nThe last part of the code calculates the percentage of simulated samples that gives a significant effect. If we are worried that we might not find a significant effect while there is one, this might be a legitimate problem. In this case, the bad approach is more likely to report a significant effect. It’s not always easy doing the right thing.\n\nresults %&gt;% select(sample, two_pvalue, one_pvalue) %&gt;%\n  pivot_longer(c(two_pvalue, one_pvalue),\n               names_to = \"var\", values_to = \"pvalue\") %&gt;%\n  mutate(is_sign = if_else(pvalue &lt; 0.05, 1, 0)) %&gt;%\n  summarise(mean = mean(is_sign), .by = var)\n\n# A tibble: 2 × 2\n  var         mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 two_pvalue 0.21 \n2 one_pvalue 0.173"
  }
]
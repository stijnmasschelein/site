[
  {
    "objectID": "freaky_friday/descriptive.html",
    "href": "freaky_friday/descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"freaky_friday/descriptive.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nmain <- readRDS(here(\"data\", \"freaky_friday\", \"main.RDS\")) %>%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\nglimpse(main)\n\nRows: 130,807\nColumns: 23\n$ ticker       <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0G\",…\n$ actual       <dbl> -0.08, -0.11, -0.11, -0.05, -0.07, -0.04, -0.10, -0.45, -…\n$ pdf          <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ gvkey        <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 10560, 88784, 8…\n$ cusip        <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ anndat       <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ N            <int> 4, 2, 3, 2, 4, 1, 5, 2, 2, 4, 5, 1, 1, 5, 3, 2, 2, 2, 1, …\n$ median       <dbl> -0.07425, -0.08965, -0.05740, -0.03700, -0.10610, -0.0900…\n$ mean         <dbl> -0.07262500, -0.08965000, -0.06800000, -0.03700000, -0.10…\n$ mean_days    <dbl> 11.500000, 8.000000, 6.666667, 13.500000, 7.000000, 21.00…\n$ car_short    <dbl> -5.311022e-02, 3.646200e-02, -6.360508e-02, -4.176757e-03…\n$ car_long     <dbl> -0.335180518, -0.209598864, 0.056618491, -0.183680852, 0.…\n$ date_minus5  <date> 2006-04-21, 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-…\n$ date         <date> 2006-04-21, 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-…\n$ prc          <dbl> 4.28, 5.89, 4.53, 5.00, 3.26, 5.63, 4.02, 14.50, 8.66, 8.…\n$ market_value <dbl> 1883949.09, 2592630.69, 1993992.84, 2200875.00, 1434970.5…\n$ surprise     <dbl> -0.0013434579, -0.0034550086, -0.0116114785, -0.002600000…\n$ weekday      <ord> Wed, Wed, Wed, Wed, Wed, Thu, Wed, Tue, Tue, Thu, Fri, Mo…\n$ group        <chr> \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         <dbl> 2006, 2005, 2005, 2005, 2005, 2004, 2006, 2001, 2005, 200…"
  },
  {
    "objectID": "freaky_friday/descriptive.html#quantiles",
    "href": "freaky_friday/descriptive.html#quantiles",
    "title": "Descriptive statistics",
    "section": "Quantiles",
    "text": "Quantiles\n\nquantiles <- main %>%\n  mutate(sign = case_when(surprise > 0 ~ \"positive\",\n                          surprise < 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %>%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %>%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %>%\n  glimpse()\n\nRows: 130,807\nColumns: 26\n$ ticker       <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0G\",…\n$ actual       <dbl> -0.08, -0.11, -0.11, -0.05, -0.07, -0.04, -0.10, -0.45, -…\n$ pdf          <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ gvkey        <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 10560, 88784, 8…\n$ cusip        <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ anndat       <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ N            <int> 4, 2, 3, 2, 4, 1, 5, 2, 2, 4, 5, 1, 1, 5, 3, 2, 2, 2, 1, …\n$ median       <dbl> -0.07425, -0.08965, -0.05740, -0.03700, -0.10610, -0.0900…\n$ mean         <dbl> -0.07262500, -0.08965000, -0.06800000, -0.03700000, -0.10…\n$ mean_days    <dbl> 11.500000, 8.000000, 6.666667, 13.500000, 7.000000, 21.00…\n$ car_short    <dbl> -5.311022e-02, 3.646200e-02, -6.360508e-02, -4.176757e-03…\n$ car_long     <dbl> -0.335180518, -0.209598864, 0.056618491, -0.183680852, 0.…\n$ date_minus5  <date> 2006-04-21, 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-…\n$ date         <date> 2006-04-21, 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-…\n$ prc          <dbl> 4.28, 5.89, 4.53, 5.00, 3.26, 5.63, 4.02, 14.50, 8.66, 8.…\n$ market_value <dbl> 1883949.09, 2592630.69, 1993992.84, 2200875.00, 1434970.5…\n$ surprise     <dbl> -0.0013434579, -0.0034550086, -0.0116114785, -0.002600000…\n$ weekday      <ord> Wed, Wed, Wed, Wed, Wed, Thu, Wed, Tue, Tue, Thu, Fri, Mo…\n$ group        <chr> \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         <dbl> 2006, 2005, 2005, 2005, 2005, 2004, 2006, 2001, 2005, 200…\n$ sign         <chr> \"negative\", \"negative\", \"negative\", \"negative\", \"positive…\n$ quintile     <int> 3, 2, 1, 3, 5, 5, 2, 2, 5, 4, 2, 1, 5, 2, 1, 1, 4, 5, 4, …\n$ quantile     <dbl> 3, 2, 1, 3, 11, 11, 2, 2, 11, 10, 2, 1, 11, 2, 6, 6, 4, 5…\n\n\nThis is a quick version of Figure 1a. It can be further cleaned up with a better axis labels. It shows the main results from Dellavigna and Pollet (2009) that the market reaction is subdued on Fridays.\n\nggplot(quantiles,\n       aes(y = car_short, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun.y = mean, geom = \"line\") +\n  scale_color_grey()\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nThis is how I would program Figure 1a and b together. It’s a good example of how using pivot_longer can make your life easier. In this case, if we need to plot multiple similar variables.\n\nquantiles %>%\n  pivot_longer(c(car_short, car_long), values_to = \"car\", names_to = \"window\") %>%\n  mutate(fig_name = if_else(window == \"car_short\", \"Figure 1a\", \"Figure 1b\")) %>%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ fig_name, nrow = 2)"
  },
  {
    "objectID": "freaky_friday/regressions.html",
    "href": "freaky_friday/regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(modelsummary)\ngof_omit <- \"Adj|RMS|IC\"\ni_am(\"freaky_friday/regressions.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-2",
    "href": "freaky_friday/regressions.html#table-2",
    "title": "Regressions",
    "section": "Table 2",
    "text": "Table 2\nThe tables do not really replicate which is interesting to me. For a number of reasons.\n\nThe results are more consistent. I wonder whether I got rid of more outliers earlier. Remember I did end up with less observations. One interpretation is that I have cleaned the data better, the other is that I got rid of important, influential observations by being too strict when cleaning the data.\nThe results for the short term CAR are consistent with the figure. Friday market reactions to bottom quantile surprises are more positive than non-friday market reactions and the sign flips for top quantile surprises.\nI also lose substantially more observations due to the inclusion of the volatility measures. I do not know exactly why that is the case.\n\n\nPanel A: Short Term CAR\n\nsubset <- readRDS(here(\"data\", \"freaky_friday\", \"subset.RDS\"))\n#| label: table2a\nmodel1a <- feols(car_short ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2a <- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3a <- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,841 observations removed because of NA values (Fixed-effects: 6,841).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1a, model2a, model3a), gof_omit = gof_omit, stars = TRUE)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    −0.036*** \n     \n     \n  \n  \n     \n    (0.001) \n     \n     \n  \n  \n    friday \n    0.014*** \n    0.012*** \n    0.013** \n  \n  \n     \n    (0.003) \n    (0.003) \n    (0.004) \n  \n  \n    top \n    0.061*** \n     \n     \n  \n  \n     \n    (0.002) \n     \n     \n  \n  \n    friday × top \n    −0.023*** \n    −0.020*** \n    −0.021*** \n  \n  \n     \n    (0.004) \n    (0.004) \n    (0.005) \n  \n  \n    Num.Obs. \n    22495 \n    22495 \n    15654 \n  \n  \n    R2 \n    0.086 \n    0.095 \n    0.110 \n  \n  \n    R2 Within \n     \n    0.001 \n    0.001 \n  \n  \n    Std.Errors \n    by: anndat \n    by: anndat \n    by: anndat \n  \n  \n    FE: size_decile \n     \n    X \n    X \n  \n  \n    FE: year \n     \n    X \n    X \n  \n  \n    FE: month \n     \n    X \n    X \n  \n  \n    FE: vol_decile \n     \n     \n    X \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\n\n\nPanel B: Long Term CAR\n\nmodel1b <- feols(car_long ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2b <- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3b <- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,841 observations removed because of NA values (Fixed-effects: 6,841).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1b, model2b, model3b), gof_omit = gof_omit, stars = TRUE)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    −0.022*** \n     \n     \n  \n  \n     \n    (0.005) \n     \n     \n  \n  \n    friday \n    −0.012 \n    −0.012 \n    −0.022 \n  \n  \n     \n    (0.013) \n    (0.013) \n    (0.015) \n  \n  \n    top \n    0.037*** \n     \n     \n  \n  \n     \n    (0.004) \n     \n     \n  \n  \n    friday × top \n    0.041** \n    0.043** \n    0.052** \n  \n  \n     \n    (0.015) \n    (0.014) \n    (0.017) \n  \n  \n    Num.Obs. \n    22495 \n    22495 \n    15654 \n  \n  \n    R2 \n    0.006 \n    0.035 \n    0.041 \n  \n  \n    R2 Within \n     \n    0.001 \n    0.001 \n  \n  \n    Std.Errors \n    by: anndat \n    by: anndat \n    by: anndat \n  \n  \n    FE: size_decile \n     \n    X \n    X \n  \n  \n    FE: year \n     \n    X \n    X \n  \n  \n    FE: month \n     \n    X \n    X \n  \n  \n    FE: vol_decile \n     \n     \n    X \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-3",
    "href": "freaky_friday/regressions.html#table-3",
    "title": "Regressions",
    "section": "Table 3",
    "text": "Table 3\n\nmain_extra <- main %>%\n  mutate(log_size = log(market_value))  %>%\n  mutate(log_size_adj = log_size - mean(log_size, na.rm = T),\n         .by = c(quarter, year)) %>%\n  mutate(size_decile = ntile(log_size_adj, 10))\n\nmodel1 <- feols(car_short ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel2 <- feols(car_short ~ friday + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\nmodel3 <- feols(car_long ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel4 <- feols(car_long ~ friday  + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\n\nmsummary(list(model1, model2, model3, model4), gof_omit = gof_omit, stars = TRUE)\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n      (4) \n  \n \n\n  \n    (Intercept) \n    −0.041*** \n     \n    −0.015*** \n     \n  \n  \n     \n    (0.001) \n     \n    (0.002) \n     \n  \n  \n    friday \n    0.016*** \n    0.014*** \n    −0.016* \n    −0.015* \n  \n  \n     \n    (0.002) \n    (0.002) \n    (0.008) \n    (0.007) \n  \n  \n    quantile \n    0.006*** \n     \n    0.003*** \n     \n  \n  \n     \n    (0.000) \n     \n    (0.000) \n     \n  \n  \n    friday × quantile \n    −0.002*** \n    −0.002*** \n    0.003** \n    0.003*** \n  \n  \n     \n    (0.000) \n    (0.000) \n    (0.001) \n    (0.001) \n  \n  \n    Num.Obs. \n    130807 \n    130807 \n    130807 \n    130807 \n  \n  \n    R2 \n    0.054 \n    0.057 \n    0.002 \n    0.015 \n  \n  \n    R2 Within \n     \n    0.000 \n     \n    0.000 \n  \n  \n    Std.Errors \n    by: anndat \n    by: anndat \n    by: anndat \n    by: anndat \n  \n  \n    FE: size_decile \n     \n    X \n     \n    X \n  \n  \n    FE: year \n     \n    X \n     \n    X \n  \n  \n    FE: month \n     \n    X \n     \n    X \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"
  },
  {
    "objectID": "method_package.html",
    "href": "method_package.html",
    "title": "Content",
    "section": "",
    "text": "The slides contain the lecture slides for the first half of the semester.\n\nSlides 1\nSlides 2\nSlides 3\nSlides 4\nSlides 5"
  },
  {
    "objectID": "method_package.html#freaky-friday",
    "href": "method_package.html#freaky-friday",
    "title": "Content",
    "section": "Freaky Friday",
    "text": "Freaky Friday\nContains an attempt at a replication from downloading the data to analyzing for Dellavigna and Pollet (2009).\nIntroduction to the Replication"
  },
  {
    "objectID": "freaky_friday/index.html",
    "href": "freaky_friday/index.html",
    "title": "Research Design",
    "section": "",
    "text": "This website is an attempt to replicate the main results in Dellavigna and Pollet (2009) from scratch. The paper is a good example because (1) it has an explicit theoretical model, (2) provides excellent descriptions on how the different measures are constructed, (3) uses the canonical finance design, an event study. I will focus most of my attention on (2) and (3) but (1) is important because it provides guidance to readers of the paper why the measures and the design is important.\nThe basic argument of the paper is that firms will bury earnings announcements on Fridays if the earnings are bad because the market pays less attention to news on Fridays."
  },
  {
    "objectID": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "href": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "title": "Research Design",
    "section": "The Unexpected Component of Earnings",
    "text": "The Unexpected Component of Earnings\n\\[ s_{t,k} = \\frac{e_{t,k} - \\hat{e}_{t,k}}{P_{t,k}} \\]\nIn this equation, \\(s_{t,k}\\) is the surprise (i.e. the unexpected component) in earnings of company \\(k\\) at time \\(t\\). It is calculated by the actual earnings per share, \\(e_{t,k}\\), minus the median expected earnings by analysts, \\(\\hat{e}_{t,k}\\), divided by the price of the stock 5 days before the earnings release, \\(P_{t,k}\\). You will see over and over that empirical researchers are wary that the day(s) just before an announcement might be special, i.e. the news might have already leaked out, for good and less good reasons. So, instead of using the price the day before the earnings release, the paper picks a couple of days earlier 2.\nThe most important part is that we try to filter out all the information in the earnings announcement that is already known to the market by subtracting the earnings estimates of analysts. The implicit assumption is that these earnings estimates are a good measure of the market’s information on the company just before the earnings are announced."
  },
  {
    "objectID": "freaky_friday/index.html#the-market-reaction",
    "href": "freaky_friday/index.html#the-market-reaction",
    "title": "Research Design",
    "section": "The Market Reaction",
    "text": "The Market Reaction\nThe market reaction is calculated as the abnormal return from day \\(h\\) to day \\(H\\). 3\n\\[ R_{t,k}^{(h, H)} = [\\Pi_{j=h}^H (1 + R_{j,k})] - 1\n- \\hat{\\beta}_{t,k} [\\Pi_{j=h}^H (1 + R_{j,m}) - 1]\\]\nThis looks complicated but it is quite simple. The first part is the raw return of the stock over the period that we are interested in 4. The second part is the return of the market times the sensitivity of the stock to the market. The latter, \\(\\hat{\\beta}_{t,k}\\) is estimated with data from before the announcement. The goal of this approach is to filter out other reasons that the stock price might go up or down because of general economic or financial events that affect the stock market as a whole.\nSpecifically, we estimate the following regression model with data from days, \\(u\\), with \\(u\\) between 46 and 300 days before the earnings announcements. This is a regression of the market return on the firm return.\n\\[ R_{u,k} = \\alpha_{t,k} + \\beta_{t,k} R_{u,m}\\]\nAgain, we are using data from long before the earnings announcement so that our estimate is not contaminated by the earnings announcement 5.\nThere are lot of different approaches in the literature to estimate these abnormal returns but they all have the same flavour of trying to filter out other reasons why the stock price might be moving. In a pure regression framework, we would include additional variables as control variables. Constructing variables like the abnormal returns and the earnings surprise like this serves exactly the same function. There are good reasons to use the approach of first constructing the measures as precise as possible in an event study design like this but there are some problem with applying this same logic in different research designs Chen, Hribar, and Melessa (n.d.)."
  },
  {
    "objectID": "freaky_friday/download_linking.html",
    "href": "freaky_friday/download_linking.html",
    "title": "WRDS linking data",
    "section": "",
    "text": "I use three packages on this page and two of them require some more explanation. The here package helps with managing the different files in this larger project. I can refer to different files relative to the root folder all the files are in. The only thing that I need to do is to say where this file is compared to the root folder with the i_am function. The second package is the RPostgres package that helps make a connection with the WRDS data sources."
  },
  {
    "objectID": "freaky_friday/download_linking.html#ibes",
    "href": "freaky_friday/download_linking.html#ibes",
    "title": "WRDS linking data",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nThe SQL code instructs the WRDS data base to get the variables ticker, cusip (another identifier), cname (company name), and sdates (the start date for this ticker) from the ibes.idsum (IBES ID summary) database of WRDS. In this paper, we only want U.S. firms.\n\nSELECT ticker, cusip, cname, sdates\nFROM ibes.idsum\nWHERE usfirm = 1\n\nWith some R code, we clean the data and save it as a file in the data > wrds folder in our main folder.\n\nibes_id <- as_tibble(ibes_query) %>%\n  rename_all(tolower)\nsaveRDS(ibes_id, here(\"data\", \"wrds\", \"ibes_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#crsp",
    "href": "freaky_friday/download_linking.html#crsp",
    "title": "WRDS linking data",
    "section": "CRSP",
    "text": "CRSP\nFrom the CRSP data, we get the permno and ncusip identifier where ncusip stands for the same cusip identifier as mentioned above. We also have the company name, start date, and end date.\n\nSELECT permno, ncusip, comnam, st_date, end_date\nFROM crsp.stocknames\n\n\ncrsp_id <- as_tibble(crsp_query) %>%\n  rename_all(tolower)\nsaveRDS(crsp_id, here(\"data\", \"wrds\", \"crsp_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-ibes",
    "href": "freaky_friday/download_linking.html#compustat-with-ibes",
    "title": "WRDS linking data",
    "section": "Compustat with I/B/E/S",
    "text": "Compustat with I/B/E/S\nFrom Compustat we use the security file which has all the financial securities (and their identifiers) that are linked to the firms in Compustat. We select all the variables from that dataset. We only select the ones where the ibes ticker is available so that we can match via the ticker in the I/B/E/S files.\n\nSELECT *\nFROM comp.security\nWHERE ibtic IS NOT NULL\n\n\ncompu_security <- as_tibble(compu_security_query) %>%\n  rename_all(tolower)\nsaveRDS(compu_security, here(\"data\", \"wrds\", \"compu_security.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-crsp",
    "href": "freaky_friday/download_linking.html#compustat-with-crsp",
    "title": "WRDS linking data",
    "section": "Compustat with CRSP",
    "text": "Compustat with CRSP\nFinally, we get the linking file in compustat. According to the documentation, not all the links are reliable and they advice to use the linktype variable and the usedflag variable to filter only the links that are most reliable. I have implemented the rules that follow best practice according to this tutorial (https://wrds-www.wharton.upenn.edu/pages/wrds-research/applications/linking-databases/linking-crsp-and-compustat/)\n\nSELECT gvkey, linktype, usedflag, liid, lpermno, linkdt, linkenddt\nFROM crsp.Ccmxpf_linktable\n\n\ncrsp_compu <- as_tibble(compu_query) %>%\n  rename_all(tolower) %>%\n  select(gvkey, linktype, usedflag, iid = liid, permno = lpermno, stdt = linkdt, enddt = linkenddt) %>%\n  filter(!is.na(permno), linktype %in% c(\"LU\", \"LC\"), usedflag == 1) %>%\n  select(gvkey, permno, stdt, enddt) %>%\n  distinct()\nsaveRDS(crsp_compu, here(\"data\", \"wrds\", \"crsp_compu.RDS\"))"
  },
  {
    "objectID": "freaky_friday/linking.html",
    "href": "freaky_friday/linking.html",
    "title": "Combining databases",
    "section": "",
    "text": "The packages are the same as before."
  },
  {
    "objectID": "freaky_friday/linking.html#linking-ibes",
    "href": "freaky_friday/linking.html#linking-ibes",
    "title": "Combining databases",
    "section": "Linking I/B/E/S",
    "text": "Linking I/B/E/S\n\nlinking_table %>%\n  select(gvkey, ticker) %>%\n  distinct() %>%\n  summarise(N = n(), .by = ticker) %>%\n  filter(N > 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: ticker <chr>, N <int>\n\n\nThere are no tickers linked with multiple gvkeys. This means that left_join from I/B/E/S is the way to start the joining process. That way, there will be no duplicate matches from Compustat."
  },
  {
    "objectID": "freaky_friday/download_data.html",
    "href": "freaky_friday/download_data.html",
    "title": "Earnings announcements",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/download_data.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\nlibrary(dtplyr)\nlinking_table <- readRDS(here(\"data\", \"freaky_friday\", \"linking_table.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_data.html#ibes",
    "href": "freaky_friday/download_data.html#ibes",
    "title": "Earnings announcements",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nWe start with the earnings announcement data from I/B/E/S with the analyst estimates. According to the method section in Dellavigna and Pollet (2009), we need the data from the start of 1995 to the middle of 2006. We will want the analyst estimates for all the firms with a ticker in the master linking_table.\nI am going to use parameters that we can calculate or set in R and then pass them on to the SQL query. The details are explained in this blogpost by Irene Steves.\n\nbegin_date <- \"'1995-01-01'\"\nend_date <- \"'2006-07-01'\"\ntickers <- unique(linking_table$ticker)\ntickers_sql <- glue::glue_sql(\"{tickers*}\", .con = wrds)\n\nThe dates of the estimate and the actual earnings announcement will be critical to construct unexpected component of the earnings and to determine the exact event data, i.e. the date that (the unexpected component of) the earnings are announced. Thankfully, WRDS provides a description of the date variables. anndats is the first day that an analyst set their estimate for the earnings per share and the revdats is the last day that the analyst confirmed their estimate. We will use revdats as the defacto date that the analyst provided the estimate. anndats_act is the earnings announcement date. value is the estimated EPS by the analyst and actual is the actual EPS as announced by the firm. pdf is flag whether the EPS if for the primary share class or on a diluted basis. I included both and that is probably appropriate for this paper. fpi is the forecast period indicator if we set this to “6”, we get the earnings estimates that are done in the quarter before the earnings announcements. All these variables can be verified in the data descriptions on WRDS. As you can see, it’s quite important if you work with data that you have not collected yourself to read the data descriptions.\nIn the actual sql query you can see that the I use the parameters that I constructed before in R by adding an ? infront of the parameter name when using them.\n\nSELECT ticker, cusip, fpi, anndats, revdats, pdf, value, anndats_act, actual, analys\nFROM ibes.det_epsus\nWHERE anndats_act BETWEEN ?begin_date AND ?end_date\nAND actual IS NOT NULL\nAND fpi = '6'\nAND ticker IN (?tickers_sql)\n\nWe save the data with R. See the previous page for how to get the output of an sql query into R.\n\nann_ibes <- as_tibble(ibes_query) %>%\n  rename_all(tolower)\nsaveRDS(ann_ibes, here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\nglimpse(ann_ibes)\n\nRows: 1,046,677\nColumns: 10\n$ ticker      <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\"…\n$ cusip       <chr> \"0039241X\", \"0039241X\", \"0039241X\", \"0039241X\", \"0039241X\"…\n$ fpi         <chr> \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\"…\n$ anndats     <date> 2006-02-02, 2006-02-01, 2006-02-01, 2006-02-01, 2006-02-0…\n$ revdats     <date> 2006-02-20, 2006-02-06, 2006-02-06, 2006-02-06, 2006-02-0…\n$ pdf         <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\"…\n$ value       <dbl> -0.0523, -0.0785, -0.0800, -0.0785, -0.0960, -0.0800, -0.0…\n$ anndats_act <date> 2006-04-26, 2006-04-26, 2006-04-26, 2006-04-26, 2006-04-2…\n$ actual      <dbl> -0.0800, -0.0800, -0.0800, -0.0800, -0.0800, -0.0800, -0.0…\n$ analys      <dbl> 43594, 84303, 87125, 5469, 44775, 478, 43594, 87125, 71785…"
  },
  {
    "objectID": "freaky_friday/download_data.html#compustat",
    "href": "freaky_friday/download_data.html#compustat",
    "title": "Earnings announcements",
    "section": "Compustat",
    "text": "Compustat\nFollowing the paper, we will verify the earnings announcement date in I/B/E/S with the earnings announcement date in Compustat. Given the importance of finding the exact date for an event study, it is not surprising that Dellavigna and Pollet (2009) spent a lot of effort to make sure that they have the date right.\n\ngvkeys <- unique(linking_table$gvkey)\ngvkeys_sql <- glue::glue_sql(\"{gvkeys*}\", .con = wrds)\n\nrdq is the earnings announcement data in Compustat.\n\nSELECT cusip, rdq, gvkey\nFROM comp.fundq\nWHERE rdq BETWEEN ?begin_date AND ?end_date\nAND gvkey IN (?gvkeys_sql)\n\n\nann_compu <- as_tibble(compu_query) %>%\n  rename_all(tolower) %>%\n  mutate(cusip = str_sub(cusip, 1, 8))\nsaveRDS(ann_compu, here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nglimpse(ann_compu)\n\nRows: 328,000\nColumns: 3\n$ cusip <chr> \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"000…\n$ rdq   <date> 1995-03-15, 1995-07-06, 1995-09-13, 1995-12-12, 1996-03-14, 199…\n$ gvkey <chr> \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001…"
  },
  {
    "objectID": "freaky_friday/download_data.html#combine-announcements",
    "href": "freaky_friday/download_data.html#combine-announcements",
    "title": "Earnings announcements",
    "section": "Combine Announcements",
    "text": "Combine Announcements\nTo combine the two datasets, we will link them through a simplified version of the larger linking table. I will also enforce that the first 6 characters of cusip are the same. I don’t think it is strictly necessary to do that but it does gives us more confidence that the links are of higher quality. We need to match the I/B/E/S data and the Compustat data based on the firm and its earnings announcement date. However, if you read the paper (Dellavigna and Pollet 2009), you will notice that the reason why want to combine is because the date in both datasets does not always match. The paper gets around that by matching earnings announcements if the date is not more than 5 days apart in the two data sources. This is why I create anndat_begin and anndat_end to define the interval in which we want to match the data. Finally, we can calculate the actual event date as the minimum of the date in the I/B/E/S data and the Compustat data (Dellavigna and Pollet 2009) 1.\nYou can also see that I have two lines of commented code. In these lines, I read in the datasets again. This is not strictly necessary to make this file fully reproducible but it does make debugging the code easier. If I want to make some changes to the code I do not have to download the data again from WRDS. I can just use the one in the data folder.\n\n# ann_ibes <- readRDS(here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\n# ann_compu <- readRDS(here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nsimple_link <- linking_table %>%\n  select(ticker, gvkey, permno, cusip) %>%\n  mutate(cusip = str_sub(cusip, end = 6)) %>%\n  distinct()\nearn_ann <- ann_ibes %>% distinct(ticker, actual, pdf, cusip, anndats_act) %>%\n  mutate(cusip = str_sub(cusip, end = 6)) %>%\n  left_join(simple_link, by = join_by(ticker)) %>%\n  filter(!is.na(gvkey), !(cusip.x != cusip.y)) %>%\n  select(-starts_with(\"cusip\")) %>%\n  mutate(anndat_begin = anndats_act - 5, anndat_end = anndats_act + 5) %>%\n  left_join(ann_compu, by = join_by(gvkey == gvkey,\n                                    anndat_begin <= rdq,  anndat_end >= rdq)) %>%\n  filter(!is.na(rdq)) %>%\n  mutate(anndat = pmin(anndats_act, rdq)) %>%\n  select(-anndat_begin, -anndat_end)\n\nWarning in left_join(., simple_link, by = join_by(ticker)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 175 of `x` matches multiple rows in `y`.\nℹ Row 11209 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nsaveRDS(earn_ann, here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nglimpse(earn_ann)\n\nRows: 154,540\nColumns: 9\n$ ticker      <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0G\", …\n$ actual      <dbl> -0.08, -0.11, -0.11, -0.05, -0.07, -0.04, -0.10, -0.45, -0…\n$ pdf         <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\"…\n$ anndats_act <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-2…\n$ gvkey       <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081\"…\n$ permno      <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 10560, 88784, 88…\n$ cusip       <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410\"…\n$ rdq         <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-2…\n$ anndat      <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-2…\n\n\nearn_ann serves as a linking table to match different earnings announcements (as opposed to firms and their securities in linking_table)\n\n\n\nvariable\ndata source\ndescription\n\n\n\n\nticker\nI/B/E/S\nIdentifier\n\n\nanndats_act\nI/B/E/S\nActual Announcement Date\n\n\ngvkey\nCompustat\nIdentifier\n\n\npermno\nCRSP\nIdentifier\n\n\ncusip\n\nIdentifier of length 6,8 or 9\n\n\nrdq\nCompustat\nActual Announcement Date\n\n\nanndat\n\npmin(rdq, anndats_act)\n\n\n\nanndat is the validated way of calculating the announcement date. However, we need to keep the other dates around because we will need them to link back to the original databases.\nThe paper states that they have 154,051 earnings announcements (Dellavigna and Pollet 2009). We have 154540 earnings announcements."
  },
  {
    "objectID": "freaky_friday/download_stocks.html",
    "href": "freaky_friday/download_stocks.html",
    "title": "Stock price data",
    "section": "",
    "text": "On this page, we download the stock price data so that we can later calculate the abnormal return after the earnings announcements.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\ni_am(\"freaky_friday/download_stocks.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nearn_ann <- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\n\n\nwrds <- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  user='stimas',\n                  sslmode='require')\n\nThis section sets the sql parameters. The beginning date is 300 days before the first earnings announcement and the end date is 75 days after the last earnings announcement. Finally, I keep the permno identifiers because these are the only stocks we want the data from.\n\ncrsp_input <- earn_ann %>%\n  summarise(begin = min(anndat) - 300, end = max(anndat) + 75, .by = permno) %>%\n  glimpse()\n\nRows: 8,759\nColumns: 3\n$ permno <dbl> 10560, 88784, 10574, 80585, 87832, 84606, 88790, 29058, 87771, …\n$ begin  <date> 2003-12-26, 2001-01-17, 2001-07-04, 2003-05-06, 2003-05-06, 20…\n$ end    <date> 2006-07-10, 2006-07-18, 2006-07-24, 2006-04-11, 2005-01-16, 20…\n\nbegin_date <- min(crsp_input$begin)\nend_date <- max(crsp_input$end)\npermno_sql <- glue::glue_sql(\"{crsp_input$permno*}\", .con = wrds)\n\nI use the same syntax as before to call the WRDS databases as before with sql interspersed with the R parameters created in the previous code block. We get the daily volume, return, price, shares outstanding, cumulative factor to adjust price, and cumulative factor to adjust shares. The latter two are adjustment factors for stock splits and dividends which we probably will not need but if we do we have them.\nThis is by far the largest download from WRDS and this is why it has it’s own page. We do not want to rerun this more than strictly necessary.\n\nSELECT permno, date, vol, ret, prc, shrout, cfacpr, cfacshr\nFROM crsp_a_stock.dsf\nWHERE permno IN (?permno_sql)\nAND date BETWEEN ?begin_date AND ?end_date\n\n\nall_stocks <- as_tibble(crsp_query) %>%\n  rename_all(tolower)\nprint(all_stocks)\n\n# A tibble: 14,973,747 × 8\n   permno date          vol      ret   prc shrout cfacpr cfacshr\n    <dbl> <date>      <dbl>    <dbl> <dbl>  <dbl>  <dbl>   <dbl>\n 1  10002 1994-03-10    700 -0.00952 13      2999   1.5     1.5 \n 2  10010 1994-03-10  65220 -0.0161   7.62   9348   1.1     1.1 \n 3  10011 1994-03-10  31600  0.0179   7.12   5303   1       1   \n 4  10012 1994-03-10  46000 -0.0500   2.38  14581   1       1   \n 5  10019 1994-03-10   2100  0        8.75   5238   1.5     1.5 \n 6  10025 1994-03-10  16304 -0.00685 18.1    7329   1       1   \n 7  10026 1994-03-10  23525 -0.00680 18.2   10341   2       2   \n 8  10032 1994-03-10   4269  0       16.2    6456   4       4   \n 9  10035 1994-03-10 268078  0.0282  18.2   13029   1       1   \n10  10042 1994-03-10 103900  0        3.56  25318   0.25    0.25\n# ℹ 14,973,737 more rows\n\nsaveRDS(all_stocks, here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\n\nOne important footnote is that the price is negative on days where there were no trades. This might be important going forward."
  },
  {
    "objectID": "freaky_friday/abnormal_returns.html",
    "href": "freaky_friday/abnormal_returns.html",
    "title": "Abnormal returns",
    "section": "",
    "text": "Setup\nThe lubridate package is the tidyverse package that helps with time related data. Dates are a specific class of variables and the package helps with managing date variables.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/abnormal_returns.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nIn order to predict the expected market return reaction to earnings, we will want to take into account the general market reaction. Dellavigna and Pollet (2009) only adjust for the overall market return. There are other adjustments possible for specific risk factors. The original factor model is the Fama-French 3 Factors model and the data is available via the Kenneth French data library.\nI have downloaded the data as a .csv file. The code reads the data in skipping 5 lines and reading the variables as numbers (double precision). We then need to transform the date variable to a date type with a function from the lubridate package. Finally, we need to scale the returns by 100 because they are expressed in percentages. For replicating the Dellavigna and Pollet (2009) paper, we only need the market return minus the risk free rate (mkt_rf) and the risk free rate (rf). If I read this paper correctly, we need to use the raw market return which is calculated as mkt.\n\nearn_ann <- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nanalyst <- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\"))\nall_stocks <- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench <- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %>%\n  mutate(date = ymd(date)) %>%\n  mutate_if(is.numeric, ~ . / 100) %>%\n  mutate(mkt = mkt_rf + rf) %>% \n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   <date>       <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\n\n\nshrout is shares outstanding in 1000\nmarket_value is in million USD\n\n\n\nAbnormal returns\nRemember that we are interested in predicting the counterfactual of what the returns would have been if there were no earnings announcement. We use the following prediction model:\n\\[ R_{u,k} = \\alpha_{t,k} + \\beta_{t,k} R_{u,m}\\]\nThat means that we need to estimate \\(\\alpha\\) and \\(\\beta\\) for each announcement based on the market return and firm return from 300 days before the earnings announcement to 46 days before the earnings announcements (Dellavigna and Pollet 2009). This is a lot of computations!\nI played around with a lot of different implementations and finally settled on a completely tidyverse style implementation as the fastest on my machine. I did not include the other implementations in this document but I will show how you can test the timing of your code with the microbenchmark package.\nThe first thing we need to do is to write the code as a function that we can call as many times as we need. 1 I call this function create_coefs. The input of the function is n, the number of earnings announcements we want to run the code for. The output is a tibble with the coefficients \\(\\alpha\\) and \\(\\beta\\) for each permno and anndat combination.\nThe actual function takes the earnings announcement data and keeps the unique permno and anndat combinations. Next, the start and end data for the return data is calculated based on the earnings announcement date. Next, we merge the return data and the Fama-French data and delete the missing or infinite observations for the return.\n\n\n\n\n\n\nNote\n\n\n\nAt this point, you can see the flexibility of my approach to keep the data sets separate until we need to join them. The earnings announcement data is one row per earnings announcement date per firm. The stock data has one observation per firm per day. The Fama-French data has one observation per day. Because these data have a different level of analysis, I have kept them separate until the point that we need to analyse the data. If I have to update one of the datasets, I do not necessarily have to update all the other ones. I would strongly encourage you to do the same thing where you keep different parts of the data cleaning separate for as long as possible. This will make your code flexible and easy to adapt.\n\n\nThe next two steps are the most advanced ones. First, remark that we have multiple rows for each earnings announcement. We use the summarise function to put all the returns in a vector y and we make a matrix X with all 1’s in the first column and the mkt. Remark that we are wrapping y and X in a list so that we effectively make y and X list columns in our tibble. The tidyverse lets us put almost anything in a dataset as long as we wrap it in a list. Finally, we summarise the y and X not just for the whole dataset but for each combination of permno and anndat. Thus, we end up with a dataset with a row for each earnings announcement with the returns we want to predict (y) from before the earnings announcement and the market returns from the same time period (X).\nIn the second step, we will run the regression for each row to estimate \\(\\alpha\\) and \\(\\beta\\). We do use a lightweight version of lm called lm.fit to speed up the estimation. lm.fit calculates less statistics like the standard errors that we need to calculate the p-values. Because ultimately, we need to this for 150,000 rows lm.fit will save considerable time compared to lm. lm.fit works slightly different in that now regression equation is specified. You need to give the predictors as the first argument and the outcome variable as the second argument. This is the reason why we formed the y and X variable in the previous step. We just need the coefficients form the fit object and we can use the coef function to extract them.\nFinally, we want to apply this lm.fit function to every row (i.e. for every earnings announcement). We use the map-family to apply a function to every element of a column. The pmap function specifically let us apply a function to multiple columns if we wrap them in a list. So, pmap takes the list of columns X and y and applies (~) the function lm.fit to the first (..1) and second (..2) element of that list. At the end, we extract the coefficients.\nThe microbenchmark function allows us to test the time it takes to run this function for 100, 1000, and 10000 earnings announcements. The computations are done 10 times for each call to get an average/median estimate 2. The surprising thing to me was that the function scales very well. The average time per announcement goes down with more announcements.\n\ncreate_coefs <- function(n = 6){\n  earn_ann %>% head(n = n) %>%\n    distinct(permno, anndat) %>%\n    mutate(start = anndat - 300, end = anndat - 46) %>%\n    left_join(select(all_stocks, permno, date, ret),\n              by = join_by(permno == permno, start <= date, end >= date)) %>%\n    left_join(select(famafrench, mkt, rf, date),\n              by = join_by(date == date)) %>%\n    filter(!is.na(ret), !is.infinite(ret)) %>%\n    summarise(y = list(cbind(ret)), X = list(cbind(alpha = 1, beta = mkt)),\n              .by = c(permno, anndat)) %>%\n    mutate(coefs = pmap(list(X, y), ~ lm.fit(..1, ..2) %>% coef()),\n           .by = c(permno, anndat)) %>%\n    select(-y, -X)\n}\n\nmicrobenchmark::microbenchmark(\n                  create_coefs(100),\n                  create_coefs(1000),\n                  create_coefs(10000),\n                  times = 10)\n\nUnit: seconds\n                expr      min       lq     mean   median       uq      max\n   create_coefs(100) 1.409918 1.474293 1.534156 1.535219 1.607870 1.634923\n  create_coefs(1000) 1.624458 1.651582 1.684543 1.674286 1.704302 1.793788\n create_coefs(10000) 3.250091 3.267919 3.321967 3.312994 3.351636 3.473761\n neval\n    10\n    10\n    10\n\n\nNext, we calculate the coefficients for all announcements and save them in the results object. Next, we need to calculate the abnormal returns based on the following formula in Dellavigna and Pollet (2009).\n\\[ R_{t,k}^{(h, H)} = [\\Pi_{j=h}^H (1 + R_{j,k})] - 1\n- \\hat{\\beta}_{t,k} [\\Pi_{j=h}^H (1 + R_{j,m}) - 1]\\]\nIn the results, the coef column is a list column where each row has two elements: alpha and beta. The unnest_wider function splits the list columns in two columns alpha and beta. Next we create the date 75 days after the announcement because that is the longest time frame the paper is interested (Dellavigna and Pollet 2009). Just like before, we add the relevant stock price data and Fama-French data. Our dataset now has a row for each day 0 to 75 days after the announcement date. Dellavigna and Pollet (2009) are interested in the short term abnormal return on the day of the announcement and the day after (0-1 days) and the long term abnormal return (2-75 days). I create the time_frame variable to denote whether a day should be counted in the short or long cumulative abnormal return. As an intermediate step, I calculate the products of the (market) returns + 1 (see the equation) by announcement and time frame (and beta because we need it for further calculations).\nFinally, I use the pivot_wider function to transform the dataset from the long format to a wider format. Specifically, each announcement has two rows one for the short term car and one for the long term car. The transformation will create a new dataset where every row has a different announcement with a car_long and a car_short. We do that by taking the values_from the car variable and the name for the column from the time_frame variable.\n\nN <- nrow(earn_ann)\nresults <- create_coefs(N)\n\nabnormal <- results %>%\n  unnest_wider(coefs) %>%\n  mutate(date75 = anndat + 75) %>%\n  left_join(select(all_stocks, permno, date, ret),\n            by = join_by(permno == permno,\n                         date75 >= date, anndat <= date)) %>%\n  left_join(select(famafrench,  mkt, rf, date),\n            by = join_by(date == date)) %>%\n  mutate(time_frame = if_else(date - anndat <= 1, \"short\", \"long\")) %>%\n  summarise(raw = prod(1 + ret), mkt = prod(1 + mkt),\n            .by = c(permno, anndat, time_frame, beta)) %>%\n  mutate(car = raw - 1 - beta * (mkt - 1)) %>%\n  select(-beta, -raw, -mkt) %>%\n  filter(!is.na(car)) %>%\n  pivot_wider(values_from = car, names_from = time_frame,\n              names_prefix = \"car_\")\nglimpse(abnormal)\n\nRows: 128,114\nColumns: 4\n$ permno    <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 10560, 88784, 8878…\n$ anndat    <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26,…\n$ car_short <dbl> -0.053110221, 0.036461999, -0.063605082, -0.004176757, -0.01…\n$ car_long  <dbl> -0.33518052, -0.20959886, 0.05661849, -0.18368085, 0.2074188…\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pivot_wider and pivot_longer functions are really, really useful when you are working with poorly structured data. As a rule of thumb when you clean and summarise the data, you will want the data to be in a long format where each row is its own distinct unit of analysis (Wickham 2014). For instance, in this code we had a row for each return day after the announcement for each announcement. The big advantage is that we can easily, with code, change the groups of data that we want to summarise, mutate, or filter. If we want to change what the short time frame means, we can just change that one line of code (e.g. to data - anndat <= 5). The wider format is more useful to present the data. That is why I use it more often at the end of an analysis.\n\n\n\n\nPutting it all together\nFinally, we want to put all the data together. You can see that we only merge all the data at the end. This is good practice. I would advise to not even try to build up a complete dataset by starting with for instance the I/B/E/S data and than to gradually add the variables that you need. This is a very error prone process and it is hard to make changes without redoing the whole data cleaning process.\nFirst, we need more stock price data and the market value from the CRSP data. We collect this in the clean_prices dataset. Remark that I remove the data where the price is negative because that means that there were no trades that day. Dellavigna and Pollet (2009) require the price and market value 5 days before the earnings announcement. My guess is that they are trying to avoid any effects of information leaking out into the market before the earnings announcement. I then merge the earnings announcement data with the analyst estimates data, the abnormal return data and the price and market value data. Finally, I calculate the earnings surprise as the difference between the actual and the median predicted earnings per share divided by the price per share to get the earnings surprise per dollar of market value.\n\nclean_prices <- all_stocks %>%\n  filter(prc > 0) %>%\n  select(permno, date, prc, shrout) %>%\n  mutate(market_value = prc * shrout) %>%\n  select(-shrout) %>%\n  print()\n\n# A tibble: 14,339,244 × 4\n   permno date         prc market_value\n    <dbl> <date>     <dbl>        <dbl>\n 1  10002 1994-03-10 13          38987 \n 2  10010 1994-03-10  7.62       71278.\n 3  10011 1994-03-10  7.12       37784.\n 4  10012 1994-03-10  2.38       34630.\n 5  10019 1994-03-10  8.75       45832.\n 6  10025 1994-03-10 18.1       132838.\n 7  10026 1994-03-10 18.2       188723.\n 8  10032 1994-03-10 16.2       104910 \n 9  10035 1994-03-10 18.2       237779.\n10  10042 1994-03-10  3.56       90195.\n# ℹ 14,339,234 more rows\n\nsurprise <- earn_ann %>%\n  left_join(analyst,\n            by = join_by(ticker, anndat, actual, pdf)) %>%\n  left_join(abnormal,\n            by = join_by(permno, anndat)) %>%\n  mutate(date_minus5 = anndat - 5) %>%\n  left_join(clean_prices,\n            by = join_by(permno, closest(date_minus5 >= date))) %>%\n  mutate(surprise = (actual - median) / prc) \nglimpse(surprise)\n\nRows: 154,540\nColumns: 20\n$ ticker       <chr> \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0G\",…\n$ actual       <dbl> -0.08, -0.11, -0.11, -0.05, -0.07, -0.04, -0.10, -0.45, -…\n$ pdf          <chr> \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ gvkey        <chr> \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       <dbl> 10560, 10560, 10560, 10560, 10560, 10560, 10560, 88784, 8…\n$ cusip        <chr> \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ anndat       <date> 2006-04-26, 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-…\n$ N            <int> 4, 2, 3, 2, 4, 1, 5, 2, 2, 4, 5, NA, NA, NA, NA, NA, 1, 1…\n$ median       <dbl> -0.07425, -0.08965, -0.05740, -0.03700, -0.10610, -0.0900…\n$ mean         <dbl> -0.072625, -0.089650, -0.068000, -0.037000, -0.100550, -0…\n$ mean_days    <dbl> 11.500000, 8.000000, 6.666667, 13.500000, 7.000000, 21.00…\n$ car_short    <dbl> -0.053110221, 0.036461999, -0.063605082, -0.004176757, -0…\n$ car_long     <dbl> -0.33518052, -0.20959886, 0.05661849, -0.18368085, 0.2074…\n$ date_minus5  <date> 2006-04-21, 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-…\n$ date         <date> 2006-04-21, 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-…\n$ prc          <dbl> 4.28, 5.89, 4.53, 5.00, 3.26, 5.63, 4.02, 14.50, 8.66, 8.…\n$ market_value <dbl> 1883949.09, 2592630.69, 1993992.84, 2200875.00, 1434970.5…\n$ surprise     <dbl> -0.001343458, -0.003455009, -0.011611478, -0.002600000, 0…\n\n\n\n\nData Cleaning\nFinally, I apply the data cleaning rules from Dellavigna and Pollet (2009). I also include the a print statement so that we can track how many observations are removed after each data cleaning step. The following rules are applied:\n- Remove the observations where `surprise` is missing.\n- Remove the observations where the median or actual estimate is larger than the price.\n- Remove penny stocks which I arbitrarily and after some googling decide to be stocks with a price lower than $2 per stock.\n- Remove the observations with earnings announcement on Saturday and Sunday.\n- I remove the observations with a car in the top and bottom .05% of observations.\n\nwinsorise <- 5/10000\nmain <- surprise %>%\n  filter(!is.na(surprise)) %>% print(n = 0) %>%\n  filter(abs(median) < prc, abs(actual) < prc) %>% print(n = 0) %>%\n  filter(prc > 2) %>% print(n = 0) %>%\n  mutate(weekday = wday(anndat, label = TRUE)) %>%\n  filter(! weekday %in% c(\"Sat\", \"Sun\")) %>% print(n = 0) %>%\n  filter(percent_rank(car_long) >= winsorise,\n         percent_rank(car_long) <= 1 - winsorise,\n         percent_rank(car_short) >= winsorise,\n         percent_rank(car_short) <= 1 - winsorise) %>%\n  print()\n\n# A tibble: 137,403 × 20\n# ℹ 137,403 more rows\n# ℹ 20 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>\n# A tibble: 136,029 × 20\n# ℹ 136,029 more rows\n# ℹ 20 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>\n# A tibble: 133,703 × 20\n# ℹ 133,703 more rows\n# ℹ 20 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>\n# A tibble: 133,611 × 21\n# ℹ 133,611 more rows\n# ℹ 21 variables: ticker <chr>, actual <dbl>, pdf <chr>, anndats_act <date>,\n#   gvkey <chr>, permno <dbl>, cusip <chr>, rdq <date>, anndat <date>, N <int>,\n#   median <dbl>, mean <dbl>, mean_days <dbl>, car_short <dbl>, car_long <dbl>,\n#   date_minus5 <date>, date <date>, prc <dbl>, market_value <dbl>,\n#   surprise <dbl>, weekday <ord>\n# A tibble: 130,807 × 21\n   ticker actual pdf   anndats_act gvkey  permno cusip    rdq        anndat    \n   <chr>   <dbl> <chr> <date>      <chr>   <dbl> <chr>    <date>     <date>    \n 1 A2      -0.08 D     2006-04-26  001081  10560 00392410 2006-04-26 2006-04-26\n 2 A2      -0.11 D     2005-01-26  001081  10560 00392410 2005-01-26 2005-01-26\n 3 A2      -0.11 D     2005-04-27  001081  10560 00392410 2005-04-27 2005-04-27\n 4 A2      -0.05 D     2005-07-27  001081  10560 00392410 2005-07-27 2005-07-27\n 5 A2      -0.07 D     2005-10-26  001081  10560 00392410 2005-10-26 2005-10-26\n 6 A2      -0.04 D     2004-10-21  001081  10560 00392410 2004-10-21 2004-10-21\n 7 A2      -0.1  D     2006-02-01  001081  10560 00392410 2006-02-01 2006-02-01\n 8 AA0G    -0.45 P     2001-11-13  133724  88784 00724X10 2001-11-13 2001-11-13\n 9 AA0G    -0.3  D     2005-03-01  133724  88784 00724X10 2005-03-01 2005-03-01\n10 AA0G    -0.31 D     2005-05-05  133724  88784 00724X10 2005-05-05 2005-05-05\n# ℹ 130,797 more rows\n# ℹ 12 more variables: N <int>, median <dbl>, mean <dbl>, mean_days <dbl>,\n#   car_short <dbl>, car_long <dbl>, date_minus5 <date>, date <date>,\n#   prc <dbl>, market_value <dbl>, surprise <dbl>, weekday <ord>\n\nsaveRDS(main, here(\"data\", \"freaky_friday\", \"main.RDS\"))\n\n\n\nOverview of the datasets\n\n\n\nFile\nDescription\n\n\n\n\nearn_ann.RDS\nThe information on the earnings announcements\n\n\nanalyst.RDS\nThe analyst estimates data\n\n\nmain.RDS\nThe main dataset to replicate the paper\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\nFootnotes\n\n\nR is fundamentally a programming language that is build around functions. The tidyverse partially works around that by making tibbles the primary object. Nevertheless, when you need to do some more advanced programming, creating functions is quite natural in R.↩︎\nFor comparison on my 2020 Mac Mini, it takes about 1.5 seconds for 100 announcements and 4.0 seconds for 10000 announcements. On my 2014 Macbook pro, that is respectively 4.3 seconds and 11.2 seconds.↩︎"
  },
  {
    "objectID": "slides/slides1.html#how-to-do-empirical-research",
    "href": "slides/slides1.html#how-to-do-empirical-research",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "How to do empirical research",
    "text": "How to do empirical research\n\nThe connection between theory and the observed data\nThe connection between practical knowledge and what you can investigate\nThe appropriate statistical tests and code\n\n\nEmpirical just means that some data in the broadest possible be sense will be collected or generated. The emphasis on the units will be on the practical and statistical issues of the data analysis part of a thesis. The influence will not be on the accounting and finance part of your thesis. The goal is to be relevant to everyone in the unit. This would be a good time to figure out whether students do a study with a more qualitative approach."
  },
  {
    "objectID": "slides/slides1.html#different-modules",
    "href": "slides/slides1.html#different-modules",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Different Modules",
    "text": "Different Modules\n\nThe research process and basic data skills in R (week 1 - 3)\nResearch design (week 4 - 6)\nAdvanced regression (week 7 - 9)\nTime series analysis (10 - 12)\n\n\nAll modules work as stand-alone units and aim to cover a wide range of topics. Not all of them will in the end be relevant for everyone. However, it is probably a good idea to get to know the different statistical methods, their advantages, and disadvantages. Bringing a new methodology to an old topic can be a valuable contribution. Some problems have already been solved in other research streams."
  },
  {
    "objectID": "slides/slides1.html#assessment",
    "href": "slides/slides1.html#assessment",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Assessment",
    "text": "Assessment\n\nNo Exam\n(Almost weekly) Assignments (70%)\n\n\n- Homework: Practical Issues (0%) (3 March)\n- Assignment 1: Theory and Regressions (10%) (10 March)\n- Assignment 2: Regression and control variables (10%) (17 March)\n- Assignment 3: Research Design (10%) (7 April)\n- Assignment 4: Event Study (10%) (28 April)\n- Assignment 5: Machine Learning (10%) (5 May)\n- Assignment 6: Simulation Research Design (20%) (26 May)\n\n\nProposal and presentation (30%)\n\n\n- Pitch (10%) (31 March)\n- Proposal (10%) (12 May)\n- Presentation (10%) (Probably Thursday 20 July)\n\n\nWe want you to (1) do some data analysis and (2) be well prepared to undertake (the data analysis part of) a research project. So we are going to evaluate you by letting you (1) analyse data and (2) prepare your honours thesis."
  },
  {
    "objectID": "slides/slides1.html#the-first-two-weeks",
    "href": "slides/slides1.html#the-first-two-weeks",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "The first two weeks",
    "text": "The first two weeks\nCEO compensation\n\nThis is not my area of expertise!\nI am not a specialist in the topic nor in this type of data analysis. CEO compensation is something that people in finance, accounting, economics, and outside of academia are interested in. The topic is probably the one with the most commonality. I am comforable with these type of economic theories and I am going to stress the role of theory in data analysis a lot. Some of you will have a topic that is at first sight less theory driven or rely more strongly on very specific knowledge about your setting. I am going to try to convince you that it is going to be useful to think about the underlying story that you are testing."
  },
  {
    "objectID": "slides/slides1.html#topic",
    "href": "slides/slides1.html#topic",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Topic",
    "text": "Topic\nCompletely based on Edmans and Gabaix (2016) in Journal of Economic Literature.\n\nThe level of CEO compensation\nCEO incentives\n\n\nI am going to focus on two topics. 1. How high can we expect the total compensation of a CEO to be (compared to other CEOs) based on some simple economic assumptions. Too high CEO compensation is sometimes seen as a signal of bad corporate goverance. To measure what ‘too high’ means, we first need to establish a baseline of normal levels of compensation. 2. How should CEOs be incentivised: equity or options? How schould we measure whether CEOs have appropriate incentives: $ for $ increases, % for % increases? Incentives are a big topic in Accounting and Finance."
  },
  {
    "objectID": "slides/slides1.html#firm-production-function",
    "href": "slides/slides1.html#firm-production-function",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Firm production function",
    "text": "Firm production function\n\\[\nV = T^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n                 \\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\]\n\\[\n\\alpha_T + \\alpha_K + \\alpha_L = 1\n\\]\n\n\\(V =\\) The value of the firm\n\\(K =\\) Capital of the firm\n\\(L =\\) Labour of the firm\n\\(T =\\) CEO talent/skills/ability/experience\n\n\nWe assume that there is nothing in the structure of the production function that favours a particular firm size, i.e. constant returns to scale."
  },
  {
    "objectID": "slides/slides1.html#ceo-decision",
    "href": "slides/slides1.html#ceo-decision",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "CEO decision",
    "text": "CEO decision\n\\[\n\\max_{K, L} W_T = V - w_L L - rK\n\\]\n\n\\(W_T =\\) wage for CEO with talent T\n\\(w_L =\\) labour unit costs\n\\(r =\\) cost of capital (or return on capital for investors)\n\n\nThe CEO maximises their income \\(W_T\\) by attracting capital at a cost, \\(r\\), and and hiring labour at a wage, \\(w_L\\). The model assumes that the CEO takes the ultimate decision. As it turns out when you assume competitive labour and financial markets, that assumptions does not really matter a lot.\nThis model is too simple to capture reality perfectly. However, that is not the goal of the model and of this exercise. The idea is to see whether we can find a reasonable baseline for CEO compensation that we can test against the data."
  },
  {
    "objectID": "slides/slides1.html#relation-between-size-and-ceo-wage",
    "href": "slides/slides1.html#relation-between-size-and-ceo-wage",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Relation between size and CEO wage",
    "text": "Relation between size and CEO wage\n\\[\nW_T = \\alpha_T V\n\\]\n\nIn this model, the driving force is that more talented CEOs grow the business to a bigger size and they earn more money when they create more value.\n\n\nFirst find the optimal level of capital …\n\\[\n\\begin{aligned}\n\\frac{\\partial W_T}{\\partial K} &=  T^{\\alpha_T}  \n\\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K - 1}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L} - r = 0\n\\\\\nT^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\frac{\\alpha_K}{K}\n&= r\n\\\\\n\\frac{V}{r} &= \\frac{K}{\\alpha_K}\n\\end{aligned}\n\\]\n… and labour\n\\[\n\\begin{aligned}\n\\frac{\\partial W_T}{\\partial K} &=  T^{\\alpha_T}  \n\\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L - 1} - w_L\n\\\\\nT^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n\\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\frac{\\alpha_L}{L} &= w_L\n\\\\\n\\frac{V}{w_L} &= \\frac{L}{\\alpha_L}\n\\end{aligned}\n\\]\nNow we can plugin \\(L\\) and \\(K\\) in \\(V\\) …\n\\[\n\\begin{align}\nV = T^{\\alpha_T} \\Bigl( \\frac{V}{r} \\Bigl) ^{\\alpha_K}\n\\Bigl( \\frac{V}{w_L} \\Bigl) ^{\\alpha_L}\n\\\\\nV^{1 - \\alpha_K - \\alpha_L} = \\frac{T^{\\alpha_T}}\n{r^{\\alpha_K} w_L^{\\alpha_L}}\n\\\\\nV^{\\alpha_T} = \\frac{T^{\\alpha_T}}\n{r^{\\alpha_K} w_L^{\\alpha_L}}\n\\\\\nV = \\frac{T}\n{r^{\\frac{\\alpha_K}{\\alpha_T}} w_L^{\\frac{\\alpha_L}{\\alpha_T}}}\n\\end{align}\n\\]\n… and in \\(W_T\\).\n\\[\n\\begin{align}\nW_T = V - V \\alpha_K - V\\alpha_L = (1 - \\alpha_K - \\alpha_L) V\n= \\alpha_T V\n\\end{align}\n\\]\nI like the basic intuition and deriviation of the model. The derivation is straightforward and (some of) the implicit assumptions are relatively easy to accept. The effect of the CEO depends on the size of the firm (\\(V\\)). When there is more capital and labour available a more talented CEO will have a bigger impact. The model also predicts a clear quantitative relationship between firm size, \\(V\\), and CEO compensation, \\(W_T\\), i.e. that relationship should be linear. This is a nice result that we can test with data. In contrast to the linear relationship between firm size and CEO talent. We can measure \\(V\\) but not \\(T\\)."
  },
  {
    "objectID": "slides/slides1.html#data-compensation-value",
    "href": "slides/slides1.html#data-compensation-value",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Data: Compensation-Value",
    "text": "Data: Compensation-Value\n\n\n\n\n\nThe data is downloaded from Compustat and Execucomp. A lot of you will use these are similar databases in your research project. I did not clean or check the data for this exercise. In your own project, you should show a better understanding of how the data are gathered and what they include than what I am displaying here.\n\nCEO compensation is fairly complete. It includes changes in the value of equity and options.\nMarket value also includes all outstanding financial instruments on the company.\n\nThe qualitative relationship holds quite well. Bigger companies have CEOs with higher compensation. However, the relationship is far from linear and looks more like a power function. Clearly there are other effects at play. In this sample, the power coefficient is 0.31. Prior studies have found a coefficient more closely to 0.33 (Baker, Jensen, and Murphy 1988). Remember that in our setup the CEO can grow the firm at will by attracting more capital and more labour. That assumption is probably too strong."
  },
  {
    "objectID": "slides/slides1.html#the-research-process",
    "href": "slides/slides1.html#the-research-process",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "The research process",
    "text": "The research process\n\n\n\n\n\n\nSummary\n\n\n\nMake assumptions\nDerive relationship between measurable quantities\nCompare the theory and the data\n\n\n\n\n\n\nNote what we have just done. We started with some assumptions about the production function of a company and competitive markets to find the theoretical relation between firm size and CEO compensation. We followed up by testing this theory to data from S&P500 firms. These are the steps that you should be following."
  },
  {
    "objectID": "slides/slides1.html#literature-search",
    "href": "slides/slides1.html#literature-search",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Literature search",
    "text": "Literature search\n\nGoogle Scholar\nUWA One Search\nWeb of Knowledge\nEBSCOhost Research Databases\nSocial Science Research Network\nNational Bureau of Economic Research\n\n\nIn the CEO compensation case above, we derived the theoretical prediction. Normally, you will build on prior theoretical and empirical research to build predictions. The\nIn most cases (ssrn is the exception), you will have to be on the university’s network if you want to actually read the full paper.\n\nGoogle Scholar is probably the most comprehensive repository. This search engine work very similar to regular Google search. There are some additional tricks you can use “author:lastname-firstname” will help you to narrow down papers from a specific author. “intitle:keyword” let’s you search for keywords in the title of papers. You can also narrow down your search based on year of publication. The advanced search features hidden in the left side bar give you additional options such as searching for certain journals. If you are on the university network, Google Scholar will tell you for every paper\nOnesearch is the university search engine. It’s the best way to figure out whether there is an easily accessible version of the paper.\nWebofknowledge and EBSCOhost are two publisher driven initiatives. They work pretty well. Each with their own quirks.\nSSRN (Social Science Research Network) and NBER (National Bureau of Economic Research) both provide access to their own not-yet-peer-reviewed paper repositories. Here you go to find cutting edge research."
  },
  {
    "objectID": "slides/slides1.html#start-of-literature-search",
    "href": "slides/slides1.html#start-of-literature-search",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Start of literature search",
    "text": "Start of literature search\n\nDon’t start too narrow!\nReview articles and journals\n\nJournal of Economic Literature\nJournal of Accounting Literature\n\nA*/A journals\n\nAccounting\nFinance\n\n\n\nMy favourite way to start a research project now is to find one or two excellent overview or review papers. A (systematic) review paper provides a state of a research field and identifies interesting new research questions. I am not sure whether my strategy will work for you. I find that a good review paper gives a good list of papers you can build on and they often already compare the most important papers in a field. The trick is to be not too picky. You probably will not find a review for your exact reserch problem but it is unlikely that you will not find a partly relevant overview paper. You can search for review papers by adding “intitle:review” or “intitle:overview” to your Google Scholar search.\nTo find other papers relevant to your topic, you can build on the review paper by (1) looking up the papers referred to in the review paper and (2) search for papers that cite the review paper. You can do the latter via Google Scholar and Webofknowledge.\nTo find good reviews, I think you should start your search in the better journals. Some journals are dedicated to these literature reviews for instance Journal of Economic Literature and Journal of Accounting Literature. I am not aware of a similar journal in finance but I will happily add it if you let me know.\nWhen you start your literature search, you don’t want to narrow. You are not going to find an overview paper about “CEO compensation in Australian mining companies after the GFC”. However, you can start with an overview paper about CEO compensation. Like the one I found: “Executive Compensation: A Modern Primer” by Alex Edmans and Xavier Gabraix in Journal of Economic Literature."
  },
  {
    "objectID": "slides/slides1.html#signs-of-bad-workload-management",
    "href": "slides/slides1.html#signs-of-bad-workload-management",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Signs of bad workload management",
    "text": "Signs of bad workload management\n\nIrregular sleeping habits\nLoss of motivation\nPostponing difficult tasks"
  },
  {
    "objectID": "slides/slides1.html#working-with-a-supervisor",
    "href": "slides/slides1.html#working-with-a-supervisor",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Working with a supervisor",
    "text": "Working with a supervisor\n\nThe role of the supervisor\n\n\nGuide you towards a feasible research project\nHelp you finish the dissertation\n\n\nWork process\n\n\nSchedule weekly or fortnightly meetings\nSubmit writing or data analysis before every meeting.\n\n\nAdd a tl;dr section.\n\n\nYour supervisor is not your copy-editor, let them know when you submit an “early” draft.\nTell your supervisor what has changed\nClarify the sample and the main variables in tables\nTell your supervisor what the main table or figure is"
  },
  {
    "objectID": "slides/slides1.html#managing-the-workload-40-hours-per-week",
    "href": "slides/slides1.html#managing-the-workload-40-hours-per-week",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Managing the workload (40 hours per week)",
    "text": "Managing the workload (40 hours per week)\n\nPlan ahead (with your supervisor) towards major deadlines\nIt’s okay to submit partial assignments, as long as you make progress. (Especially for programming exercises)\nKeep writing!\nReach out when you need help with planning or when you feel overwhelmed.\n\nstijn.masschelein@uwa.edu.au\nUWA Counselling services\n\n\n\nThe plan can be an excel sheet with deadlines and milestones. Breaking down 5000 words into 10 weeks of 500 words is a lot less daunting."
  },
  {
    "objectID": "slides/slides1.html#questions",
    "href": "slides/slides1.html#questions",
    "title": "Introduction to Research Methods in Finance and Accounting",
    "section": "Questions",
    "text": "Questions\n\nAnswer in Quarto (.qmd) format. File > New File > Quarto Document ... >\nYou can use the code examples that I used in the video. I have uploaded the file to LMS. Use a different level 2 header for each question. Use R chunks to\n\nLoad the CEO compensation data from LMS so that you can work with it.\nPrint the dataset with only the CEOs without a cash bonus in 2013. You do not need to print the whole dataset. The default number of lines is sufficient.\nCalculate the number of observations, and the average and median bonus per year for the entire dataset.\n\nClick the Render button and upload the qmd and html version to LMS.\n\n\nThere is going to be some trial-and-error and debugging. That is fine. Carefully read the errors you get and use the resouces for help. Don’t be afraid to ask me or each other for help.\n\nGive a name to your document and enter your name\nSee the examples\nSee the render button in RStudio"
  }
]
[
  {
    "objectID": "generated/residual_independent.html",
    "href": "generated/residual_independent.html",
    "title": "Generated Independent",
    "section": "",
    "text": "This is probably not worthwhile further investigating. It’s quite niche.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modelsummary)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"generated/residual_independent.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\nN &lt;- 100\nnsim &lt;- 1000\nntotal &lt;- N * nsim"
  },
  {
    "objectID": "generated/residual_independent.html#residuals",
    "href": "generated/residual_independent.html#residuals",
    "title": "Generated Independent",
    "section": "Residuals",
    "text": "Residuals\n\nsim_data &lt;-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    x1 = rnorm(ntotal),\n    x2 = rnorm(ntotal),\n    z1 = rnorm(ntotal)\n  ) %&gt;%\n  mutate(\n    z = rnorm(ntotal, z1, 5),\n    y = rnorm(ntotal, x1 + (z - z1), 3)\n  ) %&gt;%\n  nest(.by = sim)\ntest_data &lt;- pull(sim_data, data)[[1]]\n\n\ntwo_step &lt;- function(data){\n  lm1 &lt;- lm(z ~ z1, data = data)\n  resid_z &lt;- residuals(lm1)\n  lm2 &lt;- lm(y ~ x1 + x2 + resid_z, data = data)\n  coefs &lt;- summary(lm2)$coefficients\n  return(coefs[, 4])\n}\ntwo_step(test_data)\n\n (Intercept)           x1           x2      resid_z \n7.680650e-01 7.963916e-05 8.095932e-01 4.606800e-28 \n\n\n\npvalues &lt;- sim_data %&gt;%\n  mutate(pvalues = map(.x = data, .f = ~ two_step(.))) %&gt;%\n  unnest_wider(pvalues) %&gt;%\n  pivot_longer(cols = c(x1, x2, resid_z),\n               values_to = \"pvalue\", names_to = \"variable\") %&gt;%\n  select(sim, pvalue, variable)\n\npvalues %&gt;%\n  mutate(is_sign = if_else(pvalue &lt; 0.05, 1, 0)) %&gt;%\n  summarise(proportion = mean(is_sign), .by = variable)\n\n# A tibble: 3 × 2\n  variable proportion\n  &lt;chr&gt;         &lt;dbl&gt;\n1 x1            0.873\n2 x2            0.043\n3 resid_z       1"
  },
  {
    "objectID": "generated/residual_independent.html#absolute-residuals",
    "href": "generated/residual_independent.html#absolute-residuals",
    "title": "Generated Independent",
    "section": "Absolute Residuals",
    "text": "Absolute Residuals\n\nsim_data &lt;-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    x1 = rnorm(ntotal),\n    x2 = rnorm(ntotal),\n    z1 = rnorm(ntotal)\n  ) %&gt;%\n  mutate(\n    z = rnorm(ntotal, z1, 5),\n    y = rnorm(ntotal, x1 + abs(z - z1), 3)\n  ) %&gt;%\n  nest(.by = sim)\ntest_data &lt;- pull(sim_data, data)[[1]]\n\n\ntwo_step_abs &lt;- function(data){\n  lm1 &lt;- lm(z ~ z1, data = data)\n  abs_resid_z &lt;- abs(residuals(lm1))\n  lm2 &lt;- lm(y ~ x1 + x2 + abs_resid_z, data = data)\n  coefs &lt;- summary(lm2)$coefficients\n  return(coefs[, 4])\n}\ntwo_step_abs(test_data)\n\n (Intercept)           x1           x2  abs_resid_z \n8.102549e-01 5.978551e-03 8.308279e-01 2.633114e-11 \n\n\n\npvalues &lt;- sim_data %&gt;%\n  mutate(pvalues = map(.x = data, .f = ~ two_step_abs(.))) %&gt;%\n  unnest_wider(pvalues) %&gt;%\n  pivot_longer(cols = c(x1, x2, abs_resid_z),\n               values_to = \"pvalue\", names_to = \"variable\") %&gt;%\n  select(sim, pvalue, variable)\n\npvalues %&gt;%\n  mutate(is_sign = if_else(pvalue &lt; 0.05, 1, 0)) %&gt;%\n  summarise(proportion = mean(is_sign), .by = variable)\n\n# A tibble: 3 × 2\n  variable    proportion\n  &lt;chr&gt;            &lt;dbl&gt;\n1 x1               0.874\n2 x2               0.053\n3 abs_resid_z      1"
  },
  {
    "objectID": "generated/inverse_probability_weight.html",
    "href": "generated/inverse_probability_weight.html",
    "title": "IPW",
    "section": "",
    "text": "The previous pages highlighted the dangers to coefficients and standard errors of using two-step approaches in the research design. Still, if they are done well, they can benefit your analysis and make it more robust. This is the general idea of double robust estimators which is in my view a better way to think of most (propensity score) matching approaches.\nSpecifically, we are going to look at a situation where we have a binary treatment-control x and an outcome of interest y with two confounding factors z1 and z2 that have a complicated effect on x and y. The two-step approach that we are using is combining matching with regression. In the matching step, we first estimate the propensity, \\(p\\), that an observation belongs to the treatment group. Then, we will use the inverse predicted propensity to weight the observations in the regression.\nSpecifically, we want to put less weight on observations that are in the treatment group and that we could expect to be in the treatment group. So, the weight for those observations will be \\(\\frac{1}{p}\\) in the regression. We will also put less weight on the observations in the treatment group, so we weight them by \\(\\frac{1}{1 - p}\\). The intuition is that we want to put more weight on observations that are not expected to be control or treatment groups because we assume that this unexpected assignment is because of some random variation 1. As we have seen before, we are trying to bring cause of the (weighted) difference between the treatment and the control group back to random variation.\nThere is a lot more to matching, weighting and double robust estimators. An excellent introduction is Chapter 14 in Huntington-Klein (2021). You will also find some dedicated packages for matching and weighting estimators."
  },
  {
    "objectID": "generated/inverse_probability_weight.html#introduction",
    "href": "generated/inverse_probability_weight.html#introduction",
    "title": "IPW",
    "section": "",
    "text": "The previous pages highlighted the dangers to coefficients and standard errors of using two-step approaches in the research design. Still, if they are done well, they can benefit your analysis and make it more robust. This is the general idea of double robust estimators which is in my view a better way to think of most (propensity score) matching approaches.\nSpecifically, we are going to look at a situation where we have a binary treatment-control x and an outcome of interest y with two confounding factors z1 and z2 that have a complicated effect on x and y. The two-step approach that we are using is combining matching with regression. In the matching step, we first estimate the propensity, \\(p\\), that an observation belongs to the treatment group. Then, we will use the inverse predicted propensity to weight the observations in the regression.\nSpecifically, we want to put less weight on observations that are in the treatment group and that we could expect to be in the treatment group. So, the weight for those observations will be \\(\\frac{1}{p}\\) in the regression. We will also put less weight on the observations in the treatment group, so we weight them by \\(\\frac{1}{1 - p}\\). The intuition is that we want to put more weight on observations that are not expected to be control or treatment groups because we assume that this unexpected assignment is because of some random variation 1. As we have seen before, we are trying to bring cause of the (weighted) difference between the treatment and the control group back to random variation.\nThere is a lot more to matching, weighting and double robust estimators. An excellent introduction is Chapter 14 in Huntington-Klein (2021). You will also find some dedicated packages for matching and weighting estimators."
  },
  {
    "objectID": "generated/inverse_probability_weight.html#setup",
    "href": "generated/inverse_probability_weight.html#setup",
    "title": "IPW",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(here)\nlibrary(cowplot)\nlibrary(bayesboot)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generated/inverse_probability_weight.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#simulated-data",
    "href": "generated/inverse_probability_weight.html#simulated-data",
    "title": "IPW",
    "section": "Simulated data",
    "text": "Simulated data\n\nN &lt;- 500\nnsim &lt;- 200\nntotal &lt;- N * nsim\n\nThe data generating process for x and y is more complicated that what I usually use. First, we have two binary variables z2 and x. I use the transformation 1 - 2*x and 1 - 2*z2 a number of times. This transformation takes a binary (0,1) and transforms it in (1, -1). The main goal with this transformation is to make sure that interactions with these variables are more meaningful. If they take the value 0, interactions without the transformation would also take a value of 0.\nIt might not be immediately clear but x just follows a probit-binomial distribution which depends on the control variables z1 and z2 and its interactions. y is a normally distributed function of x and the control variables and the effect of x depends on the interaction between the controls.\n\nsim_data &lt;-\n  tibble(\n    sim = rep(1:nsim, each = N),\n    z1 = rnorm(ntotal, 0, 1),\n    z2 = rbinom(ntotal, 1, .5)\n    ) %&gt;%\n  mutate(\n    x = rbinom(ntotal, 1, pnorm((z2 + 2 * z1 + (1 - 2 * z2) * z1))),\n    y = rnorm(ntotal, x + z1 + z2 + (1 - 2 * x) * z1 * ( 1 - 2 * z2), 5)\n  )\n\nOne of the problems with this data is that there is little overlap in the z1 variable between the treatment and the control group. In our case, this an indication that z1 is causing x and we have a potential confounder.\n\nggplot(sim_data, aes(x = z1, group = x)) +\n  geom_density()"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#bias",
    "href": "generated/inverse_probability_weight.html#bias",
    "title": "IPW",
    "section": "Bias",
    "text": "Bias\nAt first, we want to figure out whether different regression specifications are biased. One way to do that is to work with the full data and ignore the sim variable.\nFirst, we run our regular regressions with different but imperfect functional forms for the control variables.\n\nbind_rows(\n  lm(y ~ x, data = sim_data) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = sim_data) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = sim_data) %&gt;% tidy() %&gt;% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic      p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 x        2.73     0.0344     79.3  0           \n2 x        0.207    0.0449      4.60 0.00000420  \n3 x        0.260    0.0461      5.65 0.0000000166\n\n\nIt’s clear that while adding the control variables (and the interaction) improves the estimate, we are still far off the correct estimate which is 1.\nNext, we first run a logit regression to predict the probability that an observations is in the treatment group based on the control variables. I deliberately misspecify the regression to demonstrate that even an imperfect the propensity model can already help. Nevertheless, for a method to be truly double robust either the propensity model or the regression model needs to be correctly specified.\nWe calculate the weights and trimmed weights where I set the maximum weight to 100 which means that propensities smaller than 0.01 are winsorised at the value of 0.01 to avoid that the estimation is dominated by a couple of observations. We can run the regressions above again but now with the weights included. The disadvantage of trimming the weight is that we might be reintroducing a little bit of the bias that we are trying to avoid but the advantage is that because outliers are less likely to dominate, the standard errors are likely to be smaller.\n\nprop_glm &lt;- glm(x ~ z1 + z2, data = sim_data,\n                family = binomial(link = \"logit\"))\nipw_data &lt;- sim_data %&gt;%\n  mutate(propensity = fitted(prop_glm),\n         weight = if_else(x == 1, 1/propensity, 1/(1 - propensity)),\n         trim_weight = pmin(weight, 100))\n\nbind_rows(\n  lm(y ~ x, data = ipw_data, weights = weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = ipw_data, weights = weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = ipw_data, weights = weight) %&gt;% tidy() %&gt;% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 x        0.554    0.0323      17.2 7.76e- 66\n2 x        1.22     0.0326      37.4 2.48e-303\n3 x        1.19     0.0370      32.2 8.17e-226\n\nbind_rows(\n  lm(y ~ x, data = ipw_data, weights = trim_weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 + z2, data = ipw_data, weights = trim_weight) %&gt;% tidy() %&gt;% filter(term == \"x\"),\n  lm(y ~ x + z1 * z2, data = ipw_data, weights = trim_weight) %&gt;% tidy() %&gt;% filter(term == \"x\"))\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 x        0.935    0.0329      28.4 3.32e-177\n2 x        0.997    0.0319      31.3 2.40e-213\n3 x        1.09     0.0352      30.9 7.43e-209"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#bootstrap-the-standard-errors",
    "href": "generated/inverse_probability_weight.html#bootstrap-the-standard-errors",
    "title": "IPW",
    "section": "Bootstrap the standard errors",
    "text": "Bootstrap the standard errors\nWe know by now that the standard errors of these type of procedures with multiple steps are likely to be wrong and we can use the Bayesian bootstrap again to estimate, the standard error for our estimate of interest. The unadjusted standard errors above are around 0.035.\n\nipw &lt;- function(data, weights){\n  prop_glm &lt;- glm(x ~ z1 + z2, data = data,\n                 # the quasibinomial is necessary for the bayesian boostrap\n                 # weights otherwise R gives warnings which can slow things down\n                  family = quasibinomial(link = \"logit\"),\n                  weights = weights)\n  ipw_data &lt;- data %&gt;%\n    mutate(propensity = fitted(prop_glm),\n           weight = weights * if_else(x == 1, 1/propensity, 1/(1 - propensity)),\n           trim_weight = weights * pmin(weight, 100))\n  # These weights are the propensity weights\n  lm &lt;- lm(y ~ x + z1 * z2, data = ipw_data, weights = trim_weight)\n  beta &lt;- coefficients(lm)[\"x\"]\n  return(beta)\n}\nipw(sim_data, rep(1, ntotal))\n\n       x \n1.087827 \n\nboot_ipw &lt;- bayesboot(sim_data, ipw, R = 200, use.weights = TRUE)\nsummary(boot_ipw)\n\nBayesian bootstrap\n\nNumber of posterior draws: 200 \n\nSummary of the posterior (with 95% Highest Density Intervals):\n statistic     mean        sd   hdi.low hdi.high\n         x 1.211649 0.1816078 0.8545564  1.52107\n\nQuantiles:\n statistic     q2.5%     q25%  median     q75%   q97.5%\n         x 0.8765427 1.070387 1.21436 1.342671 1.543616\n\nCall:\n bayesboot(data = sim_data, statistic = ipw, R = 200, use.weights = TRUE)\n\n\nThe bootstrapped standard error is around 0.20 and noticeably higher. The standard errors are also quite wide given that we are using a quite large simulated data set. Unfortunately, these type of estimators come at a cost. The estimated uncertainty can be quite high because ultimiately when the data is far from ideal, there is only so much information you can glean from it.\nThis is only reinforced by doing the simulation properly and calculating the bootstrapped standard error separately for each of our 200 simulated datasets with 500 observations. The average standard error is more than 2.3 and thus with a sample of 500, we will never be able to report a significant effect. Again, this is not necessarily a weakness of the inverse probability approach. It just highlights that more advanced problems that correct for potential biases are not a free lunch. You cannot learn more from the data than what is in your data.\n\nlibrary(furrr)\ncores &lt;- parallel::detectCores()\nplan(multisession, workers = cores - 2)\nboot_function &lt;- function(data){\n  bb &lt;- bayesboot(data, ipw, R = 200, use.weights = TRUE)\n  se &lt;- sd(bb$x)\n  return(se)\n}\n\nsim &lt;- sim_data %&gt;%\n  nest(.by = sim) %&gt;%\n  mutate(se = future_map_dbl(.x = data, .f = ~ boot_function(.x),\n                             .options = furrr_options(seed = TRUE),\n                             .progress = TRUE))\n\nsummarise(sim, mean = mean(se), sd = sd(se))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2.37 0.523"
  },
  {
    "objectID": "generated/inverse_probability_weight.html#footnotes",
    "href": "generated/inverse_probability_weight.html#footnotes",
    "title": "IPW",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the accounting and finance literature, you are more likely to see propensity score matching where we find appropriate matching control observations for each treatment observations based on the propensity scores. The disadvantage of propensity score matching is similar to the regular bootstrap. The inclusion of an observation is discrete which can lead to weird behaviour. That’s why I recommend inverse probability weights.↩︎"
  },
  {
    "objectID": "generalised/poisson.html",
    "href": "generalised/poisson.html",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(fixest)\nlibrary(cowplot)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nset.seed(830323)\nN &lt;- 4000\nn_firm &lt;- 500\ngof_omit &lt;- \"Adj|Lik|IC|RMSE\""
  },
  {
    "objectID": "generalised/poisson.html#setup",
    "href": "generalised/poisson.html#setup",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(fixest)\nlibrary(cowplot)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nset.seed(830323)\nN &lt;- 4000\nn_firm &lt;- 500\ngof_omit &lt;- \"Adj|Lik|IC|RMSE\""
  },
  {
    "objectID": "generalised/poisson.html#introduction",
    "href": "generalised/poisson.html#introduction",
    "title": "Poisson Regression",
    "section": "Introduction",
    "text": "Introduction\nIn the previous section, I made the case for using OLS regressions even when the outcome variable is a discrete variable. This is especially true in the case that we are interested in estimating the effect of an intervention. The coefficient that we are getting can easily be interpreted as the difference in the probability of getting one outcome over the other."
  },
  {
    "objectID": "generalised/poisson.html#a-multiplicative-process",
    "href": "generalised/poisson.html#a-multiplicative-process",
    "title": "Poisson Regression",
    "section": "A multiplicative process",
    "text": "A multiplicative process\nThere is an exception for count and count-like data. The type of outcome variables I have in mind are the result of a stable multiplicative process. In earlier lectures, I have made the point that we can think of the contribution of a CEO to the firm as a multiplicative effect. The CEO’s ability has a larger contribution to firm value if they are working in a larger firm. So, if they grow the value of the firm by making the right decisions, the effect will be larger for a large firm.\nProbably the most basic example in finance of a multiplicative process is compound interest. If we start with $100 and the yearly interest rate is 5%, we can write our wealth as a function of time \\(T\\) (in number of years).\n\\[\nW(T) = 100 (1 + 0.05)^T\n\\]\nNow, imagine that we divide the interest by \\(N &gt; 1\\). That is, imagine that we pay an interest of \\(\\frac{0.05}{N}\\) every period with \\(N\\) periods per year.\n\\[\nW(T) = 100 (1 + \\frac{0.05}{N})^{T N}\n\\]\nIn the where we have a lot of small periods (\\(N \\to \\infty\\)), we can write our wealth as follows.\n\\[\nW(T) = 100 e^{0.05 T}\n\\]\nIn general, if we have a variable \\(V\\) that is the results of a multiplicative process of small components with a rate of change \\(r\\) and \\(S\\) steps and a starting value \\(V_0\\), we can write \\(V(S)\\) as follows.\n\\[\n\\begin{aligned}\nV(S) &= V_0 e^{rS} \\\\\n\\textrm{log} (V(S)) &= \\textrm{log} (V_0) + rS \\\\\n\\frac{V(S)}{V_0} &= e^{rS} \\\\\n\\textrm{log} \\frac{V(S)}{V_0} &= rS \\\\\n\\end{aligned}\n\\]\nThe Poisson distribution itself is the discrete equivalent of this idea. The distribution models the number of events, \\(V(S)\\), for a population, \\(V_0\\), when the underlying process follows a fixed occurrence rate \\(r\\) per unit of time and per element of the population. For instance, the number of patents a firm has can be expected to be higher when the firm is larger. The theoretical case for the Poisson regression is that the coefficients on the linear scale targets, \\(r\\), the instantaneous rate of change or the rate of occurrences 1 has a meaningful economic interpretation for non-negative variables such as number of corporate patents, carbon emissions, or distance between companies (Cohn, Liu, and Wardlaw 2022). For instance, it allows us to ask the question what the effect is of increasing R&D investments with a certain percentage to the percentage change in the number of patents. The Poisson approach also make sense for variables that naturally grow like firm size, revenues, or CEO wealth and income."
  },
  {
    "objectID": "generalised/poisson.html#the-case-for-a-poisson-regression",
    "href": "generalised/poisson.html#the-case-for-a-poisson-regression",
    "title": "Poisson Regression",
    "section": "The Case for a Poisson regression",
    "text": "The Case for a Poisson regression\n\nIntuition\nThe statistical case for the Poisson regression is extensively documented in Cohn, Liu, and Wardlaw (2022). Here I will just list the main advantages and shortly demonstrate them with a simulated example. There are a number of alternative approaches that we could use to model these type of variables. The obvious alternative is to model the variable \\(\\textrm{log}(V)\\) in a linear regression. However, this does not work if we have a lot of observations where \\(V = 0\\). One proposed solution in the literature is then to use the transformation \\(\\textrm{log}(V + 1)\\). Cohn, Liu, and Wardlaw (2022) show that the coefficients with the log plus 1 approach are hard to interpret and can have a different sign than with a poisson regression, where the poisson regression has a more straightforward interpretation. A further strength of the Poisson approach is that it allows for the inclusion of fixed effects in the regression without changing the interpretation of the coefficients. Remember that with generalised linear models in general the effect depends on other parts of the model if we are interested in the non-transformed scale, \\(V\\). The concession that we have to make is that we are interpreting the effects on the transformed scale of \\(r\\) (the change) and not on the scale of \\(V_0\\) (the size). As I explained above, this can often be a reasonable assumption to make.\nOne criticism of the Poisson regression is that it assumes that the variation around the mean is proportional to the mean. However, if this assumption does not hold, the estimates of the coefficients will not be biased and (cluster) robust standard errors are robust against violations of this assumption.\nA last point is that, just like in the binomial case, we could just use a linear model on \\(V\\) or \\(\\frac{V}{V_0}\\). However, because with multiplicative effects (or exponential growth) \\(V\\) can vary by multiple orders of magnitude, the estimates of the coefficients can be noisy and have large standard errors.\nCohn, Liu, and Wardlaw (2022) retest six published papers that use a log transformed dependent variable and compare it to a Poisson regression. They find that in all six cases the coefficient is markedly different and in three cases the sign changes. Moreover, the change in the coefficient is larger than removing any control variables. The type of regression matters more than the control variables.\n\n\n\n\n\n\nNote\n\n\n\nIn my view, Cohn, Liu, and Wardlaw (2022) makes a strong case that for a lot of non-negative outcome variables in accounting and finance research designs, the Poisson regression should be the default. This is also my recommendation.\n\n\n\n\nSimulation\nIn the simulation below, I create a dataset for a discrete and a continuous \\(y\\) where the expected value of \\(y\\) is given by\n\\[\nE(y|x_1, x_2) = e^{-0.3 x_1 + x_2}\n\\]\nThis is the data generating process that we associate with a multiplicative process or from a Poisson count process. We will be interested in estimating the effect of \\(x_1\\) on \\(y\\) which in the Poisson regression should give an estimate of \\(-0.3\\).\nThe data generating process also includes fixed effects and additional variation around this expected value which violates the assumptions of the naive Poisson regression. The details of this approach are not important and require knowledge of the negative binomial distribution(rnbinom) to get count data and the chi-squared distribution (rchisq) for the continuous case.\n\noverdispersion &lt;- 0.5\nhetero &lt;- 1\nbeta &lt;- - 0.3\nfirm &lt;-\n  tibble(\n    firm = 1:n_firm,\n    fixed = rnorm(n_firm, 0, .5))\npanel &lt;-\n  tibble(\n    firm = sample(1:100, N, replace = TRUE),\n    x1_noise = rnorm(N, 0, .5),\n    noise = rnorm(N, 0, .5)) %&gt;%\n  left_join(firm) %&gt;%\n  mutate(\n    x1 = fixed + x1_noise,\n    x2 = rnorm(N, x1 + x1^2, 2),\n    ydiscrete = rnbinom(n = N, mu = exp(beta * x1 + x2),\n                        size = 1/overdispersion),\n    ycontinuous = rchisq(n = N, ydiscrete))\n\nJoining with `by = join_by(firm)`\n\n\nI plot the data on log + 1 scale and you can see that the figure looks distorted or weird for lower values of ydiscrete or ycontinuous. This is by now means proof but it is indicative of some of the problems with the log or log plus 1 transformation.\n\npanel %&gt;%\n  pivot_longer(c(ydiscrete, ycontinuous), values_to = \"y\") %&gt;%\n  ggplot(aes(y = y + 1, x = x1)) +\n  geom_point() +\n  scale_y_log10() +\n  facet_wrap(~name)\n\n\n\n\nFor both the continuous and count variable, I run four regression models with fixed effects.\n\nThe Poisson regression with \\(y\\) as dependent variable.\nAn OlS regression with \\(\\textrm{log}(y + 1)\\) as dependent variable.\nAn OLS regression with \\(y\\) as dependent variable. Because this regression is not on the rate of change scale, we do not expect a coefficient of -0.3 here. The main purpose is to show how noisy the estimate is.\nAn OLS regression with \\(\\frac{y/x_2\\) as dependent variable. This approach will suffer from the same noisy estimates.\n\n\npoisson_disc &lt;- feglm(ydiscrete ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\npoisson_cont &lt;- feglm(ycontinuous ~ x1 + x2 | firm,\n                      family = \"poisson\", data = panel)\nlog_plus1_disc &lt;- feols(log(ydiscrete + 1) ~ x1 + x2 | firm,\n                        data = panel)\nlog_plus1_cont &lt;- feols(log(ycontinuous + 1) ~ x1 + x2 | firm,\n                        data = panel)\nols_disc &lt;-feols(ydiscrete ~ x1 + x2 | firm, data = panel)\nols_cont &lt;-feols(ycontinuous ~ x1 + x2 | firm, data = panel)\nrate_disc &lt;-feols(I(ydiscrete / exp(x2)) ~ x1 | firm, data = panel)\nrate_cont &lt;-feols(I(ycontinuous / exp(x2)) ~ x1 | firm, data = panel)\nmsummary(list(poisson = poisson_disc, log1 = log_plus1_disc,\n              ols = ols_disc, rate = rate_disc),\n         gof_omit = gof_omit)\n\n\n\n\n\npoisson\nlog1\nols\nrate\n\n\n\n\nx1\n−0.261\n−0.081\n12.167\n−0.304\n\n\n\n(0.095)\n(0.030)\n(9.304)\n(0.062)\n\n\nx2\n0.983\n0.541\n21.597\n\n\n\n\n(0.021)\n(0.012)\n(4.107)\n\n\n\nNum.Obs.\n4000\n4000\n4000\n4000\n\n\nR2\n0.923\n0.705\n0.110\n0.034\n\n\nR2 Within\n0.902\n0.684\n0.081\n0.005\n\n\nStd.Errors\nby: firm\nby: firm\nby: firm\nby: firm\n\n\nFE: firm\nX\nX\nX\nX\n\n\n\n\n\n\nmsummary(list(poisson = poisson_cont, log1 = log_plus1_cont,\n              ols = ols_cont, rate = rate_cont),\n         gof_omit = gof_omit)\n\n\n\n\n\npoisson\nlog1\nols\nrate\n\n\n\n\nx1\n−0.259\n−0.087\n12.037\n−0.292\n\n\n\n(0.095)\n(0.035)\n(9.336)\n(0.085)\n\n\nx2\n0.979\n0.536\n21.686\n\n\n\n\n(0.020)\n(0.012)\n(4.076)\n\n\n\nNum.Obs.\n4000\n4000\n4000\n4000\n\n\nR2\n0.916\n0.653\n0.112\n0.031\n\n\nR2 Within\n0.893\n0.630\n0.082\n0.002\n\n\nStd.Errors\nby: firm\nby: firm\nby: firm\nby: firm\n\n\nFE: firm\nX\nX\nX\nX\n\n\n\n\n\n\n\nYou can see that the Poisson regression is pretty close to recovering the true estimate of -0.3 both for the count as for the continuous case with small standard errors. The rate estimate with OLS is also pretty good but this only works if we know the scale variable exp(x2) before hand. The log plus 1 approach gives a considerably different estimate and the OLS estimates are positive instead of negative but with large standard errors."
  },
  {
    "objectID": "generalised/poisson.html#footnotes",
    "href": "generalised/poisson.html#footnotes",
    "title": "Poisson Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA semi-elasticity in economics terms (Cohn, Liu, and Wardlaw 2022).↩︎"
  },
  {
    "objectID": "generalised/interpretation.html",
    "href": "generalised/interpretation.html",
    "title": "Interpretation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(cowplot)\nlibrary(fixest)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nN &lt;- 1001\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generalised/interpretation.html#setup",
    "href": "generalised/interpretation.html#setup",
    "title": "Interpretation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(cowplot)\nlibrary(fixest)\nlibrary(modelsummary)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nN &lt;- 1001\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generalised/interpretation.html#interpretation",
    "href": "generalised/interpretation.html#interpretation",
    "title": "Interpretation",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe problem\nThe problem with non-linear, generalised linear models is that the interpretation of the coefficients is more murky than in our linear models. In a linear model, we can interpret the coefficient, \\(\\beta\\), of the variable \\(x\\) easily. When \\(x\\) is a continuous variable, we can interpret the coefficient as the increase in \\(y\\) when \\(x\\) increases by 1. When \\(x\\) indicates whether an observation is in the treatment group (\\(x=1\\)) or in the control group (\\(x=0\\)), the coefficient estimates the difference between the control and treatment group for \\(y\\).\nUnfortunately, the non-linear transformation for the generalised linear models complicates the interpretation. Let’s illustrate this with an example where we know the true model for the probability that a firm has a female CEO (\\(y_1\\)) and the probability that the firm has a female CFO (\\(y_2\\)). The true probabilities are given by the following logistic models.\n\\[\n\\begin{aligned}\ny_1 &= \\frac{e^{2 + 3x}}{1 + e^{2 + 3x}} &&= g(2 + 3x) \\\\\ny_2 &= \\frac{e^{-2 + 3x}}{1 + e^{-2 + 3x}} &&= g(-2 + 3x)\n\\end{aligned}\n\\]\nYou can think of \\(x\\) as a characteristic of the company that increases the likelihood of female executives in the company. A casual glance of the equations would give the impression that the effect of the on the CEOs and CFOs is the same because \\(\\beta = 3\\) is the same in both equations. We can also plot the two probabilities as functions of \\(x\\) and you can see that that one curve is just the other one shifted horizontally.\n\ninterpretation &lt;-\n  tibble(x = rnorm(N, 0, 1)) %&gt;%\n  mutate(y1 = plogis(2 + 3 * x),\n         y2 = plogis(-2 + 3 * x))\n\ninterpretation %&gt;%\n  pivot_longer(cols = c(y1, y2)) %&gt;%\nggplot(aes(y = value, x = x,\n                        colour = name)) +\n  scale_color_viridis_d() +\n  geom_line()\n\n\n\n\nHowever, from the plots you can already see that firms at \\(x = 0\\) will see a larger increase in the probability of having a female CFO (\\(y_2\\)) than a female CEO (\\(y_1\\)) when \\(x\\) increases. Because at \\(x=0\\) most firms are already more likely to have a female CEO and thus the effect of \\(x\\) cannot be very large. In other words, despite the fact that the two probabilities follow a very similar function, the effect of an increase in \\(x\\) can be very different depending on the value of \\(x\\).\nAnother way to look at the problem is to think of \\(x\\) as a policy that is either present (\\(x=1\\)) or not (\\(x=0\\)). We can calculate the causal effect of the policy as follows.\n\ny1_effect &lt;- plogis(2 + 3 * 1) - plogis(2 + 3 * 0)\ny2_effect &lt;- plogis(-2 + 3 * 1) - plogis(-2 + 3 * 0)\nprint(c(y1_effect, y2_effect))\n\n[1] 0.1125101 0.6118557\n\n\nAgain, we see that the policy has a large effect (0.61) for \\(y_2\\) and a small effect for \\(y_1\\) (0.11).\n\n\nWhy does it matter?\nThere are a number of reasons when this will matter. First of all, the coefficient (\\(\\beta = 3\\)) does not directly map onto the causal effect. The same coefficient can lead to different causal effects depending on the value of \\(x\\) and depending on other parts of the model (e.g. the intercept). That means that we cannot just rely on the coefficient.\nBecause the true effect depends on other parts of the model, generalised linear models make the use of fixed effects and robust standard errors more tricky as well. I am not going to go into the details of these issues but that is the overall problem with generalised linear model: the effect of each term is dependent on the other parts of the model.\nThere are two questions to ask whether this is a problem for your research question.\n\nThe first question is whether you are interested in predicting the variable \\(y\\) or whether you are interested in estimating an effect on the variable \\(y\\). As I discussed in the introduction to machine learning, when we are mainly interested in prediction, the coefficients or effects are less important. In the case of predictions, we definitely want to use a model that does not allow to make impossible predictions (e.g. probabilities that are higher than 1).\nThe second question is whether we are interested in the effect on the linear scale (\\(\\alpha + \\beta x\\)) or on the transformed scale (\\(g(\\alpha + \\beta x)\\)). In the working example I have used so far, you probably would be interested in the transformed scale, i.e. the probability that the firm has a female CEO. In a lot of cases in accounting and finance, we would be interested directly in those probabilities e.g. the probability that a firm goes bankrupt, or discloses certain information. In contrast, studies in consumer finance or behavioural economics are more interested in the utility that consumers derive from certain interventions. While the utility is often unobserved, we can observe the consumers choices (e.g. which mortgage they choose). A typical approach is to model the unobserved utility on the a linear scale (\\(z\\)) and model the probability of a choice as a transformation of the utility (\\(g(z)\\)). In fact, the logistic transformation is the workhorse function in this literature. Finally, there is literature that prefers the linear scale for logistic models because they argue that we can interpret the effects on the log odds scale, \\(\\textrm{log}(\\frac{p}{1-p})\\). That is, the effect on the linear scale represents an effect on the relative probability.1 Betting markets often present winning probabilities as odds or relative probabilities."
  },
  {
    "objectID": "generalised/interpretation.html#solution-without-controls",
    "href": "generalised/interpretation.html#solution-without-controls",
    "title": "Interpretation",
    "section": "Solution without controls",
    "text": "Solution without controls\nIn the next, section I am going to assume that we are only interested in the effect on the probability scale and we would like to summarise the effect in one number. We will start with the simplest case where the variable of interest is a simple treatment (\\(x = 1\\)) versus control (\\(x = 0\\)). We generate data according to the two functions above. In this simple case, it’s relatively easy to estimate the causal effects by just looking at the difference between the treatment and the control condition for both variables.\n\nsolutions &lt;-\n  tibble(x = rbinom(N, 1, 0.5)) %&gt;%\n  mutate(y1 = rbinom(N, 1, plogis(2 + 3 * x)),\n         y2 = rbinom(N, 1, plogis(-2 + 3 * x)))\n\nsolutions %&gt;%\n  summarise(across(c(y1, y2), mean), .by = x)\n\n# A tibble: 2 × 3\n      x    y1    y2\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 0.986 0.708\n2     0 0.886 0.136\n\n\nYou can see that the differences are pretty close to the theoretical effects y1_effect = 0.11 and y2_effect = 0.61 that we calculated before.\n\nLinear model\nThe next step is to figure out which regression approach gives us similar estimates. You can see that the OLS model, where we ignore that the outcome variable is restricted, gives us the estimate that we are interested in. To account for the distribution of the outcome variable, it is a good idea to specify se = \"hetero\". This way, feols uses a more robust estimation for the standard errors, accounting for the likely non-normal distribution of the error term.\nThe code also shows who you can run a logit or probit regression in R with the glm function. The main difference with the lm function is that you need to specify the family of models and the transformation link.\n\nols1 &lt;- feols(y1 ~ x, data = solutions, se = \"hetero\")\nlogit1 &lt;- glm(y1 ~ x, data = solutions, family = binomial(link = logit))\nprobit1 &lt;- glm(y1 ~ x, data = solutions, family = binomial(link = probit))\nols2 &lt;- feols(y2 ~ x, data = solutions, se = \"hetero\")\nlogit2 &lt;- glm(y2 ~ x, data = solutions, family = binomial(link = logit))\nprobit2 &lt;- glm(y2 ~ x, data = solutions, family = binomial(link = probit))\nmsummary(list(ols1 = ols1, logit1 = logit1, probit1 = probit1,\n              ols2 = ols2, logit2 = logit2, probit2 = probit2),\n         gof_map = gof_map, fmt = 2)\n\n\n\n\n\nols1\nlogit1\nprobit1\n ols2\n logit2\n probit2\n\n\n\n\n(Intercept)\n0.89\n2.05\n1.21\n0.14\n−1.85\n−1.10\n\n\n\n(0.01)\n(0.14)\n(0.07)\n(0.02)\n(0.13)\n(0.07)\n\n\nx\n0.10\n2.22\n1.00\n0.57\n2.73\n1.64\n\n\n\n(0.02)\n(0.41)\n(0.16)\n(0.03)\n(0.16)\n(0.09)\n\n\nNum.Obs.\n1001\n1001\n1001\n1001\n1001\n1001\n\n\nR2\n0.043\n\n\n0.333\n\n\n\n\n\n\n\n\n\n\n\nMarginal Effects\nYou can get the best of both worlds in this simple case. We can use the non-linear models to give predictions on the probability scale with fitted and then look at the average between the treatment and the control group.\n\nsolutions %&gt;%\n  mutate(pred_logit = fitted(logit1),\n         pred_probit = fitted(probit1)) %&gt;%\n  summarise(logit = mean(pred_logit),\n            probit = mean(pred_probit), .by = x)\n\n# A tibble: 2 × 3\n      x logit probit\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 0.986  0.986\n2     0 0.886  0.886\n\n\nWith the regression objects, we can use the marginaleffects package to get these estimates directly.\n\nlibrary(marginaleffects)\navg_comparisons(logit1, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n    x    1 - 0      0.1     0.0152 6.58   &lt;0.001 34.3 0.0705   0.13\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(probit1, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n    x    1 - 0      0.1     0.0152 6.58   &lt;0.001 34.3 0.0705   0.13\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response"
  },
  {
    "objectID": "generalised/interpretation.html#solution-with-controls",
    "href": "generalised/interpretation.html#solution-with-controls",
    "title": "Interpretation",
    "section": "Solution with controls",
    "text": "Solution with controls\nThings become more complicated when we introduce control variables. Remember, the effect of a variable is not constant in a non-linear model and it depends on other parts of the model. That means that the size of effect of x depends on the value of the control variable. Below, I simulate a dataset with a discrete control variable that takes three values (-1, 0, 1) with different probabilities. We can then run our three models and report them as before.\n\nsol_controls &lt;-\n  tibble(x = rbinom(N, 1, 0.5),\n         control = sample(c(-1, 0, 1), N, replace = TRUE,\n                          prob = c(0.5, 0.3, 0.2))) %&gt;%\n  mutate(y = rbinom(N, 1, plogis(-2 + 3 * x + 2 * control)))\n\nols_controls &lt;- feols(y ~ x + control, data = sol_controls,\n                      se = \"hetero\")\nlogit_controls &lt;- glm(y ~ x + control, data = sol_controls,\n                      family = binomial(link = logit))\nprobit_controls &lt;- glm(y ~ x + control, data = sol_controls,\n                       family = binomial(link = probit))\nmsummary(list(ols = ols_controls, logit = logit_controls,\n              probit = probit_controls),\n         gof_map = gof_map, fmt = 2)\n\n\n\n\n\nols\nlogit\nprobit\n\n\n\n\n(Intercept)\n0.24\n−1.99\n−1.10\n\n\n\n(0.02)\n(0.16)\n(0.09)\n\n\nx\n0.42\n3.19\n1.74\n\n\n\n(0.02)\n(0.24)\n(0.12)\n\n\ncontrol\n0.29\n2.06\n1.14\n\n\n\n(0.02)\n(0.15)\n(0.08)\n\n\nNum.Obs.\n1001\n1001\n1001\n\n\nR2\n0.418\n\n\n\n\n\n\n\n\n\nLet’s now calculate the effect for each value of the control variable 2. We use the same procedure but we just do it by control. We calculate the difference between the predicted probability for the control and treatment group. We also keep track of the number of observations in each cell. You can see in the printed intermediate calculation that the estimated effect differs for the three different values of the control variable. Because we want a one number summary, we can take the weighted (by number of observation) average of the effect to get the Average Marginal Effect.\n\nsol_controls %&gt;%\n  mutate(predictions = fitted(logit_controls)) %&gt;%\n  summarise(prob = mean(predictions), n = n(),\n            .by = c(x, control)) %&gt;%\n  pivot_wider(values_from = c(prob, n), names_from = x) %&gt;%\n  mutate(effect = prob_1 - prob_0, n = n_1 + n_0) %&gt;%\n  print() %&gt;%\n  summarise(AME = sum(effect * n)/sum(n))\n\n# A tibble: 3 × 7\n  control prob_0 prob_1   n_0   n_1 effect     n\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n1      -1 0.0171  0.296   249   254  0.279   503\n2       1 0.517   0.963    99   100  0.446   199\n3       0 0.120   0.768   146   153  0.648   299\n\n\n# A tibble: 1 × 1\n    AME\n  &lt;dbl&gt;\n1 0.422\n\n\nThere is another way of doing the calculation. We can also predict the probability assuming that an observation is in the treatment group (pred_x1) and assuming that an observation is in the control group (pred_x2). We can then calculate the Average Marginal Effect as the mean effect within the sample and we get a very similar result.\n\npred_new &lt;- function(x = 0, control = 1){\n  plogis(predict(logit_controls, newdata = tibble(x = x, control = control)))\n}\nsol_controls %&gt;%\n  mutate(pred_x1 = map_dbl(control, ~ pred_new(1, ..1)),\n         pred_x0 = map_dbl(control, ~ pred_new(0, ..1)),\n         effect = pred_x1 - pred_x0) %&gt;%\n  print() %&gt;%\n  summarise(AME = mean(effect))\n\n# A tibble: 1,001 × 6\n       x control     y pred_x1 pred_x0 effect\n   &lt;int&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     0      -1     0   0.296  0.0171  0.279\n 2     1       1     1   0.963  0.517   0.446\n 3     1       0     1   0.768  0.120   0.648\n 4     0       1     0   0.963  0.517   0.446\n 5     0      -1     0   0.296  0.0171  0.279\n 6     1       1     1   0.963  0.517   0.446\n 7     0      -1     0   0.296  0.0171  0.279\n 8     0      -1     0   0.296  0.0171  0.279\n 9     1      -1     1   0.296  0.0171  0.279\n10     0      -1     0   0.296  0.0171  0.279\n# ℹ 991 more rows\n\n\n# A tibble: 1 × 1\n    AME\n  &lt;dbl&gt;\n1 0.422\n\n\nThe last approach is what the avg_comparisons package does. One advantage of the last approach is that it scales better if we have multiple control variables and some of them are continuous. The predictions will account for the heterogeneity in the effect because of the differences in the control variables. The flip side of all of this is that the estimate really depends on the sample. A different sample, with radically different values for the control variables will have a different estimate for the effect of interest.\n\navg_comparisons(logit_controls, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    x    1 - 0    0.422     0.0225 18.8   &lt;0.001 258.5 0.378  0.466\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_comparisons(probit_controls, variables = \"x\")\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    x    1 - 0    0.419     0.0229 18.3   &lt;0.001 247.0 0.374  0.464\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the linear model ols_controls recovers the average marginal effect directly in the regression. If you are interested in a treatment effect of a treatment compared to a control group, in my opinion, you should just use a linear model with robustly estimated standard errors. The linear model makes it easy to incorporate fixed effects, deal with panel data, and expand the model to two-stage-least-squares or difference-in-differences while giving the estimate that you care about.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe average marginal effect calculation highlights that the effect that we estimate depends on the values of the control variables. As a result, we need to be careful about extrapolating the effect to different settings where the control variables might take different values.\n\n\n\nSolution with continuous variable\nThe case of a continuous variable x is slightly more complicated because we already know that the relation between the outcome variable y and x is not perfectly linear. However, even with a continuous outcome variable, we also have to make the assumption of a linear relation. However, if we can make the assumption that the relation between y and x is roughly linear, the OLS estimate will be a good approximation of the average marginal effect as we can see below. As before, we can easily incorporate all the benefits of using feols while getting the estimate that we are targeting.\n\nsol_continuous &lt;-\n  tibble(x = rnorm(N, 0, 1),\n         control = rnorm(N, 0, 1)) %&gt;%\n  mutate(y = rbinom(N, 1, plogis(-2 + 3 * x + 2 * control)))\n\nols_continuous &lt;- feols(y ~ x + control, data = sol_continuous,\n                        se = \"hetero\")\nlogit_continuous &lt;- glm(y ~ x + control, data = sol_continuous,\n                      family = binomial(link = logit))\nprobit_continuous &lt;- glm(y ~ x + control, data = sol_continuous,\n                       family = binomial(link = probit))\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nmsummary(list(ols = ols_continuous, logit = logit_continuous,\n              probit = probit_continuous),\n         gof_map = gof_map, fmt = 2)\n\n\n\n\n\nols\nlogit\nprobit\n\n\n\n\n(Intercept)\n0.30\n−2.41\n−1.35\n\n\n\n(0.01)\n(0.18)\n(0.09)\n\n\nx\n0.26\n3.46\n1.92\n\n\n\n(0.01)\n(0.25)\n(0.13)\n\n\ncontrol\n0.19\n2.43\n1.35\n\n\n\n(0.01)\n(0.19)\n(0.10)\n\n\nNum.Obs.\n1001\n1001\n1001\n\n\nR2\n0.487\n\n\n\n\n\n\n\n\n\nTo get the average marginal effect with a continuous x, we use the avg_slopes function from marginaleffects. The function does something similar as avg_comparisons does for the discrete x. It will predict the probability of the outcome y for x and for x + 0.0013 and then take the difference. In other words, the function will approximate the estimate slope between y and x for each observations. The average marginal effect is again the average for the whole sample.\nThe fact that the calculation needs to be done for every observation can be a disadvantage of you have a lot of observations and a lot of effects to estimate.\n\navg_slopes(logit_continuous, variables = \"x\")\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n    x    0.265    0.00703 37.6   &lt;0.001 Inf 0.251  0.278\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\navg_slopes(probit_continuous, variables = \"x\")\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|)      S 2.5 % 97.5 %\n    x    0.263    0.00703 37.4   &lt;0.001 1012.7 0.249  0.276\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nAll estimates point to the following effect: if x increases by .01, we expect the probability of y to increase by about .25% in our sample."
  },
  {
    "objectID": "generalised/interpretation.html#further-information",
    "href": "generalised/interpretation.html#further-information",
    "title": "Interpretation",
    "section": "Further information",
    "text": "Further information\nThere is a lot more that can be done to with marginal effects and they can also be used to estimate non-linear effects in linear models. Andrew Heiss has a good overview of the theory and the marginaleffects package has excellent vignettes on how to implement the different possibilities."
  },
  {
    "objectID": "generalised/interpretation.html#footnotes",
    "href": "generalised/interpretation.html#footnotes",
    "title": "Interpretation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\[\n\\begin{aligned}\np &= \\frac{e^z}{1 + e^z} \\\\\np &= \\frac{1}{e^{-z} + 1} \\\\\n\\frac{1}{p} &= e^{-z} + 1 \\\\\n\\frac{1 - p}{p} &= e^{-z} \\\\\n\\frac{p}{1-p} &= e^z \\\\\n\\textrm{log}(\\frac{p}{1-p}) &= z\n\\end{aligned}\n\\]↩︎\nThis is the reason why I used a discrete control variable. It makes all of this a bit easier to illustrate.↩︎\nor another amount that can be chosen in the function.↩︎"
  },
  {
    "objectID": "machine_learning/predictions.html",
    "href": "machine_learning/predictions.html",
    "title": "Predictions",
    "section": "",
    "text": "Introduction\nSo far in the course, I have made the assumption that our research is mainly interested in estimating a certain parameter of interest, i.e. we want to know the effect of a certain variable on a certain outcome variable. For instance, we are interested in the effect of a policy uncertainty on a firm’s investment decisions (Falk and Shelton 2018), or the use of options in CEO compensation on risk taking (Shue and Townsend 2017). In terms of a regression model, \\(y_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i\\), we are interested in \\(\\beta\\), the effect of \\(x\\) on \\(y\\) after controlling for \\(z\\). All the statistical and conceptual tools in the lectures had the aim of making sure that the estimate \\(\\hat{\\beta}\\) is the best possible estimate of the true underlying and unobserved \\(\\beta\\) (Mullainathan and Spiess 2017).\nIn contrast, some research questions focus on building a model to predict the outcomes. In that case, we are not interested in the estimated parameter \\(\\beta\\) (or \\(\\alpha\\) or \\(\\gamma\\)). We are interested in the prediction \\(\\hat{y}\\), especially for future or unobserved cases (Mullainathan and Spiess 2017). One of the biggest risks for prediction tasks is that our model excels at predicting the outcomes in the data that we have but it is rubbish at predicting for new data. In that case, our model is overfitting for the purpose of predicting future outcomes. The strength of a lot of machine learning applications is that they excel at incorporating a lot of variables while at the same time guard against overfitting. If prediction is the research question these are the methods we should turn to.\n\n\nContent\nFor this course, I will give a very gentle introduction to the tidymodels framework from the same team as the tidyverse. The framework provides a unified workflow to work with different machine learning (and traditional regression) models for prediction tasks. I will not use it for a typical prediction task. I will use it for the research question whether (initial) stock price reactions to Friday earnings announcements are muted (Dellavigna and Pollet 2009).\nFor that research question, we want to estimate the abnormal return after a company’s earnings announcement. That means that we need to predict what the counterfactual return would have been in the absence of an announcement. That is a prediction task! We did not really care whether we correctly estimated the market beta or the effect of the factors. We cared whether we predicted the expected returns correctly. When we only have one or three predictors the advantage of the machine learning methods is likely to be minimal but we could use more predictors. The market return and the factors are essentially stand ins for controlling for other factors besides that announcement that could effect the return. A straight forward extension of the factor models would be to include the returns of a number of peer firms as predictors and allow the weight (i.e. the betas) to vary (Baker and Gelbach 2020). In this case, we might have a lot of predictors and only a limited amount of observations to estimate the betas. This is a classical case where we might be at risk of overfitting.\n\n\nOther Applications\nThe inclusion of abnormal returns is a specific application of a more general use case for machine learning techniques in accounting and finance research. Sometimes, we are not interested in some parameters. We just want to include (control) predictors to remove some potential confounding effect. For instance, in my original regression model, if we have a lot of variables \\(z\\) but we don’t care about the parameters \\(\\gamma\\), we can use machine learning techniques to include more variables than a typical regression would allow (Mullainathan and Spiess 2017).\n\\[\ny_i = \\alpha + \\beta x_i + \\gamma z_i + \\epsilon_i\n\\]\nOne way to think about this is that we are splitting up the regression in two steps. First, we predict the outcome variable, \\(\\hat{y}\\) based on the many predictors \\(z\\). Second, we estimate the parameter \\(\\beta\\) with \\(y - \\hat{y}\\) as the dependent variable (Mullainathan and Spiess 2017).\nAnother related application is that we want to use the machine learning techniques to predict the causal variable of interest with the predictors z, so that we can use \\(x - \\hat{x}\\) as the new causal variable and estimate it’s effect on \\(y - \\hat{y}\\). For some research questions, it will be appropriate to say that after we removed the influence of a bunch of observable factors, we are more confident that the remaining variation in \\(x\\) and \\(y\\) is the capturing the causal effect we are interested in. However, remember that sometimes the opposite is true. In the (Shue and Townsend 2017) paper, we wanted to predict the changes in the option awards that followed a predictable schedule. The paper essentially used a very simple prediction model to predict the option awards that we could treat as if they are random (or exogenous).\nA final application is when you want to increase the dataset when you need to collect some variable by manually labelling or categorising some data. Sometimes it is possible to collect the data manually for a subset of the data. You can then use the machine learning tools to predict the key variable based on the relation between predictors and your measure in the manually collected data.\n\n\n\n\n\nReferences\n\nBaker, Andrew, and Jonah B. Gelbach. 2020. “Machine Learning and Predicted Returns for Event Studies in Securities Litigation.” Journal of Law, Finance, and Accounting 5 (2): 231–72. https://doi.org/10.1561/108.00000047.\n\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x.\n\n\nFalk, Nathan, and Cameron A. Shelton. 2018. “Fleeing a Lame Duck: Policy Uncertainty and Manufacturing Investment in US States.” American Economic Journal: Economic Policy 10 (4): 135–52. https://doi.org/10.1257/pol.20160365.\n\n\nMullainathan, Sendhil, and Jann Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106. https://doi.org/10.1257/jep.31.2.87.\n\n\nShue, Kelly, and Richard R. Townsend. 2017. “How Do Quasi-Random Option Grants Affect CEO Risk-Taking?” The Journal of Finance 72 (6): 2551–88. https://doi.org/10.1111/jofi.12545."
  },
  {
    "objectID": "machine_learning/application.html",
    "href": "machine_learning/application.html",
    "title": "Peer Firms",
    "section": "",
    "text": "In this section, I will show how you can code a prediction model for the return of a company’s stock price based on the returns of a number of peer companies. I loosely follow the same approach as in Baker and Gelbach (2020). In that paper, peer firms are identified based on the SIC code in the Compustat data which indicates what the industry is of the firm’s primary product.\nWe are using the tidymodel package for the application and I will follow the introduction to tidymodels. The advantage of the tidymodels approach is that you can follow a very similar workflow for other machine learning methods than the one I will be showing here."
  },
  {
    "objectID": "machine_learning/application.html#get-some-data",
    "href": "machine_learning/application.html#get-some-data",
    "title": "Peer Firms",
    "section": "Get Some Data",
    "text": "Get Some Data\nWe are going to use some example data from one earnings announcement and we are going to use data from an earnings announcement where the firm has a lot of peers because in that situation prediction is more likely to suffer from overfitting. The data_ml data excludes all peers with missing observations because most of the algorithms do not deal well with missing values. There are more sophisticated approaches possible to deal with missing data but they are not necessary to illustrate the main workflow.\n\npeers &lt;- readRDS(here(\"data\", \"machine_learning\", \"peers.RDS\")) %&gt;%\n  filter(N == max(N))\nglimpse(peers)\n\nRows: 4\nColumns: 6\n$ gvkey   &lt;chr&gt; \"063799\", \"063799\", \"063799\", \"017208\"\n$ permno  &lt;dbl&gt; 84058, 84058, 84058, 83729\n$ anndat  &lt;date&gt; 2003-01-23, 2003-04-22, 2003-07-24, 2003-04-18\n$ permnos &lt;list&gt; &lt;62770, 65138, 20694, 85073, 22032, 86685, 23916, 35167, 36469…\n$ N       &lt;dbl&gt; 200, 200, 200, 200\n$ data    &lt;list&gt; [&lt;tbl_df[157 x 199]&gt;], [&lt;tbl_df[156 x 200]&gt;], [&lt;tbl_df[155 x 2…\n\ndata_ml &lt;- pull(peers, data)[[1]] %&gt;%\n  select_if(~!any(is.na(.)))\nprint(data_ml)\n\n# A tibble: 157 × 193\n    `10002`  `10304`  `10562`  `10563`  `10588`   `10623`  `10725`  `10825`\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0508  -0.00128  0.00498  0.00394 -0.00187 -0.0106    0.00609 -0.0147 \n 2  0        0.00511  0.00142  0        0.00900  0         0.00700 -0.00199\n 3  0.0242  -0.00551  0.00566 -0.0196   0.00747 -0.00391  -0.00474  0.0115 \n 4  0.0142   0.00469  0.00141  0.0299   0.0221   0.000357  0.0133   0.00296\n 5  0.0349  -0.00551  0.00351  0        0.0227   0.0250    0.0282  -0.00246\n 6  0.00674  0.0128  -0.00210 -0.0275   0.0113   0.0157    0.0146   0.0173 \n 7  0.0179  -0.0105   0.00421  0.00806  0.0192   0.0257    0.0231   0.0155 \n 8  0.0263   0.0195   0.0216   0.0160   0.0137   0.00836   0.0349   0.00382\n 9  0.0231  -0.0187  -0.0219   0        0.0287  -0.00166  -0.0182   0.00429\n10 -0.0141   0.0187   0.0238   0.0331   0.00952  0.0498    0.0142   0.0491 \n# ℹ 147 more rows\n# ℹ 185 more variables: `10906` &lt;dbl&gt;, `10913` &lt;dbl&gt;, `10916` &lt;dbl&gt;,\n#   `11216` &lt;dbl&gt;, `11348` &lt;dbl&gt;, `11389` &lt;dbl&gt;, `11634` &lt;dbl&gt;, `11808` &lt;dbl&gt;,\n#   `11823` &lt;dbl&gt;, `16644` &lt;dbl&gt;, `20694` &lt;dbl&gt;, `22032` &lt;dbl&gt;, `23326` &lt;dbl&gt;,\n#   `23916` &lt;dbl&gt;, `24628` &lt;dbl&gt;, `27254` &lt;dbl&gt;, `35167` &lt;dbl&gt;, `35503` &lt;dbl&gt;,\n#   `35829` &lt;dbl&gt;, `35917` &lt;dbl&gt;, `36469` &lt;dbl&gt;, `39766` &lt;dbl&gt;, `41807` &lt;dbl&gt;,\n#   `44688` &lt;dbl&gt;, `47159` &lt;dbl&gt;, `52265` &lt;dbl&gt;, `52840` &lt;dbl&gt;, …"
  },
  {
    "objectID": "machine_learning/application.html#linear-model",
    "href": "machine_learning/application.html#linear-model",
    "title": "Peer Firms",
    "section": "Linear model",
    "text": "Linear model\nWe first going to run a traditional linear model within the tidymodels framework. The approach is overkill for just running a linear model but it will help us to build up the full workflow. In the code, I first split the data in a proportion \\(\\frac{N-20}{N}\\) of training data and a proportion \\(\\frac{20}{N}\\) of test data, where \\(N\\) is the number of days of data available. As a result, we will use the training data to determine the parameters and than use that model to predict the last 20 days as test data. Next, we set which type of model we want to use, a linear model. We than fit the model where we specify the return variable as the outcome variable and we use the dot . to indicate that we want to use all the other variables as predictors. Finally, the tidy function gives the estimates and statistics for the estimates in a format that works with the tidyverse.\n\nN &lt;- nrow(data_ml)\ndata_split &lt;- initial_time_split(data_ml, prop = (N - 20)/N)\ntrain &lt;- training(data_split)\ntest &lt;- testing(data_split)\nlm_mod &lt;- linear_reg()\nlm_fit &lt;- lm_mod %&gt;%\n  fit(return ~ ., data = train)\ntidy(lm_fit)\n\n# A tibble: 193 × 5\n   term         estimate std.error statistic p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  -0.00257       NaN       NaN     NaN\n 2 `10002`       2.23          NaN       NaN     NaN\n 3 `10304`     -21.6           NaN       NaN     NaN\n 4 `10562`      -5.43          NaN       NaN     NaN\n 5 `10563`      -2.23          NaN       NaN     NaN\n 6 `10588`      -5.37          NaN       NaN     NaN\n 7 `10623`       1.66          NaN       NaN     NaN\n 8 `10725`       5.77          NaN       NaN     NaN\n 9 `10825`      -5.44          NaN       NaN     NaN\n10 `10906`       9.65          NaN       NaN     NaN\n# ℹ 183 more rows\n\n\nYou can immediately see that the model does not report any standard errors or p-values. This happens because we have more predictors than we have trading days and thus we can perfectly fit the in-sample returns. This type of data is exactly why would want to use a regularised regression approach.\nBefore we do that, we need to fix another issue. In the previous code, we used the day variable as a predictor which was not what we intended. You can see that we can get an estimate for day in the code below. Also notice the advantage of using tidy representation of the estimated coefficients is that we can use filter to focus on some terms. To solve the issue with the day variable, we will use a recipe to set the formula and do some data cleaning. Specifically, we will specify that the day variable is an ID variable which means that it should not be used for prediction but we want to keep it in the data for investigating the fit later.\n\ntidy(lm_fit) %&gt;%\n  filter(term == \"day\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 day         NA        NA        NA      NA\n\nlm_recipe &lt;- recipe(return ~ ., data = train) %&gt;%\n  update_role(day, new_role = \"ID\")\nsummary(lm_recipe)\n\n# A tibble: 193 × 4\n   variable type      role      source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 10002    &lt;chr [2]&gt; predictor original\n 2 10304    &lt;chr [2]&gt; predictor original\n 3 10562    &lt;chr [2]&gt; predictor original\n 4 10563    &lt;chr [2]&gt; predictor original\n 5 10588    &lt;chr [2]&gt; predictor original\n 6 10623    &lt;chr [2]&gt; predictor original\n 7 10725    &lt;chr [2]&gt; predictor original\n 8 10825    &lt;chr [2]&gt; predictor original\n 9 10906    &lt;chr [2]&gt; predictor original\n10 10913    &lt;chr [2]&gt; predictor original\n# ℹ 183 more rows\n\n\nLet’s put everything together in a workflow, fit the workflow and make predictions for the test data.\n\nlm_workflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_mod) %&gt;%\n  add_recipe(lm_recipe)\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(data = train)\npreds &lt;- predict(lm_fit, test, type = \"numeric\")\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\nglimpse(preds)\n\nRows: 20\nColumns: 1\n$ .pred &lt;dbl&gt; -1.0977181, -0.8374522, 2.2027743, -1.1720824, -0.4099103, 0.489…\n\n\nThe predictions give us 20 predictions for the days. We can also calculate the RMSE for the training data (in-sample fit) and the test data (out-of-sample fit).\n\naugment(lm_fit, train) %&gt;%\n  rmse(return, .pred)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard    7.48e-16\n\naugment(lm_fit, test) %&gt;%\n  rmse(return, .pred)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        1.47\n\n\nWhile the RMSE is almost 0 within the sample, it’s 1.47 out-of-sample. That is there is a large difference between the prediction error within the training data and the prediction error in the test data. This is the reason why we want to use a regularised regression to do out-of-sample predictions 2"
  },
  {
    "objectID": "machine_learning/application.html#elastic-net",
    "href": "machine_learning/application.html#elastic-net",
    "title": "Peer Firms",
    "section": "Elastic Net",
    "text": "Elastic Net\nIn this section, we are going to run the regularised regression as described in the theory. We will center and scale the data so that all returns have a mean of 0 and a standard deviation of 1. Remember that the penalty term punishes coefficients that are higher, however the size of the coefficient also depends on the variation in the variable. To put all the coefficients on equal footing, we scale them first.\nWe will set the penalty \\(\\lambda = .4\\) and the mixture proportion \\(\\alpha = 0.5\\) in the elastic net. We will need to install the glmnet package because that is the engine we use in our linear regression model. As before, we can put the updated data cleaning and the linear model in a new workflow and fit the model. Finally, we report the in-sample and out-of-sample RMSE.\n\nnet_recipe &lt;- lm_recipe %&gt;%\n  step_center(all_predictors()) %&gt;%\n  step_scale(all_predictors())\nnet_model &lt;- linear_reg(penalty = .4, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\")\nnet_workflow &lt;-\n  workflow() %&gt;%\n  add_model(net_model) %&gt;%\n  add_recipe(net_recipe)\nnet_fit &lt;- net_workflow %&gt;%\n  fit(data = train)\naugment(net_fit, train) %&gt;%\n  rmse(return, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0241\n\naugment(net_fit, test) %&gt;%\n  rmse(return, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0278\n\n\nIn contrast to the ordinary linear model, we see that the regularised model has a similar prediction error in-sample and out-of-sample. Furthermore, the out-of-sample prediction is much better with the regularised regression."
  },
  {
    "objectID": "machine_learning/application.html#tuning-elastic-net",
    "href": "machine_learning/application.html#tuning-elastic-net",
    "title": "Peer Firms",
    "section": "Tuning Elastic Net",
    "text": "Tuning Elastic Net\nThe problem with the previous analysis is that we had to set the penalty and mixture values. We can do better and test multiple values and see which ones give us the best prediction. This process is called tuning of the hyper parameters and it is a standard procedure in many machine learning applications.\nThe above code shows the general principle. We are dividing the training data in 10 randomly selected folds. We use the same workflow as above but we use the part of the training data that is not in a fold to predict the data in the fold. Now, we have 10 out-of-sample RMSEs. With collect_metrics, we get an overview of the mean RMSE and the standard error around the mean.\n\nfolds &lt;- vfold_cv(train, v = 10)\nnet_fit_rf &lt;-\n  net_workflow %&gt;%\n  fit_resamples(folds, metrics = metric_set(rmse))\ncollect_metrics(net_fit_rf)\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.0235    10 0.00186 Preprocessor1_Model1\n\n\nWhile the randomly selected folds is often appropriate, in our case it is not. We have a time series and we want to predict future returns based on earlier returns. So, we are going to use a different function to create rolling windows of 75 days of data to build the model to predict the next 5 days. So for instance, we use a day 1-75 to predict day 76-80. Next, we use day 6-80 to predict day 81-85. We repeat the procedure until we run out of data. When we print windows, you can see that we have 10 splits with 75 modeling observations and 5 observations that we want to predict.\n\nwindows = rolling_origin(train, initial = 75, assess = 5,\n                         skip = 5, cumulative = FALSE)\nprint(windows)\n\n# Rolling origin forecast resampling \n# A tibble: 10 × 2\n   splits         id     \n   &lt;list&gt;         &lt;chr&gt;  \n 1 &lt;split [75/5]&gt; Slice01\n 2 &lt;split [75/5]&gt; Slice02\n 3 &lt;split [75/5]&gt; Slice03\n 4 &lt;split [75/5]&gt; Slice04\n 5 &lt;split [75/5]&gt; Slice05\n 6 &lt;split [75/5]&gt; Slice06\n 7 &lt;split [75/5]&gt; Slice07\n 8 &lt;split [75/5]&gt; Slice08\n 9 &lt;split [75/5]&gt; Slice09\n10 &lt;split [75/5]&gt; Slice10\n\nnet_fit_windows &lt;-\n  net_workflow %&gt;%\n  fit_resamples(windows, metrics = metric_set(rmse))\ncollect_metrics(net_fit_windows)\n\n# A tibble: 1 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model1\n\n\nAgain, we find that the regularised version gives better out-of-sample predictions than the non-regularised version. In the next step, we will use different values for the penalty and the mixture. With the same procedure as before, we can estimate the mean out-of-sample RMSE for each combination of the penalty and mixture and then decide to use the combination that leads to the best predictions. This process is also called cross validation.\nFirst, we specify that we want a linear model with two hyper-parameters, the penalty and the mixture, that need to be tuned during the cross validation process. I specify 6 possible values for the penalty and 11 for the mixture. Next we specify the workflow and then fit the model over the grid of all 66 possible values of the penalty and the mixture.\n\nnet_tune_spec &lt;-\n  linear_reg(\n    penalty = tune(),\n    mixture = tune()\n  ) %&gt;%\n  set_engine(\"glmnet\")\ngrid_hyper &lt;- expand.grid(\n  penalty = c(0, 0.1, 0.2, 0.3, 0.4, 0.5),\n  mixture = seq(0, 1, length.out = 11)\n)\nnet_tune_wf &lt;-\n  workflow() %&gt;%\n  add_recipe(net_recipe) %&gt;%\n  add_model(net_tune_spec)\n\nnet_tune &lt;-\n  net_tune_wf %&gt;%\n  tune_grid(\n    resamples = windows,\n    grid = grid_hyper,\n    metrics = metric_set(rmse)\n  )\n\nNext, we collect the RMSE measures for the out-of-sample predictions for each combination of penalty and mixture. We see that a lot of models with some regularisation give the best out-of-sample predictions. In a more realistic, complicated example, I would expect there to be one combination that is the best.\n\ncollect_metrics(net_tune) %&gt;%\n  arrange(mean)\n\n# A tibble: 66 × 8\n   penalty mixture .metric .estimator   mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     0.2     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model09\n 2     0.3     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model10\n 3     0.4     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model11\n 4     0.5     0.1 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model12\n 5     0.1     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model14\n 6     0.2     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model15\n 7     0.3     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model16\n 8     0.4     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model17\n 9     0.5     0.2 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model18\n10     0.1     0.3 rmse    standard   0.0165    10 0.00352 Preprocessor1_Model20\n# ℹ 56 more rows\n\n\nNext, we select the best combination of penalty and mixture and use the final model on all the training data. We can then evaluate the prediction quality based on the RMSE metric.\n\nbest_model &lt;- select_best(net_tune, \"rmse\")\nfinal_wf &lt;- net_tune_wf %&gt;%\n  finalize_workflow(best_model)\nfinal_fit &lt;-\n  final_wf %&gt;%\n  last_fit(data_split, metrics = metric_set(rmse))\ncollect_metrics(final_fit)\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      0.0278 Preprocessor1_Model1\n\n\nFinally, we can calculate the predictions for the test set which we split off at the beginning and add the day column from the test set. We can than graph the deviations between the actual return and the predicted return for the 25 days in the test data.\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\naug_fit &lt;- bind_cols(collect_predictions(final_fit), select(test, day))\nggplot(aug_fit, aes(x = day, y = return - .pred)) +\n  geom_point()\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\nI can see no clear patterns in the deviations between the actual return and the predicted values."
  },
  {
    "objectID": "machine_learning/application.html#the-data",
    "href": "machine_learning/application.html#the-data",
    "title": "Peer Firms",
    "section": "The data",
    "text": "The data\nI rewrote the function to get the returns necessary for the machine learning model. The biggest difficulty is to make sure that we have the correct dates because the training data and the test data are not an uninterrupted time series. So, the extra parameters are mainly to deal with the end and start day relative to the earnings date. At the end of the data cleaning pipe, I also delete all the columns with missing data. Because, we have need to run the data for a lot of earnings announcements, we might end up with some less than ideal cases with missing data, insufficient variables, or missing returns for the firm that we are interested in. There are more elegant solutions to avoid getting errors for these issues but I will just try to avoid them or try to make sure that the code keeps running even when an error occurs for one of the earnings announcements.\nI left some of my test code at the end of the code chunk. It’s a good idea to have these quick tests when you write more extensive functions. Over time, you might need to update the functions and you want to make sure that they still work on examples that worked before.\n\nget_returns_event &lt;- function(anndat, permno_y, permnos,\n                        before_start = 300, before_end = 25,\n                        after_start = 0, after_end = 75\n                        ){\n  before_begin &lt;- anndat - before_start\n  before_last &lt;- anndat - before_end\n  after_begin &lt;- anndat + after_start\n  after_last &lt;- anndat + after_end\n  data &lt;- returns %&gt;%\n    # all dates within the range\n    filter(permno %in% permnos, date &gt;= before_begin, date &lt;= after_last) %&gt;%\n    # exclude the dates after training and before event\n    filter(date &lt; before_last | date &gt;= after_begin) %&gt;%\n    filter(!is.na(ret)) %&gt;%\n    pivot_wider(values_from = ret, id_cols = date, names_from = permno) %&gt;%\n    mutate(day = date - anndat) %&gt;%\n    select(-date) %&gt;%\n    arrange(day) %&gt;%\n    select_if(~!any(is.na(.)))\n\n  vars &lt;- names(data)\n  names(data) &lt;- if_else(vars == as.character(permno_y),\n                         \"return\", vars)\n  return(data)\n}\nget_returns_event(anndat, permnos[3], permnos) %&gt;%\n  arrange(-day)\n\n# A tibble: 244 × 3\n    `50906`   return day    \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;drtn&gt; \n 1  0.0194  -0.0184  74 days\n 2  0.0665   0.0155  73 days\n 3  0.0380   0.00323 72 days\n 4  0.0262  -0.0178  71 days\n 5  0.00620 -0.00316 70 days\n 6  0.0179  -0.00629 67 days\n 7  0.0485   0.0108  66 days\n 8 -0.00110 -0.0193  65 days\n 9  0.0231   0.0926  64 days\n10  0.0114  -0.00339 60 days\n# ℹ 234 more rows\n\nget_returns_event(anndat = peers$anndat[1],\n                  permno_y = peers$permno[1],\n                  permnos = peers$permnos[[1]])\n\n# A tibble: 243 × 192\n    `10002`  `10304`  `10562`  `10563`   `10623`  `10725`  `10825`   `10906`\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1  0.0508  -0.00128  0.00498  0.00394 -0.0106    0.00609 -0.0147  -0.00645 \n 2  0        0.00511  0.00142  0        0         0.00700 -0.00199  0.0138  \n 3  0.0242  -0.00551  0.00566 -0.0196  -0.00391  -0.00474  0.0115  -0.00791 \n 4  0.0142   0.00469  0.00141  0.0299   0.000357  0.0133   0.00296 -0.00532 \n 5  0.0349  -0.00551  0.00351  0        0.0250    0.0282  -0.00246  0.00573 \n 6  0.00674  0.0128  -0.00210 -0.0275   0.0157    0.0146   0.0173   0.0137  \n 7  0.0179  -0.0105   0.00421  0.00806  0.0257    0.0231   0.0155  -0.000749\n 8  0.0263   0.0195   0.0216   0.0160   0.00836   0.0349   0.00382  0.0124  \n 9  0.0231  -0.0187  -0.0219   0       -0.00166  -0.0182   0.00429 -0.0118  \n10 -0.0141   0.0187   0.0238   0.0331   0.0498    0.0142   0.0491   0.00824 \n# ℹ 233 more rows\n# ℹ 184 more variables: `10913` &lt;dbl&gt;, `10916` &lt;dbl&gt;, `11216` &lt;dbl&gt;,\n#   `11348` &lt;dbl&gt;, `11389` &lt;dbl&gt;, `11634` &lt;dbl&gt;, `11808` &lt;dbl&gt;, `11823` &lt;dbl&gt;,\n#   `16644` &lt;dbl&gt;, `20694` &lt;dbl&gt;, `22032` &lt;dbl&gt;, `23326` &lt;dbl&gt;, `23916` &lt;dbl&gt;,\n#   `24628` &lt;dbl&gt;, `27254` &lt;dbl&gt;, `35167` &lt;dbl&gt;, `35503` &lt;dbl&gt;, `35829` &lt;dbl&gt;,\n#   `35917` &lt;dbl&gt;, `36469` &lt;dbl&gt;, `39766` &lt;dbl&gt;, `41807` &lt;dbl&gt;, `44688` &lt;dbl&gt;,\n#   `47159` &lt;dbl&gt;, `52265` &lt;dbl&gt;, `52840` &lt;dbl&gt;, `56232` &lt;dbl&gt;, …\n\n\nThe next functions runs the full elastic net workflow with cross validation for the hyper parameters. As you can see in the test code at the end, you can just use the function above to get data and then pass it on to the function to run the full elastic net. I make sure that the training data is from before the event day and the test data is from after the event day. For the rolling prediction windows in the cross validation step I choose 100 observations to predict the next 15 days which will result in 5 windows in the typical situation. I also limit the number of hyper parameters to test, just to limit the computations. The remainder of the code is very similar to the code in the previous section for the single prediction task.\n\nrun_elastic_net &lt;- function(data, event_day = 0){\n  # split the data on day 0\n  n_total &lt;- nrow(data)\n  n_before &lt;- nrow(filter(data, day &lt; event_day))\n  prop &lt;- n_before/n_total\n  data_split &lt;- initial_time_split(data, prop = prop)\n  train &lt;- training(data_split)\n  test &lt;- testing(data_split)\n  # prepare data and windows \n  net_recipe &lt;-\n    recipe(return ~ ., data = train) %&gt;%\n    update_role(day, new_role = \"ID\") %&gt;%\n    step_center(all_predictors()) %&gt;%\n    step_scale(all_predictors())\n  folds &lt;- rolling_origin(train, initial = 100, assess = 15,\n                          skip = 15, cumulative = FALSE)\n  # set up the hyper parameters\n  net_tune_spec &lt;-\n    linear_reg(\n      penalty = tune(),\n      mixture = tune()\n    ) %&gt;%\n      set_engine(\"glmnet\")\n  grid_hyper &lt;- expand.grid(\n    penalty = c(0, 0.2, 0.4, 0.6, 0.8, 1),\n    mixture = seq(0, 1, length.out = 6))\n  # run model\n  net_tune_wf &lt;-\n    workflow() %&gt;%\n    add_model(net_tune_spec) %&gt;%\n    add_recipe(net_recipe)\n  net_tune &lt;-\n    net_tune_wf %&gt;%\n    tune_grid(\n      resamples = folds,\n      grid = grid_hyper,\n      metrics = metric_set(rmse))\n # Get best model\n  best_model &lt;- select_best(net_tune, \"rmse\")\n  final_wf &lt;- net_tune_wf %&gt;%\n    finalize_workflow(best_model)\n  final_fit &lt;-\n    final_wf %&gt;%\n    last_fit(data_split, metric_set(rmse))\n  return(final_fit)\n}\n\ntest &lt;- get_returns_event(anndat = peers$anndat[1],\n                  permno_y = peers$permno[1],\n                  permnos = peers$permnos[[1]]) %&gt;%\n        run_elastic_net(event_day = 0)\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: ''\n\n\nFinally, we can combine the two functions above in one function and calculate the abnormal returns as the difference between the prediction from the elastic net and actual return. The cumulative return is the the sum of those abnormal returns. Notice that the bulk of the function is wrapped in the tryCatch function. This is an R function that let’s you control what happens if an error occurs in the code it wraps. I use the function to return NA in the case that any of the functions gives an error.\n\ncalculate_car &lt;- function(anndat, permno_y, permnos,\n                         before_start = 300, before_end = 25,\n                         after_start = 0, after_end = 75,\n                         event_day = 0, car_short_last = 1){\n  result &lt;- NA\n  tryCatch(\n    {data &lt;- get_returns_event(anndat, permno_y, permnos,\n                              before_start, before_end,\n                              after_start, after_end)\n    fit &lt;- run_elastic_net(data = data, event_day = event_day)\n    result &lt;- augment(fit) %&gt;%\n      select(return, .pred, day) %&gt;%\n      mutate(ar = return - .pred) %&gt;%\n      mutate(window = if_else(day &lt;= car_short_last, \"short\", \"long\")) %&gt;%\n      summarise(car = sum(ar), .by = window)},\n    error = function(err) {return(NA)})\n  return(result)\n}\n\ncalculate_car(anndat = peers$anndat[1], permno_y = peers$permno[1], permnos = peers$permnos[[1]],\n              event_day = 0)\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: ''\n\n\n# A tibble: 2 × 2\n  window      car\n  &lt;chr&gt;     &lt;dbl&gt;\n1 short  -0.00746\n2 long   -0.0150 \n\n\nIn the last code block, we run the functions for N_announce earnings announcements. The code is very similar to the original abnormal return code. We first construct the data in a way that allows us to use the functions we just created. The last mutate step then runs the function in parallel depending on how many cores that are available in our computer. The reason I chose to only save the CARs is that in this way the function in future_map does not have to pass on huge amount of data as input or output which generally improves performance.\n\nN_announce &lt;- 500\ncores &lt;- parallel::detectCores()\ntictoc::tic()\nplan(multisession, workers = cores - 2)\ncars &lt;- earn_ann %&gt;%\n  head(N_announce) %&gt;%\n  left_join(compu_sic_new, by = join_by(gvkey)) %&gt;%\n  filter(!is.na(sic_new)) %&gt;%\n  left_join(compu_sic_new, by = join_by(sic_new),\n            relationship = \"many-to-many\",\n            suffix = c(\"\", \"_peer\")) %&gt;%\n  left_join(linking_table,\n            by = join_by(gvkey_peer == gvkey,\n                         between(anndat, start_date, end_date)),\n            suffix = c(\"\", \"_peer\")) %&gt;%\n  filter(!is.na(permno_peer)) %&gt;%\n  select(gvkey, permno, anndat, permno_peer) %&gt;%\n  distinct(.) %&gt;%\n  summarise(permnos = list(permno_peer),\n            .by = c(gvkey, permno, anndat)) %&gt;%\n  mutate(N = map_dbl(permnos, ~ length(.x))) %&gt;%\n  filter(N &gt; minimum_peers) %&gt;%\n  mutate(car = future_pmap(list(anndat, permno, permnos),\n                            ~ calculate_car(..1, ..2, ..3, event_day = 0),\n                           .options = furrr_options(seed = 230383),\n                           .progress = TRUE))\ntictoc::toc()\n\n804.243 sec elapsed"
  },
  {
    "objectID": "machine_learning/application.html#footnotes",
    "href": "machine_learning/application.html#footnotes",
    "title": "Peer Firms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr maybe it should not be that surprising anymore at this point.↩︎\nAlso notice the warning message. The rank-deficient fit is a consequence of having more predictors than observation days. Again, it reflects the difficulty of using an ordinary linear regression.↩︎"
  },
  {
    "objectID": "freaky_friday/regressions.html",
    "href": "freaky_friday/regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(fixest)\nlibrary(modelsummary)\ngof_omit &lt;- \"Adj|RMS|IC\"\ni_am(\"freaky_friday/regressions.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-2",
    "href": "freaky_friday/regressions.html#table-2",
    "title": "Regressions",
    "section": "Table 2",
    "text": "Table 2\nThe tables do not really replicate which is interesting to me. For a number of reasons.\n\nThe results are more consistent. I wonder whether I got rid of more outliers earlier. Remember I did end up with less observations. One interpretation is that I have cleaned the data better, the other is that I got rid of important, influential observations by being too strict when cleaning the data.\nThe results for the short term CAR are consistent with the figure. Friday market reactions to bottom quantile surprises are more positive than non-friday market reactions and the sign flips for top quantile surprises.\nI also lose substantially more observations due to the inclusion of the volatility measures. I do not know exactly why that is the case.\n\n\nPanel A: Short Term CAR\n\nsubset &lt;- readRDS(here(\"data\", \"freaky_friday\", \"subset.RDS\"))\nmodel1a &lt;- feols(car_short ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2a &lt;- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3a &lt;- feols(car_short ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,839 observations removed because of NA values (Fixed-effects: 6,839).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1a, model2a, model3a), gof_omit = gof_omit, stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n−0.036***\n\n\n\n\n\n(0.001)\n\n\n\n\nfriday\n0.014***\n0.012***\n0.013**\n\n\n\n(0.003)\n(0.003)\n(0.004)\n\n\ntop\n0.061***\n\n\n\n\n\n(0.002)\n\n\n\n\nfriday × top\n−0.023***\n−0.020***\n−0.021***\n\n\n\n(0.004)\n(0.004)\n(0.005)\n\n\nNum.Obs.\n22486\n22486\n15647\n\n\nR2\n0.086\n0.095\n0.110\n\n\nR2 Within\n\n0.001\n0.001\n\n\nStd.Errors\nby: anndat\nby: anndat\nby: anndat\n\n\nFE: size_decile\n\nX\nX\n\n\nFE: year\n\nX\nX\n\n\nFE: month\n\nX\nX\n\n\nFE: vol_decile\n\n\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nPanel B: Long Term CAR\n\nmodel1b &lt;- feols(car_long ~ friday * top,\n                 cluster = \"anndat\",\n                 data = subset)\nmodel2b &lt;- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmodel3b &lt;- feols(car_long ~ friday * top | (year[top] + month[top] + size_decile[top] + vol_decile[top]),\n                 cluster = \"anndat\",\n                 data = subset)\n\nNOTE: 6,839 observations removed because of NA values (Fixed-effects: 6,839).\nThe variable 'top' has been removed because of collinearity (see $collin.var).\n\nmsummary(list(model1b, model2b, model3b), gof_omit = gof_omit, stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n−0.022***\n\n\n\n\n\n(0.005)\n\n\n\n\nfriday\n−0.012\n−0.012\n−0.022\n\n\n\n(0.013)\n(0.013)\n(0.015)\n\n\ntop\n0.037***\n\n\n\n\n\n(0.004)\n\n\n\n\nfriday × top\n0.041**\n0.043**\n0.052**\n\n\n\n(0.015)\n(0.014)\n(0.017)\n\n\nNum.Obs.\n22486\n22486\n15647\n\n\nR2\n0.006\n0.035\n0.041\n\n\nR2 Within\n\n0.001\n0.001\n\n\nStd.Errors\nby: anndat\nby: anndat\nby: anndat\n\n\nFE: size_decile\n\nX\nX\n\n\nFE: year\n\nX\nX\n\n\nFE: month\n\nX\nX\n\n\nFE: vol_decile\n\n\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "freaky_friday/regressions.html#table-3",
    "href": "freaky_friday/regressions.html#table-3",
    "title": "Regressions",
    "section": "Table 3",
    "text": "Table 3\n\nmain_extra &lt;- main %&gt;%\n  mutate(log_size = log(market_value))  %&gt;%\n  mutate(log_size_adj = log_size - mean(log_size, na.rm = T),\n         .by = c(quarter, year)) %&gt;%\n  mutate(size_decile = ntile(log_size_adj, 10))\n\nmodel1 &lt;- feols(car_short ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel2 &lt;- feols(car_short ~ friday + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\nmodel3 &lt;- feols(car_long ~ friday * quantile,\n                cluster = \"anndat\",\n                data = main_extra)\nmodel4 &lt;- feols(car_long ~ friday  + friday : quantile\n                | (year[quantile] + month[quantile] + size_decile[quantile]),\n                cluster = \"anndat\",\n                data = main_extra)\n\nmsummary(list(model1, model2, model3, model4), gof_omit = gof_omit, stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n  (4)\n\n\n\n\n(Intercept)\n−0.041***\n\n−0.015***\n\n\n\n\n(0.001)\n\n(0.002)\n\n\n\nfriday\n0.016***\n0.014***\n−0.016*\n−0.015*\n\n\n\n(0.002)\n(0.002)\n(0.008)\n(0.007)\n\n\nquantile\n0.006***\n\n0.003***\n\n\n\n\n(0.000)\n\n(0.000)\n\n\n\nfriday × quantile\n−0.002***\n−0.002***\n0.003**\n0.003***\n\n\n\n(0.000)\n(0.000)\n(0.001)\n(0.001)\n\n\nNum.Obs.\n130759\n130759\n130759\n130759\n\n\nR2\n0.054\n0.057\n0.002\n0.015\n\n\nR2 Within\n\n0.000\n\n0.000\n\n\nStd.Errors\nby: anndat\nby: anndat\nby: anndat\nby: anndat\n\n\nFE: size_decile\n\nX\n\nX\n\n\nFE: year\n\nX\n\nX\n\n\nFE: month\n\nX\n\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "freaky_friday/abnormal_returns.html",
    "href": "freaky_friday/abnormal_returns.html",
    "title": "Abnormal returns",
    "section": "",
    "text": "The lubridate package is the tidyverse package that helps with time related data. Dates are a specific class of variables and the package helps with managing date variables. There are a bunch of other packages out there that can help with managing date variables. I just picked one.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/abnormal_returns.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nIn order to predict the expected market return reaction to earnings, we will want to take into account the general market reaction. Dellavigna and Pollet (2009) only adjust for the overall market return. There are other adjustments possible for specific risk factors. The original factor model is the Fama-French 3 Factors model and the data to run those models is available via the Kenneth French data library.\nI have downloaded the data as a .csv file. The code reads the data in skipping 5 lines and reading the variables as numbers (double precision). We then need to transform the date variable to a date type with a function from the lubridate package. Finally, we need to scale the returns by 100 because they are expressed in percentages. For replicating the Dellavigna and Pollet (2009) paper, we only need the market return minus the risk free rate (mkt_rf) and the risk free rate (rf). If I read the paper correctly, we need to use the raw market return which is calculated as mkt.\n\nearn_ann &lt;- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nanalyst &lt;- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\"))\nall_stocks &lt;- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench &lt;- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  mutate_if(is.numeric, ~ . / 100) %&gt;%\n  mutate(mkt = mkt_rf + rf) %&gt;% \n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\nshrout is shares outstanding in 1000 shares\nmarket_value is in million USD"
  },
  {
    "objectID": "freaky_friday/abnormal_returns.html#footnotes",
    "href": "freaky_friday/abnormal_returns.html#footnotes",
    "title": "Abnormal returns",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR is fundamentally a programming language that is build around functions. The tidyverse partially works around that by making tibbles the primary object. Nevertheless, when you need to do some more advanced programming, creating functions is quite natural in R.↩︎\nFor comparison on my 2020 Mac Mini, it takes about 1.5 seconds for 100 announcements and 4.0 seconds for 10000 announcements. On my 2014 Macbook pro, that is respectively 4.3 seconds and 11.2 seconds.↩︎"
  },
  {
    "objectID": "freaky_friday/download_data.html",
    "href": "freaky_friday/download_data.html",
    "title": "Earnings announcements",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\ni_am(\"freaky_friday/download_data.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\nlibrary(dtplyr)\nlibrary(dbplyr)\nlinking_table &lt;- readRDS(here(\"data\", \"freaky_friday\", \"linking_table.RDS\"))\nwrds &lt;- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  user='stimas',\n                  sslmode='require')"
  },
  {
    "objectID": "freaky_friday/download_data.html#ibes",
    "href": "freaky_friday/download_data.html#ibes",
    "title": "Earnings announcements",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nWe start with the earnings announcement data from I/B/E/S with the analyst estimates. According to the method section in Dellavigna and Pollet (2009), we need the data from the start of 1995 to the middle of 2006. We will want the analyst estimates for all the firms with a ticker in the master linking_table. I am going to use parameters that we can calculate or set in R and then pass them on to the database query.\n\nbegin_date &lt;- \"'1995-01-01'\"\nend_date &lt;- \"'2006-07-01'\"\ntickers &lt;- unique(linking_table$ticker)\n\nThe dates of the estimate and the actual earnings announcement will be critical to construct unexpected component of the earnings and to determine the exact event data, i.e. the date that (the unexpected component of) the earnings are announced. Thankfully, WRDS provides a description of the date variables. anndats is the first day that an analyst set their estimate for the earnings per share and the revdats is the last day that the analyst confirmed their estimate. We will use revdats as the defacto date that the analyst provided the estimate. anndats_act is the earnings announcement date. value is the estimated EPS by the analyst and actual is the actual EPS as announced by the firm. pdf is flag whether the EPS if for the primary share class or on a diluted basis. I included both and that is probably appropriate for this paper. fpi is the forecast period indicator if we set this to “6”, we get the earnings estimates that are done in the quarter before the earnings announcements. All these variables can be verified in the data descriptions on WRDS. As you can see, it’s quite important if you work with data that you have not collected yourself to read the data descriptions.\n\nibes_query &lt;- tbl(wrds, in_schema(\"ibes\", \"det_epsus\")) %&gt;%\n  select(ticker, cusip, fpi, anndats, revdats, pdf, value, anndats_act, actual, analys) %&gt;%\n  filter(fpi == \"6\", ticker %in% tickers, !is.null(actual)) %&gt;%\n  filter(anndats_act &gt;= begin_date, anndats_act &lt;= end_date)\nann_ibes &lt;- collect(ibes_query)\nsaveRDS(ann_ibes, here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\nglimpse(ann_ibes)\n\nRows: 1,046,666\nColumns: 10\n$ ticker      &lt;chr&gt; \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\"…\n$ cusip       &lt;chr&gt; \"0039241X\", \"0039241X\", \"0039241X\", \"0039241X\", \"0039241X\"…\n$ fpi         &lt;chr&gt; \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\"…\n$ anndats     &lt;date&gt; 2005-10-26, 2005-10-27, 2005-10-27, 2005-10-31, 2005-11-2…\n$ revdats     &lt;date&gt; 2005-12-23, 2005-10-29, 2006-01-02, 2006-01-31, 2005-12-2…\n$ pdf         &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"P\"…\n$ value       &lt;dbl&gt; -0.0500, -0.0935, -0.0425, -0.0800, -0.0684, -0.0600, -0.0…\n$ anndats_act &lt;date&gt; 2006-02-01, 2006-02-01, 2006-02-01, 2006-02-01, 2006-02-0…\n$ actual      &lt;dbl&gt; -0.1000, -0.1000, -0.1000, -0.1000, -0.1000, -0.1000, -0.1…\n$ analys      &lt;dbl&gt; 87125, 44775, 43594, 478, 5469, 84303, 43594, 44775, 87125…"
  },
  {
    "objectID": "freaky_friday/download_data.html#compustat",
    "href": "freaky_friday/download_data.html#compustat",
    "title": "Earnings announcements",
    "section": "Compustat",
    "text": "Compustat\nFollowing the paper, we will verify the earnings announcement date in I/B/E/S with the earnings announcement date in Compustat. Given the importance of finding the exact date for an event study, it is not surprising that Dellavigna and Pollet (2009) spent a lot of effort to make sure that they have the date right.\n\ngvkeys &lt;- unique(linking_table$gvkey)\n\nrdq is the earnings announcement date in Compustat. It does not make it easier that different databases use different variable names. That is why it so important to read the documentation of the database.\n\ncompu_query &lt;- tbl(wrds, in_schema(\"comp\", \"fundq\")) %&gt;%\n  filter(rdq &gt;= begin_date, rdq &lt;= end_date, gvkey %in% gvkeys) %&gt;%\n  select(cusip, rdq, gvkey)\nann_compu &lt;- as_tibble(compu_query) %&gt;%\n  mutate(cusip = str_sub(cusip, 1, 8))\nsaveRDS(ann_compu, here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nglimpse(ann_compu)\n\nRows: 327,964\nColumns: 3\n$ cusip &lt;chr&gt; \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"00036110\", \"000…\n$ rdq   &lt;date&gt; 1995-03-15, 1995-07-06, 1995-09-13, 1995-12-12, 1996-03-14, 199…\n$ gvkey &lt;chr&gt; \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001004\", \"001…"
  },
  {
    "objectID": "freaky_friday/download_data.html#combine-announcements",
    "href": "freaky_friday/download_data.html#combine-announcements",
    "title": "Earnings announcements",
    "section": "Combine Announcements",
    "text": "Combine Announcements\nTo combine the two datasets, we will link them through a simplified version of the larger linking table. I will also enforce that the first 6 characters of cusip are the same. I don’t think it is strictly necessary to do that but it does gives us more confidence that the links are of higher quality. We need to match the I/B/E/S data and the Compustat data based on the firm and its earnings announcement date. However, if you read the paper (Dellavigna and Pollet 2009), you will notice that the reason why we want to combine the I/B/E/S and Compustat data is because the date in both datasets does not always match. The paper gets around that by matching earnings announcements if the date is not more than 5 days apart in the two data sources. This is why I create anndat_begin and anndat_end to define the interval in which we want to match the data. Finally, we can calculate the actual event date as the minimum of the date in the I/B/E/S data and the Compustat data (Dellavigna and Pollet 2009) 1.\nYou can also see that I have two lines of code at the start to read in the datasets again. This is not strictly necessary to make this file fully reproducible but it does make debugging the code easier. If I want to make some changes to the code I do not have to download the data again from WRDS. I can just use the one in the data folder.\n\nann_ibes &lt;- readRDS(here(\"data\", \"freaky_friday\", \"ann_ibes.RDS\"))\nann_compu &lt;- readRDS(here(\"data\", \"freaky_friday\", \"ann_compu.RDS\"))\nsimple_link &lt;- linking_table %&gt;%\n  select(ticker, gvkey, permno, cusip) %&gt;%\n  mutate(cusip = str_sub(cusip, end = 6)) %&gt;%\n  distinct()\nearn_ann &lt;- ann_ibes %&gt;%\n  distinct(ticker, actual, pdf, cusip, anndats_act) %&gt;%\n  mutate(cusip = str_sub(cusip, end = 6)) %&gt;%\n  left_join(simple_link, by = join_by(ticker)) %&gt;%\n  filter(!is.na(gvkey), !(cusip.x != cusip.y)) %&gt;%\n  select(-starts_with(\"cusip\")) %&gt;%\n  mutate(anndat_begin = anndats_act - 5, anndat_end = anndats_act + 5) %&gt;%\n  left_join(ann_compu, by = join_by(gvkey == gvkey,\n                                    anndat_begin &lt;= rdq,  anndat_end &gt;= rdq)) %&gt;% \n  filter(!is.na(rdq)) %&gt;%\n  mutate(anndat = pmin(anndats_act, rdq)) %&gt;%\n  select(-anndat_begin, -anndat_end)\n\nWarning in left_join(., simple_link, by = join_by(ticker)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 202 of `x` matches multiple rows in `y`.\nℹ Row 11209 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nsaveRDS(earn_ann, here(\"data\", \"freaky_friday\", \"earn_ann.RDS\"))\nglimpse(earn_ann)\n\nRows: 154,465\nColumns: 9\n$ ticker      &lt;chr&gt; \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0A\", \"AA0A\", …\n$ actual      &lt;dbl&gt; -0.100, -0.040, -0.110, -0.110, -0.080, -0.050, -0.070, 0.…\n$ pdf         &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\"…\n$ anndats_act &lt;date&gt; 2006-02-01, 2004-10-21, 2005-01-26, 2005-04-27, 2006-04-2…\n$ gvkey       &lt;chr&gt; \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081\"…\n$ permno      &lt;dbl&gt; 10560, 10560, 10560, 10560, 10560, 10560, 10560, 10656, 10…\n$ cusip       &lt;chr&gt; \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410\"…\n$ rdq         &lt;date&gt; 2006-02-01, 2004-10-21, 2005-01-26, 2005-04-27, 2006-04-2…\n$ anndat      &lt;date&gt; 2006-02-01, 2004-10-21, 2005-01-26, 2005-04-27, 2006-04-2…\n\n\nearn_ann serves as a linking table to match different earnings announcements (as opposed to firms and their securities in linking_table)\n\n\n\nvariable\ndata source\ndescription\n\n\n\n\nticker\nI/B/E/S\nIdentifier\n\n\nanndats_act\nI/B/E/S\nActual Announcement Date\n\n\ngvkey\nCompustat\nIdentifier\n\n\npermno\nCRSP\nIdentifier\n\n\ncusip\n\nIdentifier of length 6,8 or 9\n\n\nrdq\nCompustat\nActual Announcement Date\n\n\nanndat\n\npmin(rdq, anndats_act)\n\n\n\nanndat is the validated way of calculating the announcement date. However, we need to keep the other dates around because we will need them to link back to the original databases.\nThe paper states that they have 154,051 earnings announcements (Dellavigna and Pollet 2009). We have 154465 earnings announcements."
  },
  {
    "objectID": "freaky_friday/download_data.html#footnotes",
    "href": "freaky_friday/download_data.html#footnotes",
    "title": "Earnings announcements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe need to use pmin so that the mutate statement knows that it needs to take the minimum between the two columns for each row and not over the whole dataset.↩︎\nNevertheless, with some smart programming you can get the tidyverse to be very speedy as well. See further when we calculate the abnormal returns.↩︎"
  },
  {
    "objectID": "freaky_friday/download_linking.html",
    "href": "freaky_friday/download_linking.html",
    "title": "WRDS linking data",
    "section": "",
    "text": "I use four packages on this page and three of them require some more explanation. The here package helps with managing the different files in this larger project. I can refer to different files relative to the root folder all the files are in. The only thing that I need to do is to say where this file is compared to the root folder with the i_am function. You can see the simplified folder structure below for this website. By telling R which file this is, I can easily refer to other files and save the data we are going to download and clean in the data folder. If it is specific to the replication it will go into the data/freaky_friday folder as an .RDS file which is an efficient way of storing R objects.\nlibrary(tidyverse)\nlibrary(here)\ni_am(\"freaky_friday/download_linking.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(RPostgres)\nlibrary(dbplyr)\n\n\nAttaching package: 'dbplyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\nThe second package is the RPostgres package that helps make a connection with the WRDS data sources. The final package is dbplyr which allows us to interact with almost any database with the tidyverse verbs. I’ll demonstrate how below."
  },
  {
    "objectID": "freaky_friday/download_linking.html#ibes",
    "href": "freaky_friday/download_linking.html#ibes",
    "title": "WRDS linking data",
    "section": "I/B/E/S",
    "text": "I/B/E/S\nThe code instructs the WRDS database to get the variables ticker, cusip (another identifier), cname (company name), and sdates (the start date for this ticker) from the ibes.idsum (IBES ID summary) database of WRDS. In this paper, we only want U.S. firms which we do by filtering by usfirm == 1.\n\nibes_query &lt;- tbl(wrds, in_schema(\"ibes\", \"idsum\")) %&gt;%\n  filter(usfirm == 1) %&gt;%\n  select(ticker, cusip, cname, sdates) %&gt;%\n  collect()\n\nWith some R code, we clean the data and save it as a file in the data &gt; wrds folder in our main folder.\n\nibes_id &lt;- as_tibble(ibes_query) %&gt;%\n  rename_all(tolower)\nsaveRDS(ibes_id, here(\"data\", \"wrds\", \"ibes_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#crsp",
    "href": "freaky_friday/download_linking.html#crsp",
    "title": "WRDS linking data",
    "section": "CRSP",
    "text": "CRSP\nFrom the CRSP data, we get the permno and ncusip identifier where ncusip stands for the same cusip identifier as mentioned above. We also have the company name, start date, and end date.\n\ncrsp_query &lt;- tbl(wrds, in_schema(\"crsp\", \"stocknames\")) %&gt;%\n  select(permno, ncusip, comnam, st_date, end_date) %&gt;%\n  collect()\n\n\ncrsp_id &lt;- as_tibble(crsp_query) %&gt;%\n  rename_all(tolower)\nsaveRDS(crsp_id, here(\"data\", \"wrds\", \"crsp_id.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-ibes",
    "href": "freaky_friday/download_linking.html#compustat-with-ibes",
    "title": "WRDS linking data",
    "section": "Compustat with I/B/E/S",
    "text": "Compustat with I/B/E/S\nFrom Compustat we use the security file which has all the financial securities (and their identifiers) that are linked to the firms in Compustat. We select all the variables from that dataset. We only select the ones where the ibes ticker is available so that we can match via the ticker in the I/B/E/S files.\n\ncompu_security_query &lt;- tbl(wrds, in_schema(\"comp\", \"security\")) %&gt;%\n  filter(!is.null(ibtic)) %&gt;%\n  collect()\n\n\ncompu_security &lt;- as_tibble(compu_security_query) %&gt;%\n  rename_all(tolower)\nsaveRDS(compu_security, here(\"data\", \"wrds\", \"compu_security.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#compustat-with-crsp",
    "href": "freaky_friday/download_linking.html#compustat-with-crsp",
    "title": "WRDS linking data",
    "section": "Compustat with CRSP",
    "text": "Compustat with CRSP\nFinally, we get the linking file in compustat. According to the documentation, not all the links are reliable and they advice to use the linktype variable and the usedflag variable to filter only the links that are most reliable. I have implemented the rules that follow best practice according to this tutorial (https://wrds-www.wharton.upenn.edu/pages/wrds-research/applications/linking-databases/linking-crsp-and-compustat/)\n\n# compu_query\ncompu_query &lt;- tbl(wrds, in_schema(\"crsp\", \"ccmxpf_linktable\")) %&gt;%\n  select(gvkey, linktype, usedflag, iid = liid, permno = lpermno, stdt = linkdt, enddt = linkenddt) %&gt;%\n  filter(!is.na(permno), linktype %in% c(\"LU\", \"LC\"), usedflag == 1) %&gt;%\n  select(gvkey, permno, stdt, enddt) %&gt;%\n  distinct()\nsaveRDS(crsp_query, here(\"data\", \"wrds\", \"crsp_compu.RDS\"))"
  },
  {
    "objectID": "freaky_friday/download_linking.html#footnotes",
    "href": "freaky_friday/download_linking.html#footnotes",
    "title": "WRDS linking data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some other overviews available for identifiers that you can use to merge datasets. I am only using the best practice advice according to WRDS.↩︎"
  },
  {
    "objectID": "slides/slides5.html#a-basic-before---after-comparison",
    "href": "slides/slides5.html#a-basic-before---after-comparison",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A basic before - after comparison",
    "text": "A basic before - after comparison\n\n\n\n\n\n\n\nevent\n\n  \n\nEvent Happened\n\n Event Happened   \n\nTreatment\n\n Treatment   \n\nEvent Happened-&gt;Treatment\n\n    \n\nOutcome\n\n Outcome   \n\nTreatment-&gt;Outcome\n\n    \n\nTime\n\n Time   \n\nTime-&gt;Event Happened\n\n    \n\nTime-&gt;Outcome\n\n   \n\n\n\n\n\nChapter 17 Event Studies in Huntington-Klein (2021)\n\nWhere Time could be standing in for a lot of other annoying things that might happen."
  },
  {
    "objectID": "slides/slides5.html#use-data-before-the-event-to-infer-the-counterfactual-outcome-in-yellow",
    "href": "slides/slides5.html#use-data-before-the-event-to-infer-the-counterfactual-outcome-in-yellow",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)",
    "text": "Use Data Before The Event to Infer The Counterfactual Outcome (in Yellow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not observe the yellow/gold returns. We have to estimate them or convince the reader that there are no trends to be expected for theoretical/institutional reasons. I will not go into the details of the estimation here but I will in week 7."
  },
  {
    "objectID": "slides/slides5.html#front-running-information-leaking-and-anticipation-are-all-annoying.",
    "href": "slides/slides5.html#front-running-information-leaking-and-anticipation-are-all-annoying.",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Front-running, information leaking, and anticipation are all annoying.",
    "text": "Front-running, information leaking, and anticipation are all annoying.\n\n\n\n\n\n\n\nevent\n\n  \n\nBefore Block Trade\n\n Before Block Trade   \n\nLarge Block Trade\n\n Large Block Trade   \n\nBefore Block Trade-&gt;Large Block Trade\n\n    \n\nPrice Impact\n\n Price Impact   \n\nLarge Block Trade-&gt;Price Impact\n\n    \n\nTime\n\n Time   \n\nTime-&gt;Before Block Trade\n\n    \n\nTime-&gt;Price Impact\n\n   \n\n\n\n\n\n\n\n\nGo also back to Assignment 2 where we modeled the same problem when investors anticipate a donation.\n\n\n\nTo gauge demand from buyers and potentially gin up interest from sellers, bankers send out lists of shares with upcoming lockup expirations, according to market participants. (Money Stuff, Matt Levine)\nSometimes, bankers also engage in hypothetical conversations with buyers before they have a mandate. Asking prospective buyers whether they might be interested in certain stocks is one thing. But if there are indeed plans afoot for block sales, such conversations, even phrased hypothetically, can tip off savvy money managers. (Money Stuff, Matt Levine)"
  },
  {
    "objectID": "slides/slides5.html#what-if-we-had-an-additional-control-group-to-estimate-the-counterfactual",
    "href": "slides/slides5.html#what-if-we-had-an-additional-control-group-to-estimate-the-counterfactual",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "What if we had an additional control group to estimate the counterfactual?",
    "text": "What if we had an additional control group to estimate the counterfactual?"
  },
  {
    "objectID": "slides/slides5.html#a-simulated-cheap-talk-example-voluntary-disclosure-in-time-2",
    "href": "slides/slides5.html#a-simulated-cheap-talk-example-voluntary-disclosure-in-time-2",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2",
    "text": "A Simulated Cheap Talk Example: Voluntary Disclosure in Time 2\n\nN &lt;- 500\nT &lt;- 2\ntime_effect &lt;- c(3.5, 0)\nrd_did_firm &lt;- tibble(\n  firm = 1:N,\n  performance = runif(N, 1, 10),\n  firm_effect = rnorm(N, 0, 2) + ifelse(performance &lt; 3, 3, 0)\n)\nrd_did_panel &lt;- tibble(\n  firm = rep(1:N, each = T),\n  time = rep(1:T, times = N)) %&gt;%\n  left_join(rd_did_firm, by = \"firm\") %&gt;%\n  mutate(\n    report = ifelse(time == 2, ifelse(performance &gt; 3, 1, 0), 0),\n    noise = rnorm(N*T, 0, 3),\n    profit_report = 6.5 + time_effect[time] + firm_effect + noise,\n    profit_no_report = 1.5 + time_effect[time] + firm_effect + noise,\n    actual_profit = ifelse(report == 1, profit_report, profit_no_report))\n\n\nThe idea is that we have firms who perform well (performance &gt; 3) and firms that perform bad (performance &lt; 3). The firms that perform well will voluntarily disclose a report in time 2. We can see the effect as the difference between time 1 and time 2 for disclosers and non-disclosers.\nImportant: the cost of misreporting is not in calculated in the profit. The reasoning would be that this might be a litigation cost that would only emerge later on."
  },
  {
    "objectID": "slides/slides5.html#the-causal-effects-in-our-simulation",
    "href": "slides/slides5.html#the-causal-effects-in-our-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Causal Effects in Our Simulation",
    "text": "The Causal Effects in Our Simulation\n\nrd_did_panel %&gt;%\n  mutate(causal_effect = profit_report - profit_no_report) %&gt;%\n  group_by(time, report2 = performance &gt; 3) %&gt;%\n  summarise(profit_report = mean(profit_report),\n            profit_no_report = mean(profit_no_report),\n            causal_effect = mean(causal_effect)) %&gt;%\n  kable(digits = 1)\n\n\n\n\ntime\nreport2\nprofit_report\nprofit_no_report\ncausal_effect\n\n\n\n\n1\nFALSE\n13.1\n8.1\n5\n\n\n1\nTRUE\n10.2\n5.2\n5\n\n\n2\nFALSE\n9.4\n4.4\n5\n\n\n2\nTRUE\n6.6\n1.6\n5"
  },
  {
    "objectID": "slides/slides5.html#a-summary-of-the-actual-profits",
    "href": "slides/slides5.html#a-summary-of-the-actual-profits",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "A Summary of The Actual Profits",
    "text": "A Summary of The Actual Profits\n\nrd_did_panel %&gt;%\n  group_by(time, report2 = performance &gt; 3) %&gt;%\n  summarise(actual_profit = mean(actual_profit)) %&gt;%\n  pivot_wider(names_from = time, values_from = actual_profit) %&gt;%\n  kable(digits = 1)\n\n\n\n\nreport2\n1\n2\n\n\n\n\nFALSE\n8.1\n4.4\n\n\nTRUE\n5.2\n6.6"
  },
  {
    "objectID": "slides/slides5.html#regressions",
    "href": "slides/slides5.html#regressions",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Regressions",
    "text": "Regressions\n\ndid_lm &lt;- feols(actual_profit ~ report, data = rd_did_panel)\ndid_sub &lt;- feols(actual_profit ~ report, data = filter(rd_did_panel, time == 2))\ndid_fixed &lt;- feols(actual_profit ~ report | firm, data = rd_did_panel)\ndid_did &lt;- feols(actual_profit ~ report | firm + time, data = rd_did_panel)\nmsummary(list(simple = did_lm, \"time 2\" = did_sub, \"firm FE\" = did_fixed, \"two-way FE\" = did_did),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\nsimple\n time 2\nfirm FE\ntwo-way FE\n\n\n\n\n(Intercept)\n5.580***\n4.380***\n\n\n\n\n\n(0.144)\n(0.308)\n\n\n\n\nreport\n1.005***\n2.206***\n1.403***\n5.091***\n\n\n\n(0.233)\n(0.352)\n(0.208)\n(0.428)\n\n\nNum.Obs.\n1000\n500\n1000\n1000\n\n\nR2\n0.018\n0.073\n0.624\n0.685\n\n\nR2 Within\n\n\n0.071\n0.222\n\n\nRMSE\n3.58\n3.34\n2.22\n2.03\n\n\nStd.Errors\nIID\nIID\nby: firm\nby: firm\n\n\nFE: firm\n\n\nX\nX\n\n\nFE: time\n\n\n\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides5.html#what-if-we-have-three-periods",
    "href": "slides/slides5.html#what-if-we-have-three-periods",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "What if we have three periods?",
    "text": "What if we have three periods?\n\n\n\n\n\n\nNote\n\n\nWe assume that over time investors and regulators get better at detecting when firms exaggerate in their report.\n\n\n\n\nTime 1: Reports are not believable, nobody reports\nTime 2: The biggest exaggerations will be caught, only well performing firms will report and communicate that they are doing excellent.\nTime 3: More subtle exaggerations will be caught. The worst performers will not report at all, the moderate performers will report and say that they will do well, the good performers will report that they are doing excellent.\n\n\n\nSee the Appendix of the assignment for the derivation of the exact parameters."
  },
  {
    "objectID": "slides/slides5.html#setup-of-three-period-simulation",
    "href": "slides/slides5.html#setup-of-three-period-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Setup of three period simulation",
    "text": "Setup of three period simulation\n\nN &lt;- 1000\nT &lt;- 3\ncutoff2 &lt;- 3 # performance cutoff to report for time 1\ncutoff3 &lt;- c(4/3, 4 + 2/3) # performance cutoff to report for time 2\nprofit1 &lt;- 5\nprofit2 &lt;- c(1.5, 6.5) #Profits for time 2 depending on report\nprofit3 &lt;- c(2/3, 3, 7 + 1/3) #Profits for time 2 depending on report\nrd_did3_firm &lt;- tibble(\n  firm = 1:N,\n  performance = runif(N, 0, 10),\n  firm_effect = rnorm(N, 0, 2) + ifelse(performance &lt; cutoff2, 3, 0)\n)"
  },
  {
    "objectID": "slides/slides5.html#three-period-simulation",
    "href": "slides/slides5.html#three-period-simulation",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Three period simulation",
    "text": "Three period simulation\n\nrd_did3_panel &lt;- tibble(\n  firm = rep(1:N, each = T),\n  time = rep(1:T, times = N)) %&gt;%\n  left_join(rd_did3_firm, by = \"firm\") %&gt;%\n  mutate(\n    # When will firms report?\n    report = case_when(\n      time == 1 ~ 0,\n      time == 2 & performance &lt; cutoff2 ~ 0,\n      time == 3 & performance &lt; cutoff3[1] ~ 0,\n      TRUE ~ 1),\n    noise = rnorm(T*N, 0, 5),\n    profit_no_report = firm_effect + noise +\n      case_when(\n        time == 1 ~ profit1,\n        time == 2 ~ profit2[1],\n        time == 3 ~ profit3[1]\n    ),\n    profit_report = firm_effect + noise +\n      case_when(\n        time == 1 ~ profit1,\n        time == 2 ~ profit2[2],\n        time == 3 & performance &lt; cutoff3[2] ~ profit3[2],\n        TRUE ~ profit3[3]\n      ),\n    actual_profit = ifelse(report == 1, profit_report, profit_no_report)\n  )"
  },
  {
    "objectID": "slides/slides5.html#overview-of-4-groups-of-firms",
    "href": "slides/slides5.html#overview-of-4-groups-of-firms",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Overview of 4 groups of firms",
    "text": "Overview of 4 groups of firms\n\nNever reporters\nReporters in year 3\nReporters in year 2 and 3 (Medium)\nReporters in year 2 and 3 (High)\n\n\ncausal_effects &lt;- rd_did3_panel %&gt;%\n  mutate(causal_effect = profit_report - profit_no_report,\n         group = case_when(\n           performance &lt; cutoff3[1] ~ 1,\n           performance &lt; cutoff2 ~ 2,\n           performance &lt; cutoff3[2] ~ 3,\n           TRUE ~ 4\n         )) %&gt;%\n  group_by(time, group) %&gt;%\n  summarise(report = mean(report),\n            N = n(),\n            M_report = mean(profit_report),\n            M_no_report = mean(profit_no_report),\n            M_causal_effect = mean(causal_effect))"
  },
  {
    "objectID": "slides/slides5.html#overview-of-4-groups-of-firms-1",
    "href": "slides/slides5.html#overview-of-4-groups-of-firms-1",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Overview of 4 groups of firms",
    "text": "Overview of 4 groups of firms\n\n\n\n\n\ntime\ngroup\nreport\nN\nM_report\nM_no_report\nM_causal_effect\n\n\n\n\n1\n1\n0\n141\n7.4\n7.4\n0.0\n\n\n1\n2\n0\n159\n7.5\n7.5\n0.0\n\n\n1\n3\n0\n168\n4.9\n4.9\n0.0\n\n\n1\n4\n0\n532\n5.0\n5.0\n0.0\n\n\n2\n1\n0\n141\n9.3\n4.3\n5.0\n\n\n2\n2\n0\n159\n9.3\n4.3\n5.0\n\n\n2\n3\n1\n168\n6.7\n1.7\n5.0\n\n\n2\n4\n1\n532\n6.7\n1.7\n5.0\n\n\n3\n1\n0\n141\n4.7\n2.3\n2.3\n\n\n3\n2\n1\n159\n5.4\n3.1\n2.3\n\n\n3\n3\n1\n168\n2.9\n0.6\n2.3\n\n\n3\n4\n1\n532\n7.2\n0.5\n6.7"
  },
  {
    "objectID": "slides/slides5.html#two-way-fixed-effects",
    "href": "slides/slides5.html#two-way-fixed-effects",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Two-way Fixed Effects",
    "text": "Two-way Fixed Effects\n\ntwoway12 &lt;- feols(actual_profit ~ report | firm + time,\n                  data = filter(rd_did3_panel, time != 3))\ntwoway13 &lt;- feols(actual_profit ~ report | firm + time,\n                  data = filter(rd_did3_panel, time != 2))\ntwoway123 &lt;- feols(actual_profit ~ report | firm + time,\n                  data = rd_did3_panel)"
  },
  {
    "objectID": "slides/slides5.html#separate-2-by-2-effects-are-larger-than-the-total-sample-effect",
    "href": "slides/slides5.html#separate-2-by-2-effects-are-larger-than-the-total-sample-effect",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Separate 2 by 2 effects are larger than the total sample effect",
    "text": "Separate 2 by 2 effects are larger than the total sample effect\n\nmsummary(list(\"time 1 and 2\" = twoway12, \"time 1 and 3\" = twoway13,\n              \"time 1, 2 and 3\" = twoway123), gof_omit = gof_omit,\n         stars = c(\"*\" = .1, \"**\" = .05, \"***\" = .01))\n\n\n\n\n\n time 1 and 2\n time 1 and 3\n time 1, 2 and 3\n\n\n\n\nreport\n4.882***\n5.671***\n4.219***\n\n\n\n(0.488)\n(0.647)\n(0.409)\n\n\nNum.Obs.\n2000\n2000\n3000\n\n\nR2\n0.578\n0.565\n0.437\n\n\nR2 Within\n0.093\n0.066\n0.051\n\n\nRMSE\n3.49\n3.71\n4.15\n\n\nStd.Errors\nby: firm\nby: firm\nby: firm\n\n\nFE: firm\nX\nX\nX\n\n\nFE: time\nX\nX\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not what I would expect. Why would the full sample lead to a smaller effect than all the subsamples?"
  },
  {
    "objectID": "slides/slides5.html#problem-statement",
    "href": "slides/slides5.html#problem-statement",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nFinally, when research settings combine staggered timing of treatment effects and treatment effect heterogeneity across firms or over time, staggered DiD estimates are likely to be biased. In fact, these estimates can produce the wrong sign altogether compared to the true average treatment effects."
  },
  {
    "objectID": "slides/slides5.html#solution",
    "href": "slides/slides5.html#solution",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Solution",
    "text": "Solution\n\nWhile the literature has not settled on a standard, the proposed solutions all deal with the biases arising from the “bad comparisons” problem inherent in TWFE DiD regressions by modifying the set of effective comparison units in the treatment effect estimation process. For example, each alternative estimator ensures that firms receiving treatment are not compared to those that previously received it.\n\n\nAgain, the solution to all our problems is to make sure that we make the right comparison."
  },
  {
    "objectID": "slides/slides5.html#simulation-setup---the-true-average-treatment-effect-of-three-groups",
    "href": "slides/slides5.html#simulation-setup---the-true-average-treatment-effect-of-three-groups",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Simulation Setup - The True Average Treatment Effect of Three Groups",
    "text": "Simulation Setup - The True Average Treatment Effect of Three Groups\n\n\nIt’s clear that the average treatment effect should be positive. It’s positive for every group."
  },
  {
    "objectID": "slides/slides5.html#the-estimated-effect-by-twoway-fixed-effects-of-500-simulations",
    "href": "slides/slides5.html#the-estimated-effect-by-twoway-fixed-effects-of-500-simulations",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Estimated Effect by Twoway Fixed Effects of 500 Simulations",
    "text": "The Estimated Effect by Twoway Fixed Effects of 500 Simulations"
  },
  {
    "objectID": "slides/slides5.html#the-sun2021-solution---restrict-the-sample",
    "href": "slides/slides5.html#the-sun2021-solution---restrict-the-sample",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Sun and Abraham (2021) Solution - Restrict The Sample",
    "text": "The Sun and Abraham (2021) Solution - Restrict The Sample"
  },
  {
    "objectID": "slides/slides5.html#the-estimated-effect-with-the-sun-and-abraham-solution",
    "href": "slides/slides5.html#the-estimated-effect-with-the-sun-and-abraham-solution",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "The Estimated Effect with the Sun and Abraham Solution",
    "text": "The Estimated Effect with the Sun and Abraham Solution"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham-in-practice",
    "href": "slides/slides5.html#sun-and-abraham-in-practice",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham in Practice",
    "text": "Sun and Abraham in Practice\n\nsa_new &lt;- readRDS(here(\"data\", \"sa_new.RDS\"))\nsa_fe &lt;- feols(roa ~ 1 + sunab(treatment_group, year) | firm + year,\n               cluster = \"state\", data = sa_new)\nsa_fe_att &lt;- summary(sa_fe, agg = \"ATT\")\nsa_fe_group &lt;- summary(sa_fe, agg = \"cohort\")\n\n\ntreatment_group: first year of treatment\nyear: calendar year\n\n\nnames(sa_fe$coefficients)\n\n [1] \"year::-18:cohort::1998\" \"year::-17:cohort::1998\" \"year::-16:cohort::1998\"\n [4] \"year::-15:cohort::1998\" \"year::-14:cohort::1998\" \"year::-13:cohort::1998\"\n [7] \"year::-12:cohort::1998\" \"year::-11:cohort::1998\" \"year::-10:cohort::1998\"\n[10] \"year::-9:cohort::1989\"  \"year::-9:cohort::1998\"  \"year::-8:cohort::1989\" \n[13] \"year::-8:cohort::1998\"  \"year::-7:cohort::1989\"  \"year::-7:cohort::1998\" \n[16] \"year::-6:cohort::1989\"  \"year::-6:cohort::1998\"  \"year::-5:cohort::1989\" \n[19] \"year::-5:cohort::1998\"  \"year::-4:cohort::1989\"  \"year::-4:cohort::1998\" \n[22] \"year::-3:cohort::1989\"  \"year::-3:cohort::1998\"  \"year::-2:cohort::1989\" \n[25] \"year::-2:cohort::1998\"  \"year::0:cohort::1989\"   \"year::0:cohort::1998\"  \n[28] \"year::1:cohort::1989\"   \"year::1:cohort::1998\"   \"year::2:cohort::1989\"  \n[31] \"year::2:cohort::1998\"   \"year::3:cohort::1989\"   \"year::3:cohort::1998\"  \n[34] \"year::4:cohort::1989\"   \"year::4:cohort::1998\"   \"year::5:cohort::1989\"  \n[37] \"year::5:cohort::1998\""
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---relative-year",
    "href": "slides/slides5.html#sun-and-abraham---relative-year",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Relative Year",
    "text": "Sun and Abraham - Relative Year\n\nmsummary(sa_fe, gof_omit = gof_omit, stars = stars, statistic = NULL,\n         estimate = \"{estimate} ({std.error}) {stars}\", coef_omit = \"-1\")\n\n\n\n\n\n (1)\n\n\n\n\nyear = -9\n−0.003 (0.006)\n\n\nyear = -8\n0.001 (0.005)\n\n\nyear = -7\n−0.001 (0.006)\n\n\nyear = -6\n−0.002 (0.005)\n\n\nyear = -5\n0.005 (0.005)\n\n\nyear = -4\n0.003 (0.005)\n\n\nyear = -3\n0.004 (0.004)\n\n\nyear = -2\n0.010 (0.006)\n\n\nyear = 0\n0.011 (0.005) *\n\n\nyear = 1\n0.025 (0.006) ***\n\n\nyear = 2\n0.042 (0.006) ***\n\n\nyear = 3\n0.055 (0.005) ***\n\n\nyear = 4\n0.062 (0.005) ***\n\n\nyear = 5\n0.082 (0.006) ***\n\n\nNum.Obs.\n119996\n\n\nR2\n0.727\n\n\nR2 Within\n0.005\n\n\nRMSE\n0.17\n\n\nStd.Errors\nby: state\n\n\nFE: firm\nX\n\n\nFE: year\nX"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---relative-year-1",
    "href": "slides/slides5.html#sun-and-abraham---relative-year-1",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Relative Year",
    "text": "Sun and Abraham - Relative Year\n\niplot(sa_fe)"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---att",
    "href": "slides/slides5.html#sun-and-abraham---att",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - ATT",
    "text": "Sun and Abraham - ATT\n\nmsummary(sa_fe_att, gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n\n\n\n\nATT\n0.046***\n\n\n\n(0.009)\n\n\nNum.Obs.\n119996\n\n\nR2\n0.727\n\n\nR2 Within\n0.005\n\n\nRMSE\n0.17\n\n\nStd.Errors\nby: state\n\n\nFE: firm\nX\n\n\nFE: year\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides5.html#sun-and-abraham---cohort-effects",
    "href": "slides/slides5.html#sun-and-abraham---cohort-effects",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Sun and Abraham - Cohort Effects",
    "text": "Sun and Abraham - Cohort Effects\n\nmsummary(sa_fe_group, gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n\n\n\n\ncohort = 1989\n0.058***\n\n\n\n(0.006)\n\n\ncohort = 1998\n0.034***\n\n\n\n(0.005)\n\n\nNum.Obs.\n119996\n\n\nR2\n0.727\n\n\nR2 Within\n0.005\n\n\nRMSE\n0.17\n\n\nStd.Errors\nby: state\n\n\nFE: firm\nX\n\n\nFE: year\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides5.html#take-away-lessons",
    "href": "slides/slides5.html#take-away-lessons",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Take-away Lessons",
    "text": "Take-away Lessons\n\n\n\n\n\n\nNote\n\n\n\nSimulations are good!\nEverything is a regression (Ok, not really)\nNot all the data should go in the regression"
  },
  {
    "objectID": "slides/slides5.html#abadie2017",
    "href": "slides/slides5.html#abadie2017",
    "title": "Research Design 2: Event Studies and Difference-in-Difference",
    "section": "Abadie et al. (2017)",
    "text": "Abadie et al. (2017)\n\n\n\n\n\n\nNote\n\n\nWhat is the level of the treatment variable? What is the comparison?\n\n\n\n\nMixed race or same-sex race\nState legislation\nCountry legislation\nFirm corporate governance changes"
  },
  {
    "objectID": "slides/slides3.html#an-example-of-a-causal-graph",
    "href": "slides/slides3.html#an-example-of-a-causal-graph",
    "title": "Control Variables",
    "section": "An Example of a Causal Graph",
    "text": "An Example of a Causal Graph\n\n\n\n\n\n\n\nbrand_capital\n\n  \n\nInfoEnvironment\n\n InfoEnvironment   \n\nCreditRating\n\n CreditRating   \n\nInfoEnvironment-&gt;CreditRating\n\n    \n\nFutureCashFlow\n\n FutureCashFlow   \n\nFutureCashFlow-&gt;CreditRating\n\n    \n\nBrandCapital\n\n BrandCapital   \n\nBrandCapital-&gt;InfoEnvironment\n\n    \n\nBrandCapital-&gt;FutureCashFlow"
  },
  {
    "objectID": "slides/slides3.html#difference-with-equilibrium-models",
    "href": "slides/slides3.html#difference-with-equilibrium-models",
    "title": "Control Variables",
    "section": "Difference with equilibrium models",
    "text": "Difference with equilibrium models\n\n\n\n\n\n\nDifferences\n\n\n\nAll the qualitative information about causal relations is in the graph.\nThe equilibrium model directly gives the relation between the variables of interest.\n\ne.g.: Signaling model\n\n\n\n\n\n\n\n\n\n\ndonations\n\n  \n\nPerformance\n\n Performance   \n\nDonation\n\n Donation   \n\nPerformance-&gt;Donation\n\n    \n\nReturn\n\n Return   \n\nDonation-&gt;Return\n\n   \n\n\n\n\n\n\n\nIs there a relation between Performance and Return?\nThere is a parallel with voluntary disclosure of information that the company does not have.\nWhat happens if firms are no longer allowed to donate? What happens if firms are all forced to donate?"
  },
  {
    "objectID": "slides/slides3.html#assignment-csr-report",
    "href": "slides/slides3.html#assignment-csr-report",
    "title": "Control Variables",
    "section": "Assignment: CSR report",
    "text": "Assignment: CSR report\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nPerformance\n\n Performance   \n\nCSR_report\n\n CSR_report   \n\nPerformance-&gt;CSR_report\n\n    \n\nScandals\n\n Scandals   \n\nPerformance-&gt;Scandals\n\n    \n\nReturn\n\n Return   \n\nCSR_report-&gt;Return\n\n    \n\nObserved_Report\n\n Observed_Report   \n\nCSR_report-&gt;Observed_Report\n\n   \n\n\n\n\n\n\n\nWhat is the effect of an increase in scandals?\nWhat happens if we keep the number of scandals constant?"
  },
  {
    "objectID": "slides/slides3.html#causal-graph",
    "href": "slides/slides3.html#causal-graph",
    "title": "Control Variables",
    "section": "Causal Graph",
    "text": "Causal Graph\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nTechIndustry\n\n TechIndustry   \n\nTechIndustry-&gt;Performance\n\n   \n\n\n\n\n\n\nAssume that we are interested in the role of the GC on performance."
  },
  {
    "objectID": "slides/slides3.html#simulation",
    "href": "slides/slides3.html#simulation",
    "title": "Control Variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nset.seed(230383)\nN &lt;- 1000\nds &lt;- tibble(CG = runif(N, 0, 10),\n             TI = rbinom(N, 1, .25)) %&gt;%\n  mutate(Performance =\n           rnorm(N, CG * .15 + TI * 10, 5))\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nlm2 &lt;- lm(Performance ~ CG + TI, data = ds)\n\n\n\ngof_omit &lt;- \"Adj|IC|Log|Pseudo\"\nstars &lt;- c('*' = .1, '**' = .05, '***' = .01)\nmsummary(list(lm1, lm2), stars = stars,\n         gof_omit = gof_omit, output = \"html\")\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n2.976***\n0.036\n\n\n\n(0.408)\n(0.310)\n\n\nCG\n0.081\n0.130**\n\n\n\n(0.073)\n(0.052)\n\n\nTI\n\n10.433***\n\n\n\n\n(0.344)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.001\n0.480\n\n\nRMSE\n6.60\n4.76\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement error typically decreases the effect and this is also what happened in the assignment. For instance, the difference between the donation and no donation should be 3 but it is less."
  },
  {
    "objectID": "slides/slides3.html#causal-graph-1",
    "href": "slides/slides3.html#causal-graph-1",
    "title": "Control Variables",
    "section": "Causal Graph",
    "text": "Causal Graph\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nTechIndustry\n\n TechIndustry   \n\nTechIndustry-&gt;CorporateGovernance\n\n    \n\nTechIndustry-&gt;Performance\n\n   \n\n\n\n\n\n\nThere is only 1 difference between this causal graph and the previous one."
  },
  {
    "objectID": "slides/slides3.html#simulation-1",
    "href": "slides/slides3.html#simulation-1",
    "title": "Control Variables",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nN &lt;- 1000\nds &lt;- tibble(TI = rbinom(N, 1, .25)) %&gt;%\n  mutate(CG = rnorm(N, .5 - TI, .2),\n         Performance = rnorm(N, TI + 0 * CG, 1))\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nlm2 &lt;- lm(Performance ~ CG + TI, data = ds)\n\n\n\nmsummary(list(lm1, lm2), stars = stars,\n         gof_omit = gof_omit, output = \"html\")\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n0.473***\n0.002\n\n\n\n(0.036)\n(0.087)\n\n\nCG\n−0.945***\n−0.090\n\n\n\n(0.067)\n(0.159)\n\n\nTI\n\n1.018***\n\n\n\n\n(0.172)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.165\n0.193\n\n\nRMSE\n1.01\n0.99\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the simulation, we set the effect of GC equal to 0 i.e. there is not effect. The reason to do that is to show why it’s necessary to adjust for TI."
  },
  {
    "objectID": "slides/slides3.html#fixed-effects-as-a-special-case",
    "href": "slides/slides3.html#fixed-effects-as-a-special-case",
    "title": "Control Variables",
    "section": "Fixed effects as a special case",
    "text": "Fixed effects as a special case\n\n\n\n\n\n\nDefinition\n\n\nEffects that are the same for every industry, year, firm, or individual can be adjusted for by using fixed effects.\n\n\n\n\n\n\n\n\n\nBenefits\n\n\nWe do not need to measure the specific variables and can just use indicators variables for each category (e.g. for each different industry).\n\n\n\nSee more in chapter 16 of Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides3.html#fixed-effects-for-industry",
    "href": "slides/slides3.html#fixed-effects-for-industry",
    "title": "Control Variables",
    "section": "Fixed effects (for industry)",
    "text": "Fixed effects (for industry)\n\nCausal DiagramSimulationRegressionsSimulation with correlated fixed effectsRegressions with correlated fixed effects\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nIndustry\n\n Industry   \n\nIndustry-&gt;CorporateGovernance\n\n    \n\nIndustry-&gt;Performance\n\n   \n\n\n\n\n\n\n\n\nNind &lt;- 20\nN &lt;- 5000\ndi &lt;- tibble(\n  ind_number = 1:Nind,\n  ind_CG = rnorm(Nind, 0, 1),\n  ind_performance = rnorm(Nind, 0, 1)\n)\nds &lt;- tibble(\n    ind_number = sample(1:Nind, N, replace = TRUE)) %&gt;%\n  left_join(\n    di, by = \"ind_number\") %&gt;%\n  mutate(\n    CG = rnorm(N, .5 + ind_CG, .2),\n    Performance = rnorm(N, 0 * CG + ind_performance, 1)\n  )\n\n\n\n\nglimpse(di, width = 50)\n\nRows: 20\nColumns: 3\n$ ind_number      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, …\n$ ind_CG          &lt;dbl&gt; 0.23567083, -0.34180999,…\n$ ind_performance &lt;dbl&gt; 1.1335103, 1.2873377, 0.…\n\n\n\n\nglimpse(ds, width = 50)\n\nRows: 5,000\nColumns: 5\n$ ind_number      &lt;int&gt; 9, 12, 5, 7, 6, 8, 15, 1…\n$ ind_CG          &lt;dbl&gt; 1.91243572, 0.16031769, …\n$ ind_performance &lt;dbl&gt; 0.1773941, -0.1250858, 0…\n$ CG              &lt;dbl&gt; 2.3076069, 0.6604549, 1.…\n$ Performance     &lt;dbl&gt; 0.09219279, -0.37244401,…\n\n\n\n\n\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nlm2 &lt;- lm(Performance ~ CG + factor(ind_number), data = ds)\nlibrary(fixest)\nfe &lt;- feols(Performance ~ CG | ind_number, data = ds)\n\n\nmsummary(list(lm1, lm2, fe), gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n0.490***\n1.054***\n\n\n\n\n(0.019)\n(0.080)\n\n\n\nCG\n−0.076***\n0.030\n0.030\n\n\n\n(0.018)\n(0.071)\n(0.061)\n\n\nfactor(ind_number)2\n\n0.207**\n\n\n\n\n\n(0.097)\n\n\n\nfactor(ind_number)3\n\n−0.490***\n\n\n\n\n\n(0.090)\n\n\n\nfactor(ind_number)4\n\n−0.372**\n\n\n\n\n\n(0.161)\n\n\n\nfactor(ind_number)5\n\n−0.410***\n\n\n\n\n\n(0.099)\n\n\n\nfactor(ind_number)6\n\n−1.907***\n\n\n\n\n\n(0.169)\n\n\n\nfactor(ind_number)7\n\n−0.083\n\n\n\n\n\n(0.134)\n\n\n\nfactor(ind_number)8\n\n−1.191***\n\n\n\n\n\n(0.178)\n\n\n\nfactor(ind_number)9\n\n−0.935***\n\n\n\n\n\n(0.148)\n\n\n\nfactor(ind_number)10\n\n−1.712***\n\n\n\n\n\n(0.087)\n\n\n\nfactor(ind_number)11\n\n−1.162***\n\n\n\n\n\n(0.122)\n\n\n\nfactor(ind_number)12\n\n−1.118***\n\n\n\n\n\n(0.090)\n\n\n\nfactor(ind_number)13\n\n0.765***\n\n\n\n\n\n(0.091)\n\n\n\nfactor(ind_number)14\n\n−0.564***\n\n\n\n\n\n(0.104)\n\n\n\nfactor(ind_number)15\n\n0.730***\n\n\n\n\n\n(0.129)\n\n\n\nfactor(ind_number)16\n\n0.689***\n\n\n\n\n\n(0.158)\n\n\n\nfactor(ind_number)17\n\n−2.056***\n\n\n\n\n\n(0.098)\n\n\n\nfactor(ind_number)18\n\n−0.503***\n\n\n\n\n\n(0.091)\n\n\n\nfactor(ind_number)19\n\n−1.907***\n\n\n\n\n\n(0.093)\n\n\n\nfactor(ind_number)20\n\n0.608***\n\n\n\n\n\n(0.086)\n\n\n\nNum.Obs.\n5000\n5000\n5000\n\n\nR2\n0.003\n0.443\n0.443\n\n\nR2 Within\n\n\n0.000\n\n\nRMSE\n1.34\n1.00\n1.00\n\n\nStd.Errors\n\n\nby: ind_number\n\n\nFE: ind_number\n\n\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the factor(ind_number) formulation?\n\n\n\n\n\nNind &lt;- 20\nN &lt;- 5000\ncorrel &lt;- -0.5\ndi &lt;- tibble(\n    ind_number = 1:Nind,\n    ind_CG = rnorm(Nind, 0, 1)) %&gt;%\n  mutate(\n    ind_performance = sqrt(1 - correl^2) * rnorm(Nind, 0, 1) + correl * ind_CG)\nds &lt;- tibble(\n    ind_number = sample(1:Nind, N, replace = TRUE)) %&gt;%\n  left_join(\n    di, by = \"ind_number\") %&gt;%\n  mutate(\n    CG = rnorm(N, .5 + ind_CG, .2),\n    Performance = rnorm(N, 0 * CG + ind_performance, 1)\n  )\n\n\n\n\nglimpse(di, width = 50)\n\nRows: 20\nColumns: 3\n$ ind_number      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, …\n$ ind_CG          &lt;dbl&gt; -0.82999044, 0.44908313,…\n$ ind_performance &lt;dbl&gt; -1.12284290, -0.57326559…\n\n\n\n\nglimpse(ds, width = 50)\n\nRows: 5,000\nColumns: 5\n$ ind_number      &lt;int&gt; 20, 17, 2, 1, 9, 20, 5, …\n$ ind_CG          &lt;dbl&gt; -1.1358960, 0.4833522, 0…\n$ ind_performance &lt;dbl&gt; 1.0252155, -0.6131181, -…\n$ CG              &lt;dbl&gt; -0.79273388, 1.44367931,…\n$ Performance     &lt;dbl&gt; 1.75927497, -1.39745179,…\n\n\n\n\n\n\n\nlm1 &lt;- lm(Performance ~ CG, data = ds)\nfe &lt;- feols(Performance ~ CG | ind_number, data = ds)\n\n\nmsummary(list(lm1, fe), gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n−0.033\n\n\n\n\n(0.022)\n\n\n\nCG\n−0.279***\n−0.012\n\n\n\n(0.015)\n(0.046)\n\n\nNum.Obs.\n5000\n5000\n\n\nR2\n0.067\n0.321\n\n\nR2 Within\n\n0.000\n\n\nRMSE\n1.17\n1.00\n\n\nStd.Errors\n\nby: ind_number\n\n\nFE: ind_number\n\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need the factor(ind_number) formulation?\nThe trick with the correlated ind_CG and ind_performance"
  },
  {
    "objectID": "slides/slides3.html#what-do-fixed-effects-do",
    "href": "slides/slides3.html#what-do-fixed-effects-do",
    "title": "Control Variables",
    "section": "What do fixed effects do?",
    "text": "What do fixed effects do?\n\nFull SampleHighlight IndustryRemove Industry Effects\n\n\n\n\nCode\nfe_plot &lt;-\n  ggplot(ds, aes(y = Performance, x = CG)) +\n  geom_point()\nplot(fe_plot)\n\n\n\n\n\n\n\n\n\nCode\nfe_colour &lt;-\n  ggplot(ds, aes(y = Performance, x = CG,\n                colour = factor(ind_number))) +\n  geom_point() + theme(legend.position=\"none\") \nplot(fe_colour)\n\n\n\n\n\n\n\n\n\nCode\nfe_demean &lt;- group_by(ds, ind_number) %&gt;%\n  mutate(Performance2 = Performance - mean(Performance),\n         CG2 = CG - mean(CG)) %&gt;%\n  ggplot(aes(y = Performance2, x = CG2,\n             colour = factor(ind_number))) +\n  geom_point() + theme(legend.position=\"none\") \nplot(fe_demean)"
  },
  {
    "objectID": "slides/slides3.html#speedboat-racing-example-booth2017",
    "href": "slides/slides3.html#speedboat-racing-example-booth2017",
    "title": "Control Variables",
    "section": "Speedboat Racing Example (Booth and Yamamura 2017)",
    "text": "Speedboat Racing Example (Booth and Yamamura 2017)\n\n\n\nMixed-sex and single-sex races determined by lottery (Randomisation)\n7 race courses\nMultiple races in the same month and location\n\n\n\n\n\n\n\n\n\nspeedboat\n\n  \n\nave_ability\n\n ave_ability   \n\nltime\n\n ltime   \n\nave_ability-&gt;ltime\n\n    \n\nmixed_race\n\n mixed_race   \n\nmixed_race-&gt;ltime\n\n    \n\nfemale\n\n female   \n\nfemale-&gt;ave_ability\n\n    \n\nfemale-&gt;ltime\n\n    \n\ncourse\n\n course   \n\ncircumstances\n\n circumstances   \n\ncourse-&gt;circumstances\n\n    \n\nmonth_location\n\n month_location   \n\nmonth_location-&gt;circumstances\n\n    \n\ncircumstances-&gt;ltime\n\n    \n\ncircumstances-&gt;female"
  },
  {
    "objectID": "slides/slides3.html#results-of-speedboat-races",
    "href": "slides/slides3.html#results-of-speedboat-races",
    "title": "Control Variables",
    "section": "Results of Speedboat Races",
    "text": "Results of Speedboat Races\n\n\nCode\nload(here(\"data\", \"booth_yamamura.Rdata\"))\ntable &lt;- as_tibble(table) %&gt;%\n  select(p_id, women_dat, time, ltime, mix_ra, course,\n         race_id, yrmt_locid)\ntable_clean &lt;- filter(table, complete.cases(table)) %&gt;%\n  select(ltime, women_dat, mix_ra, course, p_id, race_id,\n         yrmt_locid)\nltime_reg &lt;- feols(ltime ~ women_dat : mix_ra + mix_ra\n                   | course + p_id + yrmt_locid,\n                   cluster = \"race_id\",\n                   data = table_clean)\nmsummary(ltime_reg, gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n\n (1)\n\n\n\n\nmix_ra\n−0.002***\n\n\n\n(0.000)\n\n\nwomen_dat × mix_ra\n0.007***\n\n\n\n(0.001)\n\n\nNum.Obs.\n142346\n\n\nR2\n0.361\n\n\nR2 Within\n0.001\n\n\nRMSE\n0.02\n\n\nStd.Errors\nby: race_id\n\n\nFE: course\nX\n\n\nFE: p_id\nX\n\n\nFE: yrmt_locid\nX\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\nThis requires an explanation of interactions. Luckily, it’s relatively simple with two discrete variables.\n| ltime | man    | woman  |\n|-------|:------:|:------:|\n| same  | 0      | 0      |\n| mixed | -0.002 | 0.005  |"
  },
  {
    "objectID": "slides/slides3.html#bad-controls-survival-bias-selection-bias-self-selection-bias",
    "href": "slides/slides3.html#bad-controls-survival-bias-selection-bias-self-selection-bias",
    "title": "Control Variables",
    "section": "Bad Controls, Survival Bias, Selection Bias, Self-Selection Bias",
    "text": "Bad Controls, Survival Bias, Selection Bias, Self-Selection Bias\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nStockPrice\n\n StockPrice   \n\nPerformance-&gt;StockPrice\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nSurvival\n\n Survival   \n\nPerformance-&gt;Survival\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nSP500\n\n SP500   \n\nPerformance-&gt;SP500\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nIPO\n\n IPO   \n\nPerformance-&gt;IPO"
  },
  {
    "objectID": "slides/slides3.html#example-in-the-assignment",
    "href": "slides/slides3.html#example-in-the-assignment",
    "title": "Control Variables",
    "section": "Example in the assignment",
    "text": "Example in the assignment\n\n\n\n\n\n\n\nmeasurement_error\n\n  \n\nPerformance\n\n Performance   \n\nCSR_report\n\n CSR_report   \n\nPerformance-&gt;CSR_report\n\n    \n\nScandals\n\n Scandals   \n\nPerformance-&gt;Scandals\n\n    \n\nReturn\n\n Return   \n\nCSR_report-&gt;Return\n\n    \n\nObserved_Report\n\n Observed_Report   \n\nCSR_report-&gt;Observed_Report"
  },
  {
    "objectID": "slides/slides3.html#simulation-bad-control",
    "href": "slides/slides3.html#simulation-bad-control",
    "title": "Control Variables",
    "section": "Simulation Bad Control",
    "text": "Simulation Bad Control\n\nCausal GraphSimulateResults\n\n\n\n\n\n\n\n\n\nconfounder\n\n  \n\nCorporateGovernance\n\n CorporateGovernance   \n\nPerformance\n\n Performance   \n\nCorporateGovernance-&gt;Performance\n\n    \n\nMarketReturn\n\n MarketReturn   \n\nCorporateGovernance-&gt;MarketReturn\n\n    \n\nPerformance-&gt;MarketReturn\n\n   \n\n\n\n\n\n\n\n\nd &lt;- tibble(corp_gov = rnorm(N, 0, 1)) %&gt;%\n  mutate(acc_profit = rnorm(N, corp_gov, sd = 3),\n         market_return = rnorm(N, 2 * corp_gov + acc_profit,\n                               sd = 3))\nlm1 &lt;- lm(acc_profit ~ corp_gov, data = d)\nlm2 &lt;- lm(acc_profit ~ corp_gov + market_return, data = d)\n\n\n\n\nmsummary(list(lm1, lm2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n0.039\n0.012\n\n\n\n(0.042)\n(0.030)\n\n\ncorp_gov\n1.000***\n−0.489***\n\n\n\n(0.042)\n(0.037)\n\n\nmarket_return\n\n0.498***\n\n\n\n\n(0.007)\n\n\nNum.Obs.\n5000\n5000\n\n\nR2\n0.101\n0.551\n\n\nRMSE\n2.98\n2.11\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides3.html#survival-bias",
    "href": "slides/slides3.html#survival-bias",
    "title": "Control Variables",
    "section": "Survival Bias",
    "text": "Survival Bias\n\nSimulateResults\n\n\n\nd &lt;- mutate(d, survival = if_else(market_return &gt; 5, 1, 0))\n\n\n\n\nlm1 &lt;- lm(acc_profit ~ corp_gov, data = filter(d, survival == 1))\nlm2 &lt;- lm(acc_profit ~ corp_gov * survival, data = d)\nmsummary(list(lm1, lm2), gof_omit = gof_omit, stars = stars)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n3.518***\n−0.549***\n\n\n\n(0.115)\n(0.043)\n\n\ncorp_gov\n−0.137\n0.606***\n\n\n\n(0.095)\n(0.045)\n\n\nsurvival\n\n4.067***\n\n\n\n\n(0.135)\n\n\ncorp_gov × survival\n\n−0.743***\n\n\n\n\n(0.115)\n\n\nNum.Obs.\n853\n5000\n\n\nR2\n0.002\n0.262\n\n\nRMSE\n2.43\n2.70\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne interpretation is that a firm can survive by being lucky (and having high returns) and by having good corporate governance (which translates in high returns). The survivors all are more likely to be having good corporate governance and there is little that can be explained further."
  },
  {
    "objectID": "slides/slides3.html#visualisation-of-colliders-and-interactions",
    "href": "slides/slides3.html#visualisation-of-colliders-and-interactions",
    "title": "Control Variables",
    "section": "Visualisation of Colliders (and Interactions)",
    "text": "Visualisation of Colliders (and Interactions)\n\nFull SampleSurvival HighlightedSurvival Only"
  },
  {
    "objectID": "slides/slides3.html#pitching-format",
    "href": "slides/slides3.html#pitching-format",
    "title": "Control Variables",
    "section": "Pitching Format",
    "text": "Pitching Format\n\n\n\nDescription (Important)\n\nTitle\nResearch Question\nKey Paper\nMotivation\n\nTHREE (IDioT) (Important)\n\nIdea\nData\nTools"
  },
  {
    "objectID": "slides/slides3.html#pitching-format-1",
    "href": "slides/slides3.html#pitching-format-1",
    "title": "Control Variables",
    "section": "Pitching Format",
    "text": "Pitching Format\n\n\n\nDescription (Important)\n\nTitle\nResearch Question\nKey Paper\nMotivation\n\nTHREE (IDioT) (Important)\n\nIdea\nData\nTools\n\n\n\n\nTWO\n\nWhat’s new?\nSo what?\n\nONE contribution\nOther considerations."
  },
  {
    "objectID": "slides/slides4.html#did-we-not-cover-that-already",
    "href": "slides/slides4.html#did-we-not-cover-that-already",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Did we not cover that already?",
    "text": "Did we not cover that already?\n\nYes, but briefly\nYes, but starting from the perspective of a regression (and the code)\n\n\nThe regression perspective is not bad. It means that we can see that more advanced regression techniques can be implemented in our linear regression framework. It’s also how most researchers in accounting and finance have been thought to think about research methods. However, there is a shift coming from economics where the focus is more on the research design."
  },
  {
    "objectID": "slides/slides4.html#the-focus-is-on-research-design",
    "href": "slides/slides4.html#the-focus-is-on-research-design",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The focus is on Research Design",
    "text": "The focus is on Research Design\n\n\n\n\n\n\nImportant\n\n\n\nWhich data should we use?\nWhich comparison identifies the effect that we are interested in?\n\n\n\n\n\n\nIs there sufficient variation that can identify the effect. - See also the pitching document - A specific example is the identification of performance effects\n\n\n\nFor instance Alcohol and Mortality, Chapter 5 in Huntington-Klein (2021).\nIs there sufficient variation in the treatment and the outcome?\nAre we reasonably sure that there are no confounders or only a few and we can measure them?"
  },
  {
    "objectID": "slides/slides4.html#prevously-we-used-models-and-assumptions-to-identify-effects",
    "href": "slides/slides4.html#prevously-we-used-models-and-assumptions-to-identify-effects",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Prevously, we used models and assumptions to identify effects",
    "text": "Prevously, we used models and assumptions to identify effects\n\n\nMathematical models\n\\[\nV = T^{\\alpha_T} \\Bigl( \\frac{K}{\\alpha_K} \\Bigl)^{\\alpha_K}\n                 \\Bigl( \\frac{L}{\\alpha_L} \\Bigl)^{\\alpha_L}\n\\] \\[\n\\alpha_T + \\alpha_K + \\alpha_L = 1\n\\]\n\n\\(V =\\) The value of the firm\n\\(K =\\) Capital of the firm\n\\(L =\\) Labour of the firm\n\\(T =\\) CEO talent/skills/ability/experience\n\n\n\n\nDAGs\n\n\n\n\n\n\n\nspeedboat\n\n  \n\nave_ability\n\n ave_ability   \n\nltime\n\n ltime   \n\nave_ability-&gt;ltime\n\n    \n\nmixed_race\n\n mixed_race   \n\nmixed_race-&gt;ltime\n\n    \n\nfemale\n\n female   \n\nfemale-&gt;ave_ability\n\n    \n\nfemale-&gt;ltime\n\n    \n\ncourse\n\n course   \n\ncircumstances\n\n circumstances   \n\ncourse-&gt;circumstances\n\n    \n\nmonth_location\n\n month_location   \n\nmonth_location-&gt;circumstances\n\n    \n\ncircumstances-&gt;ltime\n\n    \n\ncircumstances-&gt;female"
  },
  {
    "objectID": "slides/slides4.html#just-focus-on-a-setting-where-we-are-confident-in-the-assumptions",
    "href": "slides/slides4.html#just-focus-on-a-setting-where-we-are-confident-in-the-assumptions",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Just focus on a setting where we are confident in the assumptions",
    "text": "Just focus on a setting where we are confident in the assumptions\n\n\nActual random assignment\nSpeedboat racing, game shows, Vietnam draft\nNatural experiments\nSee Gippel, Smith, and Zhu (2015), Chapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nPolicy Changes\nChapter 18, Difference-in-Difference in Huntington-Klein (2021)\nDiscrete cutoffs\ne.g. WAM &gt; 75, Chapter 20 Regression Continuity Design in Huntington-Klein (2021)\nUnexpected news\nChapter 17 Event Studies in Huntington-Klein (2021)\n\n\n\nNatural experiments is not the best terminology because most of these instances are not natural nor real experiments. Nevertheless, I still prefer the name over an instrumental variable approach. In too many proposals, I read an off hand comment that the student proposes to use a robustness test where they are going to use an instrumental variable approach. My answer to that is (1) if you have a natural experiment where you can exploit an instrumental variable, this should be the main analysis and (2) instrumental variables need to be defended as a research design based on your understanding of the setting. Calling the design a natural experiment forces you to think more about the experiment (i.e. the research design)."
  },
  {
    "objectID": "slides/slides4.html#look-for-these-designs",
    "href": "slides/slides4.html#look-for-these-designs",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Look for these designs!",
    "text": "Look for these designs!\n\n\nBased on your understanding of the industry and setting or the Data Generating Process\nWhen you read good papers for this unit and other units.\n\n\n\nThis is one of the main reasons that I want you to read broadly. It is unlikely that you will find a paper with a good research design exactly for the research question that you are interested in. However, you might find inspiration in similar or related fields that help you to design a better study for the research question that you are interested in."
  },
  {
    "objectID": "slides/slides4.html#what-effect-can-we-identify",
    "href": "slides/slides4.html#what-effect-can-we-identify",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "What effect can we identify?",
    "text": "What effect can we identify?\n\nAverage Treatment Effect\nAverage Treatment on the Treated\nAverage Treatment on the Untreated\nLocal Average Treatment Effect\nWeigthed Average Treatment Effect\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)\n\n\nDo you have an example of an effect that we might be interested in in Accounting and Finance?\nAverage implies that not all firms will respond the same to the treatment. This is the source of a lot trouble.\nAverage over which population?\nHow would you put these different effects in your own words?\nWATE is evil and I am going to largely ignore it."
  },
  {
    "objectID": "slides/slides4.html#it-all-depends-on-where-the-variation-is-coming-from.",
    "href": "slides/slides4.html#it-all-depends-on-where-the-variation-is-coming-from.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "It all depends on where the variation is coming from.",
    "text": "It all depends on where the variation is coming from.\n\n\n\n\n\n\nWarning\n\n\nDifferent firms react differently and are differently represented in the control group and the treatment group.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith actual random assignment, you probably have an ATE for the population that received the assignment.\nIf you can use a control group because that is what the treated group would look like if they were not treated, you probably have an ATT.\nIf you use a natural experiment to identify part of the variation, you probably have a LATE.\n\n\n\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#why-do-we-care",
    "href": "slides/slides4.html#why-do-we-care",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Why do we care?",
    "text": "Why do we care?\n\n\n\n\n\n\nResearch Design\n\n\nThere is a deep connection between the variation in your research design and the effect you can identify.\n\n\n\n\n\n\n\n\n\n\nPolicy Implications\n\n\nWhether your study has implications for “regulators and investors” depends heavily on the type of effect you can identify.\n\n\n\nChapter 10, Treatment Effects in Huntington-Klein (2021)\n\nThat is the setting of your data determines which research design you can use. The research design determines which effect you can identify. The effect you can identify determines which conclusions you can draw."
  },
  {
    "objectID": "slides/slides4.html#generate-the-data",
    "href": "slides/slides4.html#generate-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Generate the Data",
    "text": "Generate the Data\n\nN &lt;- 1000\nrd1 &lt;- tibble(\n  firm = 1:N,\n  high_performance = rbinom(N, 1, 0.5),\n  noise = rnorm(N, 0, 3)\n) %&gt;%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_donation = 4 - 8 / performance + noise,\n    payoff_no_donation = 1 + noise\n  )\nglimpse(rd1) \n\nRows: 1,000\nColumns: 7\n$ firm               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …\n$ high_performance   &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1…\n$ noise              &lt;dbl&gt; 0.8634427, 3.9991062, -2.208085…\n$ donation           &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1…\n$ performance        &lt;dbl&gt; 4, 1, 4, 1, 1, 1, 1, 4, 4, 4, 4…\n$ payoff_donation    &lt;dbl&gt; 2.863443e+00, -8.938289e-04, -2…\n$ payoff_no_donation &lt;dbl&gt; 1.8634427, 4.9991062, -1.208085…\n\n\n\n\nWhat is the effect that we are we interested in?\nWhat are the policy implications?"
  },
  {
    "objectID": "slides/slides4.html#have-a-look-at-the-data",
    "href": "slides/slides4.html#have-a-look-at-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Have a look at the data",
    "text": "Have a look at the data\n\n\nWe will talk more about the pivot_wider and pivot_longer functions in week 7."
  },
  {
    "objectID": "slides/slides4.html#have-a-second-look-at-the-data",
    "href": "slides/slides4.html#have-a-second-look-at-the-data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Have a second look at the data",
    "text": "Have a second look at the data"
  },
  {
    "objectID": "slides/slides4.html#real-data-does-not-have-the-counterfactuals.-we-only-observe-blue",
    "href": "slides/slides4.html#real-data-does-not-have-the-counterfactuals.-we-only-observe-blue",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Real data does not have the counterfactuals. We only observe blue!",
    "text": "Real data does not have the counterfactuals. We only observe blue!\n\n\n\n\n\n\n\nNote\n\n\nThe actual sample determines which comparisons we can make.\n\n\n\n\nWhy does this work? What effect are we identifying and how."
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-simulated-example-with-averages",
    "href": "slides/slides4.html#lets-redo-the-simulated-example-with-averages",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the simulated example with averages",
    "text": "Let’s redo the simulated example with averages\n\nrd1 %&gt;%\n  mutate(causal_effect = payoff_donation - payoff_no_donation) %&gt;%\n  summarise(M_causal = mean(causal_effect),\n            sd_causal = sd(causal_effect),\n            N = n()) %&gt;%\n  knitr::kable(format = \"markdown\", digits = 2)\n\n\n\n\nM_causal\nsd_causal\nN\n\n\n\n\n-2.14\n3\n1000\n\n\n\n\n\n\n\nThe causal effect of donating for each firm is difference in payoff between donating and not donating.\nWhat effect are we estimating here?"
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-simulated-example-with-averages-1",
    "href": "slides/slides4.html#lets-redo-the-simulated-example-with-averages-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the simulated example with averages",
    "text": "Let’s redo the simulated example with averages\n\n\n\n\n\nM_causal\nsd_causal\nN\n\n\n\n\n-2.14\n3\n1000\n\n\n\n\n\n\nrd1 %&gt;%\n  mutate(causal_effect = payoff_donation - payoff_no_donation) %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_causal = mean(causal_effect),\n            sd_causal = sd(causal_effect),\n            N = n()) %&gt;%\n  knitr::kable(format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_causal\nsd_causal\nN\n\n\n\n\n0\n-5\n0\n523\n\n\n1\n1\n0\n477"
  },
  {
    "objectID": "slides/slides4.html#lets-redo-the-regression-with-averages",
    "href": "slides/slides4.html#lets-redo-the-regression-with-averages",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Let’s redo the regression with averages",
    "text": "Let’s redo the regression with averages\n\nsummary_data  &lt;- rd1 %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-4.17\n0.83\n\n\n1\n1.76\n0.76\n\n\n\n\ncausal_effect_true &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 1]\ncausal_effect_reg &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 0]\n\n\n\n\n\n\n\nNote\n\n\n\nThe true ATT is 1\nThe effect estimated by the regression is 0.926"
  },
  {
    "objectID": "slides/slides4.html#if-you-do-not-believe-me-here-is-the-regression",
    "href": "slides/slides4.html#if-you-do-not-believe-me-here-is-the-regression",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "If you do not believe me, here is the regression",
    "text": "If you do not believe me, here is the regression\n\n\n\nrd1 &lt;- mutate(rd1, actual_payoff =\n       ifelse(donation, payoff_donation, payoff_no_donation))\nols &lt;- feols(actual_payoff ~ donation, data = rd1)\n\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.833***\n\n\n\n(0.131)\n\n\ndonation\n0.926***\n\n\n\n(0.190)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides4.html#what-could-possibly-go-wrong",
    "href": "slides/slides4.html#what-could-possibly-go-wrong",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "What could possibly go wrong?",
    "text": "What could possibly go wrong?\n\nrd2 &lt;- tibble(\n  high_performance = rbinom(N, 1, 0.5),\n  noise = rnorm(N, 0, 3)) %&gt;%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_donation = 4 - 8 / performance + noise,\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + noise\n  )"
  },
  {
    "objectID": "slides/slides4.html#causal-effect-estimates-with-a-confounder",
    "href": "slides/slides4.html#causal-effect-estimates-with-a-confounder",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Effect Estimates with a Confounder",
    "text": "Causal Effect Estimates with a Confounder\n\nsummary_data  &lt;- rd2 %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, format = \"markdown\", digits = 2)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-4.01\n1.99\n\n\n1\n1.97\n0.97\n\n\n\n\ncausal_effect_true &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 1]\ncausal_effect_reg &lt;-\n  summary_data$M_payoff_donation[summary_data$donation == 1] -\n  summary_data$M_payoff_no_donation[summary_data$donation == 0]\n\n\nThe true ATT is 1\nThe effect estimated by the regression is -0.024"
  },
  {
    "objectID": "slides/slides4.html#where-is-the-variation-coming-from",
    "href": "slides/slides4.html#where-is-the-variation-coming-from",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Where is the variation coming from?",
    "text": "Where is the variation coming from?\n\n\n\n\n\n\nWe need firms that make mistakes\n\n\n\nFirms that should donate but do not always do it.\nFirms that should not donate but sometimes donate."
  },
  {
    "objectID": "slides/slides4.html#panel-data-simulation-100-firms",
    "href": "slides/slides4.html#panel-data-simulation-100-firms",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Panel Data Simulation (100 firms)",
    "text": "Panel Data Simulation (100 firms)\n\nN &lt;- 100\nrd_firm &lt;- tibble(\n  firm = 1:N,\n  high_performance = rbinom(N, 1, 0.5),\n  other_payoff = rnorm(N, 0, 3)) %&gt;%\n  mutate(\n    donation = high_performance,\n    performance = ifelse(high_performance == 1, 4, 1),\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + other_payoff,\n    payoff_donation = 4 - 8/performance + other_payoff\n  )\nsummary_data  &lt;- rd_firm %&gt;%\n  group_by(donation) %&gt;%\n  summarise(M_payoff_donation = mean(payoff_donation),\n            M_payoff_no_donation = mean(payoff_no_donation))\nknitr::kable(summary_data, digits = 1)\n\n\n\n\ndonation\nM_payoff_donation\nM_payoff_no_donation\n\n\n\n\n0\n-3.9\n2.1\n\n\n1\n1.8\n0.8"
  },
  {
    "objectID": "slides/slides4.html#panel-data-simulation-10-time-periods",
    "href": "slides/slides4.html#panel-data-simulation-10-time-periods",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Panel Data Simulation (10 time periods)",
    "text": "Panel Data Simulation (10 time periods)\nThe variation comes from high performers not donating some years\n\nT &lt;- 10\nrd_panel_forget &lt;- tibble(\n  firm = rep(1:N, each = T),\n  year = rep(1:T, times = N)) %&gt;%\n  left_join(rd_firm, by = \"firm\") %&gt;%\n  mutate(forget_donation = rbinom(N * T, 1, plogis(-other_payoff)),\n         actual_donation = (1 - forget_donation) * donation,\n         actual_payoff = ifelse(actual_donation == 1,\n                                payoff_donation, payoff_no_donation))\n\n\n\nThe way we simulate the data reflects the firm fixed effects and the time varying effects.\nWhich effect are we identifying with this sample?"
  },
  {
    "objectID": "slides/slides4.html#the-new-assignment",
    "href": "slides/slides4.html#the-new-assignment",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The New Assignment",
    "text": "The New Assignment\n\nRun a fixed effect model and interpret the result\nCreate a new dataset where all firms make mistakes\nRun a fixed effect model and interpret the result"
  },
  {
    "objectID": "slides/slides4.html#causal-diagram",
    "href": "slides/slides4.html#causal-diagram",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Diagram",
    "text": "Causal Diagram\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx-&gt;y\n\n    \n\ncollider\n\n collider   \n\nx-&gt;collider\n\n    \n\ny-&gt;collider\n\n    \n\nconfounder\n\n confounder   \n\nconfounder-&gt;x\n\n    \n\nconfounder-&gt;y"
  },
  {
    "objectID": "slides/slides4.html#causal-diagram-1",
    "href": "slides/slides4.html#causal-diagram-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Causal Diagram",
    "text": "Causal Diagram\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx-&gt;y\n\n    \n\ncollider\n\n collider   \n\nx-&gt;collider\n\n    \n\ny-&gt;collider\n\n    \n\nconfounder\n\n confounder   \n\nconfounder-&gt;x\n\n    \n\nconfounder-&gt;y\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\nrandomisation\n\n  \n\nx\n\n x   \n\ny\n\n y   \n\nx-&gt;y\n\n    \n\ncollider\n\n collider   \n\nx-&gt;collider\n\n    \n\ny-&gt;collider\n\n    \n\niv\n\n iv   \n\niv-&gt;x\n\n    \n\nrandom\n\n random   \n\nrandom-&gt;iv\n\n    \n\nconfounder\n\n confounder   \n\nconfounder-&gt;x\n\n    \n\nconfounder-&gt;y\n\n   \n\n\n\n\n\n\n\nSee Instrumental Variables, Chapter 19 in Huntington-Klein (2021).\n\nMechanically, there are two regressions. (2-stage-least-squares) 1. Use the IV to estimate the randomly generated variation in X -&gt; fitted(X) 2. Use fitted(X) to estimate the effect of random variation in X on Y"
  },
  {
    "objectID": "slides/slides4.html#simulation-and-implementation-with-fixest",
    "href": "slides/slides4.html#simulation-and-implementation-with-fixest",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation and Implementation with fixest",
    "text": "Simulation and Implementation with fixest\n\n#|label: simulation-iv\nd &lt;- tibble(\n  iv = rnorm(N, 0, 1),\n  confounder = rnorm(N, 0, 1)) %&gt;%\n  mutate(\n    x = rnorm(N, .6 * iv - .6 * confounder, .6),\n    y = rnorm(N, .6 * x + .6 * confounder, .6),\n    survival = if_else(y &gt; 0, 1, 0)\n  )\nsurv &lt;- filter(d, survival == 1)\nlm1 &lt;- lm(y ~ x, d)\nlm2 &lt;- lm(y ~ x + confounder, d)\nlm3 &lt;- lm(y ~ x, surv)\nlm4 &lt;- lm(y ~ x + confounder, surv)\niv1 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = d)\niv2 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = surv)\n\n\nAll the exogenous variable are in the tibble statement, all the endogenous variables are in the mutate statement. That is not a coincidence. It also highlights the value and tight link between being able to simulate your theory and understanding it.\nNote, the collider bias is the biggest problem if the selection bias is on both x and y because then the collider bias effects the first stage regressions."
  },
  {
    "objectID": "slides/slides4.html#simulation-results-with-a-real-effect-of-0.6",
    "href": "slides/slides4.html#simulation-results-with-a-real-effect-of-0.6",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation results with a real effect of 0.6",
    "text": "Simulation results with a real effect of 0.6\n\nmsummary(list(\"confounded\" = lm1, \"with control\" = lm2, \"collider\" = lm3, \"collider\" = lm4,\n              \"iv no collider\" = iv1, \"iv with collider\" = iv2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\nconfounded\nwith control\ncollider\ncollider\niv no collider\niv with collider\n\n\n\n\n(Intercept)\n−0.082\n−0.082\n0.591***\n0.442***\n−0.095\n0.546***\n\n\n\n(0.082)\n(0.061)\n(0.072)\n(0.089)\n(0.104)\n(0.082)\n\n\nx\n0.257***\n0.595***\n0.103\n0.293***\n\n\n\n\n\n(0.081)\n(0.070)\n(0.067)\n(0.097)\n\n\n\n\nconfounder\n\n0.644***\n\n0.248**\n\n\n\n\n\n\n(0.070)\n\n(0.095)\n\n\n\n\nfit_x\n\n\n\n\n0.874***\n0.271**\n\n\n\n\n\n\n\n(0.195)\n(0.129)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not strictly a collider because there is no effect of x on survival. However, it already shows that there are problems with “simple” selection bias."
  },
  {
    "objectID": "slides/slides4.html#simulation-without-an-effect",
    "href": "slides/slides4.html#simulation-without-an-effect",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation without an effect",
    "text": "Simulation without an effect\n\nd &lt;- tibble(\n  iv = rnorm(N, 0, 1),\n  confounder = rnorm(N, 0, 1)) %&gt;%\n  mutate(\n    x = rnorm(N, .6 * iv - .6 * confounder, .6),\n    y = rnorm(N, .6 * confounder, .6),\n    survival = if_else(y &gt; 0, 1, 0)\n  )\nsurv &lt;- filter(d, survival == 1)\nlm1 &lt;- lm(y ~ x, d)\nlm2 &lt;- lm(y ~ x + confounder, d)\nlm3 &lt;- lm(y ~ x, surv)\nlm4 &lt;- lm(y ~ x + confounder, surv)\niv1 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = d)\niv2 &lt;- feols(y ~ 1 | 0 | x ~ iv, data = surv)"
  },
  {
    "objectID": "slides/slides4.html#simulation-without-an-effect-1",
    "href": "slides/slides4.html#simulation-without-an-effect-1",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Simulation without an effect",
    "text": "Simulation without an effect\n\nmsummary(list(\"confounded\" = lm1, \"with control\" = lm2, \"collider\" = lm3, \"collider\" = lm4,\n              \"iv no collider\" = iv1, \"iv with collider\" = iv2),\n         gof_omit = gof_omit, stars = stars)\n\n\n\n\n\nconfounded\nwith control\ncollider\ncollider\niv no collider\niv with collider\n\n\n\n\n(Intercept)\n0.017\n0.005\n0.696***\n0.483***\n0.057\n0.714***\n\n\n\n(0.075)\n(0.058)\n(0.083)\n(0.077)\n(0.083)\n(0.087)\n\n\nx\n−0.125*\n0.151**\n−0.020\n0.118*\n\n\n\n\n\n(0.069)\n(0.064)\n(0.080)\n(0.068)\n\n\n\n\nconfounder\n\n0.619***\n\n0.484***\n\n\n\n\n\n\n(0.077)\n\n(0.093)\n\n\n\n\nfit_x\n\n\n\n\n0.181\n0.035\n\n\n\n\n\n\n\n(0.112)\n(0.113)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides4.html#examplesitting-duck-governors-by-falk2018",
    "href": "slides/slides4.html#examplesitting-duck-governors-by-falk2018",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Example:Sitting Duck Governors by Falk and Shelton (2018)",
    "text": "Example:Sitting Duck Governors by Falk and Shelton (2018)\n\n\n\n\n\n\nNote\n\n\n\nResearch Question: Does political uncertainty effect investment?\nMore uncertainty in a state when governor does not come up for reelection.\nState level laws with term limits (~ Random)\n\n\n\n\n\nAn exercise to be run in class"
  },
  {
    "objectID": "slides/slides4.html#data",
    "href": "slides/slides4.html#data",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Data",
    "text": "Data\n\nlibrary(readit)\nduck &lt;- readit(here(\"data\", \"LameDuckData.dta\")) %&gt;%\n  select(-starts_with(\"nstate\"), -starts_with(\"stdum\"),\n         -starts_with(\"yd_alt\")) %&gt;%\n  group_by(statename) %&gt;%\n  arrange(year) %&gt;%\n  mutate(log_I_1 = lag(log_I), log_I_2 = lag(log_I, 2),\n         log_Y_1 = lag(log_Y), log_Y_2 = lag(log_Y, 2),\n         log_real_GDP_1 = lag(log_real_GDP),\n         log_real_GDP_2 = lag(log_real_GDP, 2)) %&gt;%\n  ungroup() %&gt;%\n  arrange(statename) %&gt;%\n  filter(year &gt;= 1967, year &lt;= 2004)"
  },
  {
    "objectID": "slides/slides4.html#reduced-form",
    "href": "slides/slides4.html#reduced-form",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Reduced Form",
    "text": "Reduced Form\n\nform_red &lt;- formula(\n  log_I ~ gov_exogenous_middling + log_I_1 + log_I_2 +\n  log_Y + log_Y_1 + log_Y_2 + log_real_GDP + log_real_GDP_1 +\n  log_real_GDP_2 | statename\n  )\nred_reg &lt;- feols(form_red, data = duck)"
  },
  {
    "objectID": "slides/slides4.html#stage-least-squares-2sls",
    "href": "slides/slides4.html#stage-least-squares-2sls",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "2 Stage Least Squares (2SLS)",
    "text": "2 Stage Least Squares (2SLS)\n\nform_iv &lt;- formula(log_I ~ log_I_1 + log_I_2 +\n  log_Y + log_Y_1 + log_Y_2 + log_real_GDP + log_real_GDP_1 +\n  log_real_GDP_2\n  # fixed effects\n  | statename\n  # 1st regression\n  | uncertainty_continuous ~ gov_exogenous_middling\n  )\niv_reg &lt;- feols(form_iv, data = duck)"
  },
  {
    "objectID": "slides/slides4.html#results",
    "href": "slides/slides4.html#results",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Results",
    "text": "Results\n\ncoef_map = c(\"gov_exogenous_middling\" = \"lame duck governor\",\n             \"fit_uncertainty_continuous\" = \"uncertainty\")\nmsummary(list(\"reduced\" = red_reg,\n              \"first stage iv\" = summary(iv_reg, stage = 1),\n              \"second stage iv\" = iv_reg),\n         gof_omit = gof_omit, stars = stars,\n         coef_map = coef_map)\n\n\n\n\n\nreduced\nfirst stage iv\nsecond stage iv\n\n\n\n\nlame duck governor\n−0.049**\n1.801***\n\n\n\n\n(0.021)\n(0.112)\n\n\n\nuncertainty\n\n\n−0.027**\n\n\n\n\n\n(0.012)\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "slides/slides4.html#diagnostics-test-for-endogeneity-durbin-wu-hausmann",
    "href": "slides/slides4.html#diagnostics-test-for-endogeneity-durbin-wu-hausmann",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Diagnostics: Test for endogeneity (Durbin-Wu-Hausmann)",
    "text": "Diagnostics: Test for endogeneity (Durbin-Wu-Hausmann)\n\n\n\n\n\n\nNote\n\n\nIs the IV result different from the OLS result?\n\n\n\n\nsumm_iv &lt;- summary(iv_reg)\nsumm_1st &lt;- summary(iv_reg, stage = 1)\nsumm_iv$iv_wh$stat  # iv wu hausmann\n\n[1] 3.829033\n\nsumm_iv$iv_wh$p     # iv wu hausmann\n\n[1] 0.05054677\n\n\nInstrumental Variables, Chapter 19 in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#diagnostics-test-for-weak-instrument",
    "href": "slides/slides4.html#diagnostics-test-for-weak-instrument",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Diagnostics: Test for weak instrument",
    "text": "Diagnostics: Test for weak instrument\n\n\n\n\n\n\nNote\n\n\nIs the instrument predicting the variable we want it to predict?\n\n\n\n\nfitstat(iv_reg, type = \"ivf\")\n\nF-test (1st stage), uncertainty_continuous: stat = 331.0, p &lt; 2.2e-16, on 1 and 1,640 DoF."
  },
  {
    "objectID": "slides/slides4.html#new-assignment",
    "href": "slides/slides4.html#new-assignment",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "New Assignment",
    "text": "New Assignment\nLet’s assume that firms are less likely to donate when there is a local election\n\nN &lt;- 5000\nrd_iv_el &lt;- tibble(\n  high_performance = rbinom(N, 1, .5),\n  extra_payoff = rnorm(N, 0, 3),\n  local_election = rbinom(N, 1, .33)) %&gt;%\n  mutate(\n    actual_donation = ifelse(high_performance == 1, 1 - local_election, 0),\n    payoff_donation = ifelse(high_performance == 1, 2, - 4) + extra_payoff,\n    payoff_no_donation = ifelse(high_performance == 1, 1, 2) + extra_payoff,\n    actual_payoff = ifelse(actual_donation == 1,\n                           payoff_donation, payoff_no_donation))\n\n\n\nWhich effect can we identify with this data?\nRun the instrumental variable analyses and interpret the results."
  },
  {
    "objectID": "slides/slides4.html#this-paper-is-a-finished-product-your-pitch-proposal-or-dissertation-is-not.",
    "href": "slides/slides4.html#this-paper-is-a-finished-product-your-pitch-proposal-or-dissertation-is-not.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "This paper is a finished product, your pitch, proposal, or dissertation is not.",
    "text": "This paper is a finished product, your pitch, proposal, or dissertation is not.\nWe are grateful to Michael Roberts (the Editor), the Associate Editor, two anonymous referees, Marianne Bertrand, Ing-Haw Cheng, Ken French, Ed Glaeser, Todd Gormley, Ben Iverson (discus- sant), Steve Kaplan, Borja Larrain (discussant), Jonathan Lewellen, Katharina Lewellen, David Matsa (discussant), David Metzger (discussant), Toby Moskowitz, Candice Prendergast, Enrichetta Ravina (discussant), Amit Seru, and Wei Wang (discussant) for helpful suggestions. We thank seminar participants at AFA, BYU, CICF Conference, Depaul, Duke, Gerzensee ESSFM, Harvard, HKUST Finance Symposium, McGill Todai Conference, Finance UC Chile, Helsinki, IDC Herzliya Finance Conference, NBER Corporate Finance and Personnel Meetings, SEC, Simon Fraser Uni- versity, Stanford, Stockholm School of Economics, University of Amsterdam, UC Berkeley, UCLA, and Wharton for helpful comments. We thank David Yermack for his generosity in sharing data. We thank Matt Turner at Pearl Meyer, Don Delves at the Delves Group, and Stephen O’Byrne at Shareholder Value Advisors for helping us understand the intricacies of executive stock option plans. Menaka Hampole provided excellent research assistance. We acknowledge financial support from the Initiative on Global Markets.\n\nOn the one hand, we do not expect you to come up with a design like this. On the other hand, why not use these hard won insights."
  },
  {
    "objectID": "slides/slides4.html#this-paper-has-1-one-research-question.-this-is-a-good-thing",
    "href": "slides/slides4.html#this-paper-has-1-one-research-question.-this-is-a-good-thing",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "This paper has 1 (one!) research question. This is a good thing!",
    "text": "This paper has 1 (one!) research question. This is a good thing!\n\nIt’s not necessarily advantageous to have too many hypotheses. You want to answer one question well."
  },
  {
    "objectID": "slides/slides4.html#do-increases-in-option-grants-increase-risk-taking",
    "href": "slides/slides4.html#do-increases-in-option-grants-increase-risk-taking",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Do increases in option grants increase risk taking?",
    "text": "Do increases in option grants increase risk taking?\n\n\n\n\n\n\n\noptions\n\n  \n\nOption Grants\n\n Option Grants   \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants-&gt;Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances-&gt;Option Grants\n\n    \n\nAnnoyances-&gt;Risk Taking\n\n   \n\n\n\n\n\n\nExample of annoyances: Risk averse CEOs might take less risks and therefore receive more option grants."
  },
  {
    "objectID": "slides/slides4.html#iv-1-scheduled-discrete-increases-in-fixed-value-option-grants",
    "href": "slides/slides4.html#iv-1-scheduled-discrete-increases-in-fixed-value-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "IV 1: Scheduled Discrete Increases in Fixed-Value Option Grants",
    "text": "IV 1: Scheduled Discrete Increases in Fixed-Value Option Grants\n\n\n\n\n\n\n\noptions\n\n  \n\nPredicted New Grant Cycle\n\n Predicted New Grant Cycle   \n\nOption Grants\n\n Option Grants   \n\nPredicted New Grant Cycle-&gt;Option Grants\n\n    \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants-&gt;Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances-&gt;Option Grants\n\n    \n\nAnnoyances-&gt;Risk Taking\n\n   \n\n\n\n\n\n\nFor our first instrument, we use fixed-value firms, for which option grants can increase only at regularly prescheduled intervals (i.e., when new cycles start). For example, consider a fixed-value firm on regular three-year cycles. Other time-varying factors may drive trends in risk for this firm. However, these trends are unlikely to coincide exactly with the timing of when new cycles are scheduled to start.\n\n\nBasically saying the beginning of a cycle effect on option grants is not affected by the annoyances."
  },
  {
    "objectID": "slides/slides4.html#iv-2-within-cycle-grant-increases-due-to-industry-shocks-in-fixed-number-option-grants",
    "href": "slides/slides4.html#iv-2-within-cycle-grant-increases-due-to-industry-shocks-in-fixed-number-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "IV 2: Within Cycle Grant Increases due to Industry Shocks in Fixed-Number Option Grants",
    "text": "IV 2: Within Cycle Grant Increases due to Industry Shocks in Fixed-Number Option Grants\n\n\n\n\n\n\n\noptions\n\n  \n\nIndustry Shocks (Fixed Number)\n\n Industry Shocks (Fixed Number)   \n\nOption Grants\n\n Option Grants   \n\nIndustry Shocks (Fixed Number)-&gt;Option Grants\n\n    \n\nRisk Taking\n\n Risk Taking   \n\nOption Grants-&gt;Risk Taking\n\n    \n\nAnnoyances\n\n Annoyances   \n\nAnnoyances-&gt;Option Grants\n\n    \n\nAnnoyances-&gt;Risk Taking\n\n   \n\n\n\n\n\n\nFor our second instrument, we focus on fixed-number firms. The value of options granted in any particular year varies with aggregate returns within a fixed-number cycle. This means that the timing of increases in option pay within a cycle will be random in the sense that the increases are driven in part by industry shocks that are beyond the control of the firm and are largely unpredictable. To account for the possibility that aggregate returns can directly affect risk, we use fixed-value firms as a control group because their option compensation must remain fixed despite changes in aggregate returns.\n\n\nThe identifying assumption is that fixed-number vs fixed-value might be a part of the annoyances. So might the industry shocks. However, the IV assumes that the industry shocks are not different except in how they effect the option grant value."
  },
  {
    "objectID": "slides/slides4.html#the-authors-know-their-setting",
    "href": "slides/slides4.html#the-authors-know-their-setting",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "The authors know their setting!",
    "text": "The authors know their setting!\n\nOur identification strategy builds on Hall’s (1999)) observation that firms often award options according to multiyear plans. Two types of plans are commonly used: fixed-number and fixed-value. Under a fixed-number plan, an executive receives the same number of options each year within a cycle. Under a fixed-value plan, an executive receives the same value of options each year within a cycle.\n\n\n\nOur conversations with leading compensation consultants suggest that multiyear plans are used to minimize contracting costs, as option compensation only has to be set once every few years. Hall (1999, p. 97) argues that firms sort into the two types of plans somewhat arbitrarily, observing that “Boards seem to substitute one plan for another without much analysis or understanding of their differences.”\n\n\n\nRead qualitative studies and descriptions of actual practice!\nWe are looking at “slightly suboptimal” decision making to get variation."
  },
  {
    "objectID": "slides/slides4.html#key-assumption-1---relevance-iv-is-related-to-option-grants",
    "href": "slides/slides4.html#key-assumption-1---relevance-iv-is-related-to-option-grants",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Key Assumption 1 - Relevance: IV is related to Option Grants",
    "text": "Key Assumption 1 - Relevance: IV is related to Option Grants\n\nWe find that the first-year indicator corresponds to a 15% larger increase in the Black-Scholes value of new option grants than in other years.\n\n\nAll estimates are highly significant, with F-statistics greatly exceeding 10, the rule of thumb threshold for concerns related to weak instruments (Staiger and Stock (1997). (III A.)\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)"
  },
  {
    "objectID": "slides/slides4.html#key-assumption-2---exclusion-or-validity-only-path-from-iv-to-risk-taking-is-through-option-grants.",
    "href": "slides/slides4.html#key-assumption-2---exclusion-or-validity-only-path-from-iv-to-risk-taking-is-through-option-grants.",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "Key Assumption 2 - Exclusion (or validity): Only path from IV to Risk Taking is through Option Grants.",
    "text": "Key Assumption 2 - Exclusion (or validity): Only path from IV to Risk Taking is through Option Grants.\n\nOne might be concerned that predicted first years provide exogenously timed but potentially anticipated increases in option compensation. However, this is not an issue for our empirical strategy. […] He would have no incentive to increase risk prior to an anticipated increase in the value of his option compensation next period.\n\n\nIn addition, we directly examine whether fixed-value cycles appear to be correlated with other firm cycles […]\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nThe key for the exclusion assumption is that anticipation would have an impact on the risk taking prior to the new cycle. This than would have an impact on the actual measure, i.e. the change in risk."
  },
  {
    "objectID": "slides/slides4.html#one-criticism",
    "href": "slides/slides4.html#one-criticism",
    "title": "Research Design 1: Fixed Effects and Instrumental Variables",
    "section": "One Criticism",
    "text": "One Criticism\n\nFirst, option compensation tends to follow an increasing step function for executives on fixed-value plans. This is because compensation tends to drift upward over time, yet executives on fixed-value plans cannot experience an upward drift within a cycle.\n\n\nWhile these two stylized facts do not hold in all cases—as can also be seen in Figure 1—our identification strategy only requires that they hold on average.\n\n\n\n\n\n\n\n\nSome more terminology\n\n\n\nCompliers\nAlways-takers/never-takers\nDefiers\n\n\n\n\nChapter 19 Instrumental Variables in Huntington-Klein (2021)\n\nThe LATE is identified for the compliers. IV assumes that there are no defiers because now our estimated effect becomes an average of the defiers and compliers. One solution is to just remove the defiers if you can (which they do in the paper as a robustness check)."
  },
  {
    "objectID": "freaky_friday/index.html",
    "href": "freaky_friday/index.html",
    "title": "Research Design",
    "section": "",
    "text": "This website is an attempt to replicate the main results in Dellavigna and Pollet (2009) from scratch. The paper is a good example because (1) it has an explicit theoretical model, (2) provides excellent descriptions on how the different measures are constructed, (3) uses the canonical finance design, an event study. I will focus most of my attention on (2) and (3) but (1) is important because it provides guidance to readers of the paper why the measures and the design is important.\nThe basic argument of the paper is that firms will bury earnings announcements on Fridays if the earnings are bad because the market pays less attention to news on Fridays."
  },
  {
    "objectID": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "href": "freaky_friday/index.html#the-unexpected-component-of-earnings",
    "title": "Research Design",
    "section": "The Unexpected Component of Earnings",
    "text": "The Unexpected Component of Earnings\n\\[ s_{t,k} = \\frac{e_{t,k} - \\hat{e}_{t,k}}{P_{t,k}} \\]\nIn this equation, \\(s_{t,k}\\) is the surprise (i.e. the unexpected component) in earnings of company \\(k\\) at time \\(t\\). It is calculated by the actual earnings per share, \\(e_{t,k}\\), minus the median expected earnings by analysts, \\(\\hat{e}_{t,k}\\), divided by the price of the stock 5 days before the earnings release, \\(P_{t,k}\\). You will see over and over that empirical researchers are wary that the day(s) just before an announcement might be special, i.e. the news might have already leaked out, for good and less good reasons. So, instead of using the price the day before the earnings release, the paper picks a couple of days earlier 2.\nThe most important part is that we try to filter out all the information in the earnings announcement that is already known to the market by subtracting the earnings estimates of analysts. The implicit assumption is that these earnings estimates are a good measure of the market’s information on the company just before the earnings are announced."
  },
  {
    "objectID": "freaky_friday/index.html#the-market-reaction",
    "href": "freaky_friday/index.html#the-market-reaction",
    "title": "Research Design",
    "section": "The Market Reaction",
    "text": "The Market Reaction\nThe market reaction is calculated as the abnormal return from day \\(h\\) to day \\(H\\). 3\n\\[ R_{t,k}^{(h, H)} = [\\Pi_{j=h}^H (1 + R_{j,k})] - 1\n- \\hat{\\beta}_{t,k} [\\Pi_{j=h}^H (1 + R_{j,m}) - 1]\\]\nThis looks complicated but it is quite simple. The first part is the raw return of the stock over the period that we are interested in 4. The second part is the return of the market times the sensitivity of the stock to the market. The latter, \\(\\hat{\\beta}_{t,k}\\) is estimated with data from before the announcement. The goal of this approach is to filter out other reasons that the stock price might go up or down because of general economic or financial events that affect the stock market as a whole.\nSpecifically, we estimate the following regression model with data from days, \\(u\\), with \\(u\\) between 46 and 300 days before the earnings announcements. This is a regression of the market return on the firm return.\n\\[ R_{u,k} = \\alpha_{t,k} + \\beta_{t,k} R_{u,m}\\]\nAgain, we are using data from long before the earnings announcement so that our estimate is not contaminated by the earnings announcement 5.\nThere are lot of different approaches in the literature to estimate these abnormal returns but they all have the same flavour of trying to filter out other reasons why the stock price might be moving. In a pure regression framework, we would include additional variables as control variables. Constructing variables like the abnormal returns and the earnings surprise like this serves exactly the same function. There are good reasons to use the approach of first constructing the measures as precise as possible in an event study design like this but there are some problem with applying this same logic in different research designs Chen, Hribar, and Melessa (n.d.). See also the section on generated variables."
  },
  {
    "objectID": "freaky_friday/index.html#footnotes",
    "href": "freaky_friday/index.html#footnotes",
    "title": "Research Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSort of. It can look like time travel to the empirical researcher because the earnings are released after the market reaction to it but ofcourse the economic facts happened before the market reacted. I am merely pointing out the implicit assumption that when news is officially announced is not necessarily when the market receives the information.↩︎\nThis is one of these forms of time travel. Remark that the if there is information leaked before the announcement, the market reaction after the announcement will be smaller than when that would not have happened. In a way, we are underestimating the effect of the earnings on stock price returns if we only look at what happened after the announcement. Most empirical researchers are ok with this because it means that they are less likely to find an effect. So they are not biasing their method towards finding something.↩︎\nIn comparison with the paper I am simplifying the formula a little bit by setting \\(\\tau = 0\\). That is, the day of the announcement is day 0.↩︎\nRecall that if a stock goes up with 10% and goes down with 10% we can write the total return as \\(1.10 \\times 0.90 - 1 = -0.01\\).↩︎\nBut it will be affected by the two previous earnings announcements …↩︎"
  },
  {
    "objectID": "freaky_friday/linking.html",
    "href": "freaky_friday/linking.html",
    "title": "Combining databases",
    "section": "",
    "text": "The packages are the same as before.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/linking.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "freaky_friday/linking.html#linking-ibes",
    "href": "freaky_friday/linking.html#linking-ibes",
    "title": "Combining databases",
    "section": "Linking I/B/E/S",
    "text": "Linking I/B/E/S\n\nlinking_table %&gt;%\n  select(gvkey, ticker) %&gt;%\n  distinct() %&gt;%\n  summarise(N = n(), .by = ticker) %&gt;%\n  filter(N &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: ticker &lt;chr&gt;, N &lt;int&gt;\n\n\nThere are no tickers linked with multiple gvkeys. This means that left_join from I/B/E/S is the way to start the joining process. That way, there will be no duplicate matches from Compustat."
  },
  {
    "objectID": "freaky_friday/linking.html#footnotes",
    "href": "freaky_friday/linking.html#footnotes",
    "title": "Combining databases",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept for some checks on the linking table.↩︎"
  },
  {
    "objectID": "freaky_friday/download_stocks.html",
    "href": "freaky_friday/download_stocks.html",
    "title": "Stock price data",
    "section": "",
    "text": "On this page, we download the stock price data so that we can later calculate the abnormal return after the earnings announcements.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(RPostgres)\nlibrary(dbplyr)\ni_am(\"freaky_friday/download_stocks.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Library/CloudStorage/Dropbox/Teaching/lecturenotes/method_package\n\nearn_ann &lt;- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\")) \n\n\nwrds &lt;- dbConnect(Postgres(),\n                  host='wrds-pgdata.wharton.upenn.edu',\n                  port=9737,\n                  dbname='wrds',\n                  user='stimas',\n                  sslmode='require')\n\nThis section sets the parameters that we will need to limit the download. Following Dellavigna and Pollet (2009), the beginning date is 300 days before the first earnings announcement and the end date is 75 days after the last earnings announcement. I could have done this for every earnings announcement specificially but Finally, I keep the permno identifiers because these are the only stocks we want the data from.\n\ncrsp_input &lt;- earn_ann %&gt;%\n  summarise(begin = min(anndat) - 300, end = max(anndat) + 75, .by = permno) %&gt;%\n  glimpse()\n\nRows: 8,754\nColumns: 3\n$ permno &lt;dbl&gt; 10560, 10656, 88784, 10659, 87832, 84606, 80585, 88836, 87771, …\n$ begin  &lt;date&gt; 2003-12-26, 2002-07-12, 2001-01-17, 2001-11-09, 2003-05-06, 20…\n$ end    &lt;date&gt; 2006-07-10, 2006-07-19, 2006-07-18, 2006-05-23, 2005-01-16, 20…\n\npermnos &lt;- crsp_input$permno\nbegin_date &lt;- min(crsp_input$begin)\nend_date &lt;- max(crsp_input$end)\n\nI use the same syntax as before to call the WRDS databases as before with sql interspersed with the R parameters created in the previous code block. We get the daily volume, return, price, shares outstanding, cumulative factor to adjust price, and cumulative factor to adjust shares. The latter two are adjustment factors for stock splits and dividends which we probably will not need but if we do we have them.\nThis is by far the largest download from WRDS and this is why it has it’s own page. We do not want to rerun this more than strictly necessary.\n\ncrsp_query &lt;- tbl(wrds, in_schema(\"crsp_a_stock\", \"dsf\")) %&gt;%\n  filter(permno %in% permnos, date &gt;= begin_date, date &lt;= end_date) %&gt;%\n  select(permno, date, vol, ret, prc, shrout, cfacpr, cfacshr)\nall_stocks &lt;- collect(crsp_query)\nsaveRDS(all_stocks, here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nprint(all_stocks)\n\n# A tibble: 14,967,132 × 8\n   permno date         vol      ret   prc shrout cfacpr cfacshr\n    &lt;int&gt; &lt;date&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  10002 1994-03-10   700 -0.00952  13     2999    1.5     1.5\n 2  10002 1994-03-11   200  0.0577   13.8   2999    1.5     1.5\n 3  10002 1994-03-14     0 -0.0455  -13.1   2999    1.5     1.5\n 4  10002 1994-03-15     0  0       -13.1   2999    1.5     1.5\n 5  10002 1994-03-16  1700  0.00952  13.2   2999    1.5     1.5\n 6  10002 1994-03-17     0 -0.00943 -13.1   2999    1.5     1.5\n 7  10002 1994-03-18     0  0       -13.1   2999    1.5     1.5\n 8  10002 1994-03-21     0  0.00457 -13.1   2999    1.5     1.5\n 9  10002 1994-03-22  2000  0.0190   13.4   2999    1.5     1.5\n10  10002 1994-03-23     0 -0.00935 -13.2   2999    1.5     1.5\n# ℹ 14,967,122 more rows\n\n\nOne important footnote is that the price is negative on days where there were no trades. This might be important going forward.\n\n\n\n\nReferences\n\nDellavigna, Stefano, and Joshua M. Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49. https://doi.org/10.1111/j.1540-6261.2009.01447.x."
  },
  {
    "objectID": "freaky_friday/descriptive.html",
    "href": "freaky_friday/descriptive.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"freaky_friday/descriptive.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\n\nmain &lt;- readRDS(here(\"data\", \"freaky_friday\", \"main.RDS\")) %&gt;%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\nglimpse(main)\n\nRows: 130,759\nColumns: 23\n$ ticker       &lt;chr&gt; \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       &lt;dbl&gt; -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        &lt;chr&gt; \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       &lt;dbl&gt; 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        &lt;chr&gt; \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            &lt;int&gt; 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 4, 2, …\n$ median       &lt;dbl&gt; -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         &lt;dbl&gt; -0.0896500, -0.0680000, -0.0370000, -0.1005500, -0.076580…\n$ mean_days    &lt;dbl&gt; 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    &lt;dbl&gt; 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     &lt;dbl&gt; -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          &lt;dbl&gt; 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value &lt;dbl&gt; 2592630.7, 1993992.8, 2200875.0, 1434970.5, 1769503.5, 24…\n$ surprise     &lt;dbl&gt; -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n$ weekday      &lt;ord&gt; Wed, Wed, Wed, Wed, Wed, Thu, Tue, Tue, Tue, Wed, Tue, We…\n$ group        &lt;chr&gt; \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         &lt;dbl&gt; 2005, 2005, 2005, 2005, 2006, 2004, 2001, 2002, 2002, 200…"
  },
  {
    "objectID": "freaky_friday/descriptive.html#quantiles",
    "href": "freaky_friday/descriptive.html#quantiles",
    "title": "Descriptive statistics",
    "section": "Quantiles",
    "text": "Quantiles\n\nquantiles &lt;- main %&gt;%\n  mutate(sign = case_when(surprise &gt; 0 ~ \"positive\",\n                          surprise &lt; 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %&gt;%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %&gt;%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %&gt;%\n  glimpse()\n\nRows: 130,759\nColumns: 26\n$ ticker       &lt;chr&gt; \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"A2\", \"AA0G\", \"AA0H\", \"AA0H…\n$ actual       &lt;dbl&gt; -0.11, -0.11, -0.05, -0.07, -0.10, -0.04, -0.45, 0.01, 0.…\n$ pdf          &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"P\", \"D\", \"D\", \"D\", \"D\", \"D…\n$ anndats_act  &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ gvkey        &lt;chr&gt; \"001081\", \"001081\", \"001081\", \"001081\", \"001081\", \"001081…\n$ permno       &lt;dbl&gt; 10560, 10560, 10560, 10560, 10560, 10560, 88784, 10574, 1…\n$ cusip        &lt;chr&gt; \"00392410\", \"00392410\", \"00392410\", \"00392410\", \"00392410…\n$ rdq          &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ anndat       &lt;date&gt; 2005-01-26, 2005-04-27, 2005-07-27, 2005-10-26, 2006-02-…\n$ N            &lt;int&gt; 2, 3, 2, 4, 5, 1, 2, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 4, 2, …\n$ median       &lt;dbl&gt; -0.08965, -0.05740, -0.03700, -0.10610, -0.07830, -0.0900…\n$ mean         &lt;dbl&gt; -0.0896500, -0.0680000, -0.0370000, -0.1005500, -0.076580…\n$ mean_days    &lt;dbl&gt; 8.000000, 6.666667, 13.500000, 7.000000, 9.200000, 21.000…\n$ car_short    &lt;dbl&gt; 0.036461999, -0.063605082, -0.004176757, -0.014869448, -0…\n$ car_long     &lt;dbl&gt; -0.20959886, 0.05661849, -0.18368085, 0.20741889, 0.10569…\n$ date_minus5  &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ date         &lt;date&gt; 2005-01-21, 2005-04-22, 2005-07-22, 2005-10-21, 2006-01-…\n$ prc          &lt;dbl&gt; 5.89, 4.53, 5.00, 3.26, 4.02, 5.63, 14.50, 11.47, 10.85, …\n$ market_value &lt;dbl&gt; 2592630.7, 1993992.8, 2200875.0, 1434970.5, 1769503.5, 24…\n$ surprise     &lt;dbl&gt; -0.0034550086, -0.0116114785, -0.0026000000, 0.0110736197…\n$ weekday      &lt;ord&gt; Wed, Wed, Wed, Wed, Wed, Thu, Tue, Tue, Tue, Wed, Tue, We…\n$ group        &lt;chr&gt; \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"Non-Friday\", \"…\n$ year         &lt;dbl&gt; 2005, 2005, 2005, 2005, 2006, 2004, 2001, 2002, 2002, 200…\n$ sign         &lt;chr&gt; \"negative\", \"negative\", \"negative\", \"positive\", \"negative…\n$ quintile     &lt;int&gt; 2, 1, 3, 5, 2, 5, 2, 4, 4, 4, 1, 3, 3, 2, 1, 2, 4, 3, 1, …\n$ quantile     &lt;dbl&gt; 2, 1, 3, 11, 2, 11, 2, 10, 10, 4, 6, 9, 9, 2, 6, 8, 4, 9,…\n\n\nThis is a quick version of Figure 1a. It can be further cleaned up with a better axis labels. It shows the main results from Dellavigna and Pollet (2009) that the market reaction is subdued on Fridays.\n\nggplot(quantiles,\n       aes(y = car_short, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun.y = mean, geom = \"line\") +\n  scale_color_grey()\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nThis is how I would program Figure 1a and b together. It’s a good example of how using pivot_longer can make your life easier. In this case, if we need to plot multiple similar variables.\n\nquantiles %&gt;%\n  pivot_longer(c(car_short, car_long), values_to = \"car\", names_to = \"window\") %&gt;%\n  mutate(fig_name = if_else(window == \"car_short\", \"Figure 1a\", \"Figure 1b\")) %&gt;%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ fig_name, nrow = 2)"
  },
  {
    "objectID": "freaky_friday/assignment.html",
    "href": "freaky_friday/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "For the assignment, you can just copy the code in the assignment and make the necessary changes where indicated. Do not forget to copy all the code. Not just the one where you change something.\n\nSetup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(theme_cowplot(font_size = 18))\nlibrary(here)\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\ni_am(\"freaky_friday/assignment.qmd\")\n\nhere() starts at /Users/stijnmasschelein/Dropbox/Teaching/lecturenotes/method_package\n\n\nLet’s restrict the data to two years. So that the computations do not take too long.\n\nbegin &lt;- ymd(\"2003-01-01\")\nend &lt;- ymd(\"2005-12-31\")\n\nI keep the full stock price data for now. This code is a slightly adapted version of the Abnormal returns page on the website. There are two changes: (1) we have to filter only the announcements between begin and end and (2) we add more factors from the French data. I will not go into the theoretical details here. The basic idea is that stock return are not only sensitive to one set of economic factors which are captured in the market return, but also to more specific factors such as the size (smb) and whether the company is value or growth company (hml). In what follows, we will treat the returns to these factors exactly the same as the market return, i.e. as predictors for the returns of the company that we are interested in.\n\nearn_ann &lt;- readRDS(here(\"data\", \"freaky_friday\", \"earn_ann.RDS\")) %&gt;%\n  filter(anndat &gt;= begin, anndat &lt;= end)\nanalyst &lt;- readRDS(here(\"data\", \"freaky_friday\", \"analyst.RDS\")) %&gt;%\n  filter(anndat &gt;= begin, anndat &lt;= end)\nall_stocks &lt;- readRDS(here(\"data\", \"freaky_friday\", \"all_stocks.RDS\"))\nfamafrench &lt;- read_csv(file = here(\"data\", \"F-F_Research_Data_Factors_daily.csv\"),\n                       col_names = c(\"date\", \"mkt_rf\", \"smb\", \"hml\", \"rf\"),\n                       skip = 5, col_type = \"ddddd\") %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  mutate_if(is.numeric, ~ . / 100) %&gt;%\n  mutate(mkt = mkt_rf + rf) %&gt;%\n  print()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n# A tibble: 25,400 × 6\n   date        mkt_rf     smb     hml      rf      mkt\n   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 1926-07-01  0.001  -0.0025 -0.0027 0.00009  0.00109\n 2 1926-07-02  0.0045 -0.0033 -0.0006 0.00009  0.00459\n 3 1926-07-06  0.0017  0.003  -0.0039 0.00009  0.00179\n 4 1926-07-07  0.0009 -0.0058  0.0002 0.00009  0.00099\n 5 1926-07-08  0.0021 -0.0038  0.0019 0.00009  0.00219\n 6 1926-07-09 -0.0071  0.0043  0.0057 0.00009 -0.00701\n 7 1926-07-10  0.0062 -0.0053 -0.001  0.00009  0.00629\n 8 1926-07-12  0.0004 -0.0003  0.0064 0.00009  0.00049\n 9 1926-07-13  0.0048 -0.0028 -0.002  0.00009  0.00489\n10 1926-07-14  0.0004  0.0007 -0.0043 0.00009  0.00049\n# ℹ 25,390 more rows\n\n\n\n\nThree-Factor model\nThe following code is almost identical to the Abnormal returns page. However, for the three-factor model we need to run the following regression for each announcement.\n\\[\nR_{u,k} = \\alpha_{t,k} + \\beta^{mkt}_{t,k} R_{u,m}\n+ \\beta^{size}_{t,k} smb_{u,m} + \\beta^{value}_{t,k} hlm_{u,m} + \\epsilon_{u,k}\n\\]\nThat is, we include two more factors (smb, and hlm) in the regression model that we run for each announcement. The three-factor model typically works with the stock return and market return adjusted for the risk free rate (i.e. ret - rf and mkt_rf respectively). The code will give us a column coefs with the 4 estimated parameters alpha, beta_mkt, beta_size, beta_value.\nYou need to change the code below in two parts as indicated by the comments in the code.\nThe code first creates a function creat_coefs_3factor for n earnings announcements where the default value for n is 6. You can test whether your function works by creating the test object which should be a data set with 6 rows. One for each of the 6 earnings announcements at the top of the earn_ann\n\ncreate_coefs_3factor &lt;- function(n = 6){\n  earn_ann %&gt;% head(n = n) %&gt;%\n    distinct(permno, anndat) %&gt;%\n    mutate(start = anndat - 300, end = anndat - 46) %&gt;%\n    left_join(select(all_stocks, permno, date, ret),\n              by = join_by(permno == permno, start &lt;= date, end &gt;= date)) %&gt;%\n    # Changes are necessary in the following lines\n    # ... needs to be changed\n    left_join(select(famafrench, ...),\n              by = join_by(date == date)) %&gt;%\n    filter(!is.na(ret), !is.infinite(ret)) %&gt;%\n    # Changes are necesary in the following lines:\n    # .x, .y., .z need to be changed\n    summarise(y = list(cbind(ret - rf)),\n              X = list(cbind(alpha = 1, beta_mkt = .x,\n                             beta_size = .y, beta_value = .z)),\n              .by = c(permno, anndat)) %&gt;%\n    mutate(coefs = pmap(list(X, y), ~ lm.fit(..1, ..2) %&gt;% coef()),\n           .by = c(permno, anndat)) %&gt;%\n    select(-y, -X)\n}\n\ntest &lt;- create_coefs_3factor()\n\nmicrobenchmark::microbenchmark(\n                  create_coefs_3factor(1000),\n                  times = 10)\n\nIf it takes on average (noticeably) more than 10 seconds to run the three-factor model on 1000 observations, it is ok to limit the sample to 1 year. You can change the start and end date above and rerun the code.\n\n\nAbnormal Returns\nIn the first parts of the next code, we will calculate the coefficients for all announcements in the 2 year dataset. This is computationally the most intensive step and I could take up to two minutes to finish. When finished, we have an object results that contains the alphas and betas, we need to calculate the abnormal returns.\nIn the next step, we need to calculate the abnormal returns for every announcement in our data based on the three factors (mkt, smb, hlm). That is, we have to subtract the actual return on a day, \\(h\\), after the announcement from the expected return based on the three-factor model.\n\\[ R_{t,k} ^ {h} = R_{h, k} - \\alpha_{t,k} - \\beta^{mkt}_{t,k} R_{h,m}\n- \\beta^{size}_{t,k} smb_{h,m} - \\beta^{value}_{t,k} hlm_{h,m}\\]\n\\(R_{t,k}^h\\) is the abnormal return on day \\(h\\) after the announcement, \\(R_{h,m}\\) is the market return on day \\(h\\), \\(smb_{h,m}\\) is the size factor and \\(hlm_{h,m}\\) is the value factor.\nIn the last change, you need to add up the abnormal returns for the days 0 and 1 for the short window after the announcement and the for the days 2-75 after the announcement. That is you need to calculate car_sum\nI also give the accumulation with the product which is more correct but the literature typically works with the sum as far as I could discern. You will also see further in the code that it does not really matter for our results even in the restricted dataset. You can use car_prod as an example for car_sum where car_sum is simpler.\n\nN &lt;- nrow(earn_ann)\nresults &lt;- create_coefs_3factor(N)\n\nabnormal &lt;- results %&gt;%\n  unnest_wider(coefs) %&gt;%\n  mutate(date75 = anndat + 75) %&gt;%\n  left_join(select(all_stocks, permno, date, ret),\n            by = join_by(permno == permno,\n                         date75 &gt;= date, anndat &lt;= date)) %&gt;% print %&gt;%\n  # Changes are necessary in the following lines\n  # ... needs to be changed\n  left_join(select(famafrench, ...),\n            by = join_by(date == date)) %&gt;%\n  # Changes are necessary in the following lines\n  # ... needs to be changed\n  mutate(ar = ...,\n         time_frame = if_else(date - anndat &lt;= 1, \"short\", \"long\")) %&gt;%\n  # Changes are necessary in the following lines\n  # .x needs to be changed\n  summarise(car_sum = ...,\n            car_prod = prod(1 + ar) - 1,\n            .by = c(permno, anndat, time_frame)) %&gt;%\n  filter(!is.na(car_sum), !is.na(car_prod)) %&gt;%\n  pivot_wider(values_from = c(car_sum, car_prod), names_from = time_frame)\nglimpse(abnormal)\n\n\n\nPutting it all together\nThe next two code blocks gather the necessary market price and market value information. Next we combine the earning announcement data with the analyst expectations data, the abnormal returns, and the stock price data. Finally, we calculate the earnings surprise scaled by the stock price.\nYou can just include this code as is.\n\nclean_prices &lt;- all_stocks %&gt;%\n  filter(prc &gt; 0) %&gt;%\n  select(permno, date, prc, shrout) %&gt;%\n  mutate(market_value = prc * shrout) %&gt;%\n  select(-shrout) %&gt;%\n  print()\n\n# A tibble: 14,333,348 × 4\n   permno date         prc market_value\n    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1  10002 1994-03-10  13         38987 \n 2  10002 1994-03-11  13.8       41236.\n 3  10002 1994-03-16  13.2       39737.\n 4  10002 1994-03-22  13.4       40112.\n 5  10002 1994-03-24  13.1       39362.\n 6  10002 1994-04-07  12.5       37488.\n 7  10002 1994-04-15  14         41986 \n 8  10002 1994-04-18  14         41986 \n 9  10002 1994-04-22  12.5       37488.\n10  10002 1994-04-28  14         41986 \n# ℹ 14,333,338 more rows\n\n\n\nsurprise &lt;- earn_ann %&gt;%\n  left_join(analyst,\n            by = join_by(ticker, anndat, actual, pdf)) %&gt;%\n  left_join(abnormal,\n            by = join_by(permno, anndat)) %&gt;%\n  mutate(date_minus5 = anndat - 5) %&gt;%\n  left_join(clean_prices,\n            by = join_by(permno, closest(date_minus5 &gt;= date))) %&gt;%\n  mutate(surprise = (actual - median) / prc)\nglimpse(surprise)\n\nThe last code block here further cleans the data following the rules set in the paper and as used by me in the Abnormal returns page. You can just include the code as is.\n\nwinsorise &lt;- 5/10000\nmain &lt;- surprise %&gt;%\n  filter(!is.na(surprise)) %&gt;%\n  filter(abs(median) &lt; prc, abs(actual) &lt; prc) %&gt;%\n  filter(prc &gt; 2) %&gt;%\n  mutate(weekday = wday(anndat, label = TRUE)) %&gt;%\n  filter(! weekday %in% c(\"Sat\", \"Sun\")) %&gt;%\n  filter(percent_rank(car_sum_long) &gt;= winsorise,\n         percent_rank(car_sum_long) &lt;= 1 - winsorise,\n         percent_rank(car_sum_short) &gt;= winsorise,\n         percent_rank(car_sum_short) &lt;= 1 - winsorise) %&gt;%\n  mutate(group = if_else(weekday == \"Fri\", \"Friday\", \"Non-Friday\"),\n         year = year(anndat))\n\n\n\nDescriptive Plot\nThe next code block creates the quantiles in the new main dataset so that we can recreate one of the figures.\n\nquantiles &lt;- main %&gt;%\n  mutate(sign = case_when(surprise &gt; 0 ~ \"positive\",\n                          surprise &lt; 0 ~ \"negative\",\n                          surprise == 0 ~ \"zero\")) %&gt;%\n  mutate(\n    quintile = ntile(surprise, 5),\n    .by = c(sign, year)) %&gt;%\n  mutate(\n    quantile = case_when(sign == \"positive\" ~ 6 + quintile,\n                         sign == \"negative\" ~ quintile,\n                         sign == \"zero\" ~ 6\n                         )\n  ) %&gt;%\n  glimpse()\n\nThe last part you need to do is to recreate the quick version of Figure 1a in the descriptives page\n\nggplot(quantiles,\n       aes(...)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey()\n\nThis is some free code just to demonstrate how easy it is to combine pivot_longer and ggplot to create similar plots for a bunch of similar variables where the variables are originally in different columns.\n\nquantiles %&gt;%\n  pivot_longer(c(car_sum_short, car_prod_short, car_sum_long, car_prod_long),\n               values_to = \"car\", names_to = \"window\") %&gt;%\n  ggplot(aes(y = car, x = quantile, group = group, colour = group)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .2) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_color_grey() +\n  facet_wrap(~ window, nrow = 2)"
  },
  {
    "objectID": "machine_learning/assignment.html",
    "href": "machine_learning/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Answer the following two questions in no more than 1 A4 page in total. My expectations are 5 to 10 sentences per question depending on your writing style. You can submit your answer on LMS in word, qmd, pdf, or html version."
  },
  {
    "objectID": "machine_learning/assignment.html#instructions",
    "href": "machine_learning/assignment.html#instructions",
    "title": "Assignment",
    "section": "",
    "text": "Answer the following two questions in no more than 1 A4 page in total. My expectations are 5 to 10 sentences per question depending on your writing style. You can submit your answer on LMS in word, qmd, pdf, or html version."
  },
  {
    "objectID": "machine_learning/assignment.html#questions",
    "href": "machine_learning/assignment.html#questions",
    "title": "Assignment",
    "section": "Questions",
    "text": "Questions\n\nSearch for an article in the same research area as your thesis that uses any machine learning technique. Give a citation to the paper and based on your reading of the abstract of the paper and skimming the methodology section, argue whether the machine learning improves the researchers’ ability to answer their research question. When you look for a paper do not restrict your search too much. I primarily want you to find a paper where you are comfortable with the main research question. For instance, I used google scholar and searched for \"voluntary disclosure\" \"machine learning\" source:accounting to find an article with a machine learning technique on voluntary disclosure in an accounting journal. If you cannot find an article in your research area, I would also accept a paper in any area that you have discussed in FINA4481, FINA4491, or ACCT4471. Do not spend too much time on finding the right paper!\nHow could a machine learning technique improve the method for your thesis? You can ignore the cost of data collection and can assume that you can use all the data sources that have been used in your research field.\n\n\n\n\n\n\n\nNote\n\n\n\nFor answering the questions, you will have to ask yourself the question whether the main research question involves a prediction task 1. If not and the research question involves estimating a causal 2 or descriptive 3 estimate, can better out-of-sample prediction help with this research question. Ask yourself the question whether traditional linear models or time series models are sufficient or whether more advanced machine learning methods are required."
  },
  {
    "objectID": "machine_learning/assignment.html#footnotes",
    "href": "machine_learning/assignment.html#footnotes",
    "title": "Assignment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nE.g. what is the future stock return based on current market conditions?↩︎\nE.g.: what is the effect of voluntary disclosure on the cost of capital?↩︎\nE.g.: what percentage of senior accountants supports changing the marginal tax rate?↩︎"
  },
  {
    "objectID": "machine_learning/theory.html",
    "href": "machine_learning/theory.html",
    "title": "Theory",
    "section": "",
    "text": "In this section, I will show how you can code a prediction model for the return of a company’s stock price based on the returns of a number of peer companies. I loosely follow the same approach as in Baker and Gelbach (2020). In that paper, peer firms are identified based on the SIC code in the Compustat data which indicates what the industry is of the firm’s primary product.\nWe are using the tidymodel package for the application and I will follow the introduction to tidymodels. The advantage of the tidymodels approach is that you can follow a very similar workflow for other machine learning methods than the one I will be showing here. The code itself is not that important. The goal is more to give you a starting point.\nTo understand the machine learning approach that we are using, I need to start of with some theory. The fundamental idea is that we want to avoid overfitting in the data that we have, so that when we use the model to predict on new data we still have good predictions. This means that when we estimate our model we do not want to have a model that fits the current data as good as possible. We want to regularize the parameters in the model so that we do not get a perfect fit in the current sample and a better out-of-sample predictions. For instance, if we use 200 trading days and have 200 peer firms, we can perfectly predict within the sample of 200 days 1 but there are no guarantees that we will get good predictions from that model out-of-sample (i.e. after the earnings announcement).\nFor the linear model to predict stock returns based on peers, we will use a the elastic net regularizer to bias the estimates within the sample data to make it more likely that the linear model will give good predictions out-of-sample. One way to think about the linear model with peers as predictors is that we are creating a bespoke market index for each firm as a weighted average of its peers.\nA regular linear model estimates the \\(\\beta\\)s by minimising the following equation where we minimise the sum of the squared difference between the outcome (\\(y_i\\)) and prediction (\\(X_i \\beta\\)) for each observation i.\n\\[\n\\sum^N_{i=1} (y_i - X_i \\beta)^2\n\\]\nThat is, we want to find estimates that give the best possible fit in the data. The regulariser puts a penalty on bigger absolute values for the \\(\\beta\\)s to limit overfitting to the in-sample data. The estimates will now be chosen to minimise the following equation.\n\\[\n\\sum^N_{i=1} (y_i - X_i \\beta)^2 + \\lambda \\left ( \\alpha \\sum^p_j \\beta^2_j + (1 - \\alpha) \\sum^p_j |\\beta_j| \\right)\n\\]\nThe size of the penalty is given by the parameters \\(\\lambda &gt; 0\\) and consists of two parts: the sum of the squared \\(\\beta\\)s and the sum of the absolute values of the \\(\\beta\\)s. The first term is the ridge reguliser and the second one is the LASSO regulariser. They both have been shown to have useful properties as regulisers and thus they are often used together with the weight \\(1 \\geq \\alpha \\geq 0\\). With \\(\\alpha = 1\\), we only use ridge regression and with \\(\\alpha = 0\\), we only use the LASSO.\nThe final step is that we need to choose the right values for \\(\\lambda\\) and \\(\\alpha\\). A common approach is to use cross validation where we split the data that we have in roughly equal sized partitions or folds. For instance, you have 10 folds with each 10% of the data. With cross validation, we will use the 90% of the remaining data, use a number of different values for \\(\\lambda\\) and \\(\\alpha\\) to estimate \\(\\beta\\)s and predict the 10% fold that we did not use for estimation. In other words, we use the data that we have to do a prediction task with the advantage that we can evaluate which \\(\\lambda\\) and \\(\\alpha\\) gives us the best predictions.\n:::{::: {.callout-important} The key insight is that if we care about a prediction task, we can use some of the data and pretend it is data that we have never seen when we estimate our prediction model. We can then test which model is actually good at predicting on data that it has never seen. :::}\nWe do need a measure to evaluate the quality of the predictions. A common choice is the Root Mean Squared Error (RMSE) which is defined as the square root of the squared difference between the actual value of the outcome and the predictions for the outcome.\n\\[\n\\sqrt{ \\frac{\\sum^N_{i=1} (y_i - \\hat{y}_i)^2}{N} }\n\\]\nWe will use the RMSE to evaluate the predictions out-of-sample. The RMSE is similar to the first equation where we choose the \\(\\beta\\)s to minimise the squared difference between the outcome and the fitted data in the in-sample data."
  },
  {
    "objectID": "machine_learning/theory.html#footnotes",
    "href": "machine_learning/theory.html#footnotes",
    "title": "Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a system of linear equations with 200 unknown parameters (the \\(\\beta\\)s) and 200 observations (the trading days).↩︎"
  },
  {
    "objectID": "generalised/introduction.html",
    "href": "generalised/introduction.html",
    "title": "Introduction and Theory",
    "section": "",
    "text": "The question in this section is how we should deal with outcome variables that are not continuous or restricted to be positive. I will limit the applications to the most common examples: (1) binary outcome variables and (2) positive outcome variables. Examples of binary outcome variables are the decision to disclose information, or merge a company. Positive outcome variables are variables such as employee salaries or the market value of a company.\nThis is a fairly contentious topic where different literature streams have different expectations and norms for what is appropriate. This is by no means a complete overview of the topic. My aim is to give you my perspective and to give you enough tools to be able to decide what is most appropriate for your research and to explain to assessors why you made those choices.\nThe basic problem is the following. In our typical linear model, the outcome variable can take on any value between \\(-\\infty\\) and \\(+\\infty\\).\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nIf we want to make sure that \\(y_i\\) is restricted to a certain domain, for instance between 0 and 1, we can use a transformation function on the linear model. These transformed linear models are often called generalised linear models.\n\\[\ny_i = g(\\alpha + \\beta x_i + \\epsilon_i)\n\\]\nYou can also think of the inverse of the transformation function as transforming the outcome so that it can take all positive and negative values. This is implicitly what we have done earlier when we used the logarithmic transformation in our earlier discussion on what the appropriate measure is for pay-performance sensitivity. In the second week, we discussed the importance of theory to determine what the right measure is according to our research question. In this week, we are asking the question whether we are interested in \\(y_i\\) or in the transformation \\(g^{-1}(y_i)\\).\n\\[\ng^{-1}(y_i) = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nThe discussion should also remind you of the difference between prediction and parameter estimation at the start of the machine learning chapter. The issue of the mismatch between the domain of the linear model and the outcome variable is less of an issue if we are really interested in estimating the parameter \\(\\beta\\). It really is a problem if we are trying to predict the outcome variable \\(y\\) because we know that we are going to get predictions that are impossible. In the reminder of this chapter I will focus on parameter estimation because that is a more common research question in finance and accounting research. That does not mean that prediction of restricted outcome variables is not important. Indeed, regularised versions of generalised linear models for binary outcome variables are popular classification models. They are fairly easy to implement in the tidymodels framework we have used in the machine learning application."
  },
  {
    "objectID": "generalised/introduction.html#introduction",
    "href": "generalised/introduction.html#introduction",
    "title": "Introduction and Theory",
    "section": "",
    "text": "The question in this section is how we should deal with outcome variables that are not continuous or restricted to be positive. I will limit the applications to the most common examples: (1) binary outcome variables and (2) positive outcome variables. Examples of binary outcome variables are the decision to disclose information, or merge a company. Positive outcome variables are variables such as employee salaries or the market value of a company.\nThis is a fairly contentious topic where different literature streams have different expectations and norms for what is appropriate. This is by no means a complete overview of the topic. My aim is to give you my perspective and to give you enough tools to be able to decide what is most appropriate for your research and to explain to assessors why you made those choices.\nThe basic problem is the following. In our typical linear model, the outcome variable can take on any value between \\(-\\infty\\) and \\(+\\infty\\).\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nIf we want to make sure that \\(y_i\\) is restricted to a certain domain, for instance between 0 and 1, we can use a transformation function on the linear model. These transformed linear models are often called generalised linear models.\n\\[\ny_i = g(\\alpha + \\beta x_i + \\epsilon_i)\n\\]\nYou can also think of the inverse of the transformation function as transforming the outcome so that it can take all positive and negative values. This is implicitly what we have done earlier when we used the logarithmic transformation in our earlier discussion on what the appropriate measure is for pay-performance sensitivity. In the second week, we discussed the importance of theory to determine what the right measure is according to our research question. In this week, we are asking the question whether we are interested in \\(y_i\\) or in the transformation \\(g^{-1}(y_i)\\).\n\\[\ng^{-1}(y_i) = \\alpha + \\beta x_i + \\epsilon_i\n\\]\nThe discussion should also remind you of the difference between prediction and parameter estimation at the start of the machine learning chapter. The issue of the mismatch between the domain of the linear model and the outcome variable is less of an issue if we are really interested in estimating the parameter \\(\\beta\\). It really is a problem if we are trying to predict the outcome variable \\(y\\) because we know that we are going to get predictions that are impossible. In the reminder of this chapter I will focus on parameter estimation because that is a more common research question in finance and accounting research. That does not mean that prediction of restricted outcome variables is not important. Indeed, regularised versions of generalised linear models for binary outcome variables are popular classification models. They are fairly easy to implement in the tidymodels framework we have used in the machine learning application."
  },
  {
    "objectID": "generalised/introduction.html#setup",
    "href": "generalised/introduction.html#setup",
    "title": "Introduction and Theory",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(cowplot)\ntheme_set(theme_cowplot(font_size = 18))\ni_am(\"generalised/introduction.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package"
  },
  {
    "objectID": "generalised/introduction.html#transformation",
    "href": "generalised/introduction.html#transformation",
    "title": "Introduction and Theory",
    "section": "Transformation",
    "text": "Transformation\nIn this section, I will visualise the most common transformation functions in the accounting and finance literature. I start with the logistic and probit transformations which are commonly used to model binary variables. We can think of those variables as having two values either \\(y_i = 1\\) or \\(y_i = 0\\). In this case, we can model the probability \\(P(y_i = 1) = g(\\alpha + \\beta x_i + \\epsilon_i)\\) and then use a binomial distribution to predict 1s and 0s. The logistic and probit transformation are two possible options for the function \\(g\\).\n\nLogistic Transformation\nThe logistic transformation transforms the linear scale, \\(z_i = \\alpha + \\beta x_i + \\epsilon_i\\), to \\(\\frac{e^{z_i}}{1+ e^{z_i}}\\). R has an inbuilt function plogis that does the transformation for us but you can obviously just write the transformation yourself. The code below shows that you get identical results.\n\nN &lt;- 1001\nlogit &lt;-\n  tibble(\n    linear_scale = seq(from = -10, to = 10, length.out = N)) %&gt;%\n  mutate(\n    logit_probability1 = exp(linear_scale)/(1 + exp(linear_scale)),\n    logit_probability2 = plogis(linear_scale),\n  )\nglimpse(logit)\n\nRows: 1,001\nColumns: 3\n$ linear_scale       &lt;dbl&gt; -10.00, -9.98, -9.96, -9.94, -9.92, -9.90, -9.88, -…\n$ logit_probability1 &lt;dbl&gt; 4.539787e-05, 4.631492e-05, 4.725050e-05, 4.820498e…\n$ logit_probability2 &lt;dbl&gt; 4.539787e-05, 4.631492e-05, 4.725050e-05, 4.820498e…\n\nggplot(logit, aes(y = logit_probability1, x = linear_scale)) +\n  geom_line() +\n  xlab(\"z\") + ylab(\"logistic(z)\")\n\n\n\n\nIn the figure, you can see how the transformed value stays between 0 and 1, exactly as we would want from a probability. Also notice how the impact of an increase of 1 on the linear scale \\(z\\) varies depending on the starting value of \\(z\\). This is because the logistic transformation is an non-linear transformation and it makes the interpretation of the the coefficients in our model more difficult.\n\n\nCumulative Normal or Probit\nThere is another function that is commonly used to transform the linear scale to the probability scale, the cumulative normal or probit function. We need to take a short detour to the normal distribution to explain the cumulative normal distribution to explain this function. You can see a normal distribution below where the lower 90% of the distribution is filled in yellow while the upper 10% is filled in blue. The value for \\(x\\) associated with the 90% is approximately 1.28. That means that we can associate a probability with each value of \\(x\\) which tells us what the probability is that a randomly generated normal value is smaller than \\(x\\). This function is the probit function.\n\ntibble(x = seq(from = -3, to = 3, length.out = N)) %&gt;%\n  mutate(dnorm = dnorm(x),\n         fill = if_else(x &lt; qnorm(0.90), \"below\", \"above\")) %&gt;%\n  ggplot(aes(y = dnorm, x = x, fill = fill)) +\n  geom_area() +\n  scale_fill_viridis_d() +\n  ggtitle(\"Normal Distribution\")\n\n\n\n\nIn the next code, I show the cumulative probability function for a normal distribution with mean 0 and standard deviation 1. You can see that it look quite similar to the logistic transformation. In the next, section we will compare the two transformations directly.\n\ncumul_norm  &lt;- logit %&gt;%\n  mutate(cumul_norm_prob = pnorm(linear_scale))\nggplot(cumul_norm, aes(y = cumul_norm_prob, x = linear_scale)) +\n  geom_line() +\n  xlab(\"z\") + ylab(\"cumulative_normal(z)\")\n\n\n\n\n\n\nComparison\nThe main reason to show the comparison is to highlight that when we divide \\(z\\) by 1.6 the probit transformation is very similar to the logistic transformation. This probit transformation is also equivalent to the cumulative normal probability for a normal distribution with mean 0 and standard deviation 1.6. In other words, the difference between the logistic or probit regression is often inconsequential.\n\ncombination &lt;- cumul_norm %&gt;%\n  mutate(cumul_norm_adj = pnorm(linear_scale/1.6)) %&gt;%\n  select(-logit_probability2) %&gt;%\n  pivot_longer(c(logit_probability1, cumul_norm_prob, cumul_norm_adj))\nggplot(combination, aes(y = value, x = linear_scale,\n                        colour = name)) +\n  scale_color_viridis_d() +\n  geom_line() +\n  xlab(\"z\") + xlab(\"g(z)\") + ggtitle(\"Binary Distribution\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe choice between a logistic regression or a probit regression is often not consequential. If your research question requires you to model a binary variable, the choice between probit or logistic regression can be based on what the preferred choice is in your research area.\n\n\n\n\nPoisson\nThe poisson distribution (rpois) is originally developed to model the number of independent events with a fixed rate happening in a given amount of time. For instance, the number of acquisitions that a firm does in a year or the number of patents they obtained. It is the general workhorse model for any count variable. As, you can see in the figure below, you can think of the poisson distribution as discrete random distribution (the dots) around the the exponentiated linear model (the line). It will be important further on to think of a poisson regression as a model for\n\\[ g^{-1}(E(y_i | x_i)) = \\alpha + \\beta x_i\\]\nwhere \\(g^{-1}(z) = log(z)\\) and \\(g(z) = exp(z)\\). That, is the logarithm of the expected value of \\(y_i\\) is given by \\(\\alpha + \\beta x_i\\). Moreover, in contrast to the exponent, the poisson distribution can give 0 values1.\n\npoisson &lt;-\n  tibble(linear_scale = seq(from = -3, to = 3, length.out = N)) %&gt;%\n  mutate(exp_scale = exp(linear_scale),\n         poisson = rpois(N, exp_scale))\nggplot(poisson, aes(y = exp_scale, x = linear_scale)) +\n  geom_line() +\n  geom_point(aes(y = poisson)) +\n  xlab(\"z\") + ylab(\"g(z)\") + ggtitle(\"Poisson Distribution\")"
  },
  {
    "objectID": "generalised/introduction.html#footnotes",
    "href": "generalised/introduction.html#footnotes",
    "title": "Introduction and Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the first lecture, I transformed a variable on the logarithm scale but that lead to problems with 0 values. I used a common hack by adding 1 to the variable and then take the logarithm. On the next page, we will see that does not always give desirable results.↩︎"
  },
  {
    "objectID": "generated/introduction.html",
    "href": "generated/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This last section is going to bring everything full circle to an extent. The topic of this section is generated variables which are variables that are the result of a regression. For instance, we could use the residuals or predicted values of one regression and use them as variables in an other regression. This is the idea behind two-stage least squares with an instrumental variable, models of abnormal returns, models of abnormal accruals, and a lot of matching approaches and many more in the accounting and finance literature.\nThe issue is that these generated variables might violate some of the implicit or explicit assumptions in the second regression model. I will go over some of the issues that are specific to certain models such as two-stage least squares on this page and the use of generated variables on the next page. The main message of this section is that the properties of these models that exists of more than one regression are not always well understood: the estimates could be biased or the standard software packages could give the wrong standard errors. The solutions I propose in this setting is to use simulations of many datasets to understand whether an approach gives biased estimates and use bootstrapped standard errors to check whether the multi-step approach gives reasonable standard errors. I use plenty of coding examples to illustrate these points."
  },
  {
    "objectID": "generated/introduction.html#introduction",
    "href": "generated/introduction.html#introduction",
    "title": "Introduction",
    "section": "",
    "text": "This last section is going to bring everything full circle to an extent. The topic of this section is generated variables which are variables that are the result of a regression. For instance, we could use the residuals or predicted values of one regression and use them as variables in an other regression. This is the idea behind two-stage least squares with an instrumental variable, models of abnormal returns, models of abnormal accruals, and a lot of matching approaches and many more in the accounting and finance literature.\nThe issue is that these generated variables might violate some of the implicit or explicit assumptions in the second regression model. I will go over some of the issues that are specific to certain models such as two-stage least squares on this page and the use of generated variables on the next page. The main message of this section is that the properties of these models that exists of more than one regression are not always well understood: the estimates could be biased or the standard software packages could give the wrong standard errors. The solutions I propose in this setting is to use simulations of many datasets to understand whether an approach gives biased estimates and use bootstrapped standard errors to check whether the multi-step approach gives reasonable standard errors. I use plenty of coding examples to illustrate these points."
  },
  {
    "objectID": "generated/introduction.html#setup",
    "href": "generated/introduction.html#setup",
    "title": "Introduction",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(modelsummary)\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nlibrary(fixest)\nlibrary(bayesboot)\ni_am(\"generated/introduction.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\nset.seed(230383)"
  },
  {
    "objectID": "generated/introduction.html#an-example-2sls",
    "href": "generated/introduction.html#an-example-2sls",
    "title": "Introduction",
    "section": "An example: 2SLS",
    "text": "An example: 2SLS\nIn this example, we are using a standard setting where an instrumental variable, iv, is used to identify the effect of a variable x on y in the presence of annoying confounding. Note that I generate the data in two steps. The initial tibble step simulates the variables that are exogenous and not affected by other parts of the model while the second mutate step simulates the endogenous variables that are causally affected by other parts of the model.\n\nN &lt;- 1e4\ndata_2sls &lt;-\n  tibble(\n    iv = rbinom(N, 1, 0.5),\n    annoying = rnorm(N)\n  ) %&gt;%\n  mutate(\n    x = rnorm(N, 2 * iv - 3 * annoying, 5),\n    y = rnorm(N, 3 * annoying + x, 5)\n  )\n\nNext, we run four different models with this data.\n\nlm1 is the confounded regression.\nlm2 controls for the annoying confounding factor and provides the best possible answer to the question. Unfortunately, in reality we often are not sure that we have all the confounding factors.\ntsls is the standard two-stage least squares estimate with iv as the instrumental variable.\ntsls_manual1 and tsls_manual2 run the two regressions of two-stage least squares manually.\n\n\nlm1 &lt;- lm(y ~ x, data = data_2sls)\nlm2 &lt;- lm(y ~ x + annoying, data = data_2sls)\ntsls &lt;- feols(y ~ 1 | 0 | x ~ iv, data = data_2sls)\ntsls_manual1 &lt;- lm(x ~ iv, data = data_2sls)\ndata_2sls &lt;- data_2sls %&gt;%\n  mutate(fit_x = fitted(tsls_manual1))\ntsls_manual2 &lt;- lm(y ~ fit_x, data = data_2sls)\ncoef_map &lt;- c('x' = 'x', 'annoying' = 'annoying',\n              'fit_x' = 'x')\nmsummary(list(confounded = lm1, ideal = lm2, tsls = tsls,\n              manual = tsls_manual2),\n         gof_map = gof_map, coef_map = coef_map)\n\n\n\n\n\nconfounded\nideal\ntsls\nmanual\n\n\n\n\nx\n0.747\n1.007\n1.040\n1.040\n\n\n\n(0.010)\n(0.010)\n(0.061)\n(0.074)\n\n\nannoying\n\n2.970\n\n\n\n\n\n\n(0.058)\n\n\n\n\nNum.Obs.\n10000\n10000\n10000\n10000\n\n\nR2\n0.378\n0.506\n0.320\n0.019\n\n\n\n\n\n\n\nIn the results, you can notice the following. The confounded regression is biased. We do not get the correct coefficient of \\(\\beta = 1\\). The ideal regression estimates the coefficient precisely with a low standard error. Despite the high number of observations (10^{4}), the two-stage least squares estimates also have an unbiased estimate but the standard error is substantially larger. The manual two-stage least squares model gets exactly the same coefficient but the standard error is larger again.\nIt turns out that the standard error is wrong in the manual version (Gelman and Hill 2006, 223). That is why we should use the fixest package to estimate the two-stage least squares model. The problem is that in our second stage regression we use fit_x as the independent variable which is the predicted value of x in the first stage regression. When lm calculates the standard error of the coefficient, it uses the standard deviation of the residuals in the regression, i.e. the difference between y and the predicted y based on x_fit. However, we know that we should actually use x to predict y. So, we need to adjust the standard errors of the coefficient and that is what the following code does manually.\nThe following code shows how to do that adjustment 1. First, we get the coefficient table from the summary which is just a matrix, where we can access the values based on the name or row/column number of the values that we need. I am extracting the coefficient and the standard error that we are interested in. We can also extract the unadjusted standard deviation of the residuals which is called sigma in the summary. Finally, we calculate the adjusted standard deviation if we use x instead of fit_x to predict y. The last line adjusts the standard error.\n\ncoef_table &lt;- summary(tsls_manual2)$coef\nintercept &lt;- coef_table[\"(Intercept)\", 1]\nbeta &lt;- coef_table[\"fit_x\", 1]\nse &lt;- coef_table[\"fit_x\", 2]\nsd_unadj &lt;- summary(tsls_manual2)$sigma\nsd_adj &lt;- data_2sls %&gt;%\n  mutate(e = y - intercept - beta * x) %&gt;%\n  summarise(sd = sd(e)) %&gt;%\n  pull(sd)\nse * sd_adj / sd_unadj\n\n[1] 0.06145269\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith two-stage least squares, statisticians and econometricians have figured out the correct adjustments to standard errors and they are build into the software packages. For more complicated models with multiple steps this might not always be possible. One possible solution is to bootstrap the standard errors."
  },
  {
    "objectID": "generated/introduction.html#bayesian-bootstrap",
    "href": "generated/introduction.html#bayesian-bootstrap",
    "title": "Introduction",
    "section": "Bayesian Bootstrap",
    "text": "Bayesian Bootstrap\nI have explained the bootstrap before. In this section, I introduce the Bayesian Bootstrap and it’s R implementation 2. In the traditional bootstrap we resample observations from the data with replacement so that we create new datasets which are similar but different from the original data. The disadvantage of the traditional approach is that the implicit weight on an observation in each resampled data is discrete and can be 0. The Bayesian bootstrap bootstrap creates explicit weights based on a Dirichlet distribution which gives a weight between for each observation so that the sum of the weights equals 1.\nWe do not need to know this to use the Bayesian bootstrap. We can just use the bayesboot package. The only thing we need to do is to create a function that returns the estimate that we are interested in based on the data and the weights. We can use the bootstrap function and tell it to start with the data_2sls, resample it R = 1000 times, calculate the beta with get_beta, and use weights in that calculation.\n\nget_beta &lt;- function(data, weights){\n  tsls_manual1 &lt;- lm(x ~ iv, data = data, weights = weights)\n  data$fit_x &lt;- fitted(tsls_manual1)\n  tsls_manual2 &lt;- lm(y ~ fit_x, data = data, weights = weights)\n  beta &lt;- coefficients(tsls_manual2)[\"fit_x\"]\n  return(beta)\n}\nget_beta(data_2sls, weights = rep(1/N, times = N))\n\n  fit_x \n1.03956 \n\nbootstrap &lt;- bayesboot(data_2sls, get_beta, R = 1000,\n                       use.weights = TRUE)\nsummary(bootstrap)\n\nBayesian bootstrap\n\nNumber of posterior draws: 1000 \n\nSummary of the posterior (with 95% Highest Density Intervals):\n statistic     mean         sd   hdi.low hdi.high\n     fit_x 1.038238 0.06325506 0.9135051 1.163498\n\nQuantiles:\n statistic     q2.5%      q25%  median     q75%   q97.5%\n     fit_x 0.9146771 0.9978153 1.03673 1.080914 1.166789\n\nCall:\n bayesboot(data = data_2sls, statistic = get_beta, R = 1000, use.weights = TRUE)"
  },
  {
    "objectID": "generated/introduction.html#the-bayesian-bootstrap-by-hand-and",
    "href": "generated/introduction.html#the-bayesian-bootstrap-by-hand-and",
    "title": "Introduction",
    "section": "The Bayesian Bootstrap by hand and",
    "text": "The Bayesian Bootstrap by hand and\nIf you want to calculate the Bayesian bootstrap by hand, I give the code below. It’s a good introduction on how to run efficient simulations with the tidyverse 3. I need to slightly change the function because we are going to put the simulated weights in the data. The next part is to actual create nsim versions of the data. The trick here is to say that we want the data data_2sls where we repeat each row nsim times.\nThe rest of the code follows more easy to understand patterns. We first create a sim variable so that we can keep track of different data simulations. In this case, the data is the same but we create new weights for each simulation. The weights are derived from a Dirichlet distribution which you can generate by an exponential distribution divided by the sum of the generated values (per simulation). We only keep the variables that are necessary in our get_beta_2 function. Then we create a dataset per simulation with the nest function and calculate the estimate in the last mutate step. And that is the Bayesian bootstrap as a simulation exercise.\n\nget_beta_2 &lt;- function(data){\n  tsls_manual1 &lt;- lm(x ~ iv, data = data, weights = weights)\n  data$fit_x &lt;- fitted(tsls_manual1)\n  tsls_manual2 &lt;- lm(y ~ fit_x, data = data, weights = weights)\n  beta &lt;- coefficients(tsls_manual2)[\"fit_x\"]\n  return(beta)\n}\nnsim &lt;- 1000\nnrows &lt;- nrow(data_2sls)\nbootstrap2 &lt;-\n  data_2sls[rep(1:nrows, times = nsim),] %&gt;%\n  mutate(sim = rep(1:nsim, each = nrows),\n         raw_weights = rexp(nsim *  nrows, 1)) %&gt;%\n  mutate(weights = raw_weights/sum(raw_weights), .by = sim) %&gt;%\n  select(sim, iv, x, y, weights) %&gt;%\n  nest(.by = sim) %&gt;%\n  mutate(beta = map_dbl(data, get_beta_2)) %&gt;%\n  print()\n\n# A tibble: 1,000 × 3\n     sim data                   beta\n   &lt;int&gt; &lt;list&gt;                &lt;dbl&gt;\n 1     1 &lt;tibble [10,000 × 4]&gt; 0.932\n 2     2 &lt;tibble [10,000 × 4]&gt; 1.15 \n 3     3 &lt;tibble [10,000 × 4]&gt; 0.994\n 4     4 &lt;tibble [10,000 × 4]&gt; 1.01 \n 5     5 &lt;tibble [10,000 × 4]&gt; 1.06 \n 6     6 &lt;tibble [10,000 × 4]&gt; 1.12 \n 7     7 &lt;tibble [10,000 × 4]&gt; 1.01 \n 8     8 &lt;tibble [10,000 × 4]&gt; 1.04 \n 9     9 &lt;tibble [10,000 × 4]&gt; 1.07 \n10    10 &lt;tibble [10,000 × 4]&gt; 1.08 \n# ℹ 990 more rows\n\nsummarise(bootstrap2,\n          estimate = mean(beta),\n          se = sd(beta))\n\n# A tibble: 1 × 2\n  estimate     se\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     1.04 0.0622"
  },
  {
    "objectID": "generated/introduction.html#footnotes",
    "href": "generated/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich you should never have to do!↩︎\nYou can also find a thorough discussion here.↩︎\nThis advice is based on the [Grant McDermott’s advice] for very efficient simulations. I did not implement everything but I did use most of the principles in the tidyverse setting.↩︎"
  },
  {
    "objectID": "generated/residual_dependent.html",
    "href": "generated/residual_dependent.html",
    "title": "Generated Independent",
    "section": "",
    "text": "This page is based on Chen, Hribar, and Melessa (2018) who show the potential problems of using a two-stage approach where in the second stage, we use the residuals from the first regression as a dependent variable. This approach is quite popular in the accounting and finance literature. The paper shows formally and with simulation that (1) these models can typically be estimated with 1 regression and that (2) the two-step procedure can be biased if they are not used carefully. The paper also re-analyses a number of accounting studies and shows that the results sometimes meaningfully change.\nI am going to illustrate the issue with a number of simulated examples. As always, we will be interested in the effect of a variable x on y where we want to control for a variable z. The bias of the two-stage procedure will often depend on unobserved correlations between those variables which I will model with the unobserved variable w.\nThe two-step approach is to: - First run a regression y ~ z and calculate the residuals. - Second use those residuals to estimate the effect of x on the residuals residuals ~ x\nThe one-step approach is to just run the regression y ~ x + z."
  },
  {
    "objectID": "generated/residual_dependent.html#introduction",
    "href": "generated/residual_dependent.html#introduction",
    "title": "Generated Independent",
    "section": "",
    "text": "This page is based on Chen, Hribar, and Melessa (2018) who show the potential problems of using a two-stage approach where in the second stage, we use the residuals from the first regression as a dependent variable. This approach is quite popular in the accounting and finance literature. The paper shows formally and with simulation that (1) these models can typically be estimated with 1 regression and that (2) the two-step procedure can be biased if they are not used carefully. The paper also re-analyses a number of accounting studies and shows that the results sometimes meaningfully change.\nI am going to illustrate the issue with a number of simulated examples. As always, we will be interested in the effect of a variable x on y where we want to control for a variable z. The bias of the two-stage procedure will often depend on unobserved correlations between those variables which I will model with the unobserved variable w.\nThe two-step approach is to: - First run a regression y ~ z and calculate the residuals. - Second use those residuals to estimate the effect of x on the residuals residuals ~ x\nThe one-step approach is to just run the regression y ~ x + z."
  },
  {
    "objectID": "generated/residual_dependent.html#setup",
    "href": "generated/residual_dependent.html#setup",
    "title": "Generated Independent",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(cowplot)\ntheme_set(theme_cowplot(font_size = 18))\nlibrary(broom)\nlibrary(here)\ni_am(\"generated/residual_dependent.qmd\")\n\nhere() starts at /Users/stijn/Dropbox/Teaching/lecturenotes/method_package\n\nlibrary(modelsummary)\n\n\nN &lt;- 1000\ngof_map &lt;- c(\"nobs\", \"r.squared\")\nset.seed(230383)"
  },
  {
    "objectID": "generated/residual_dependent.html#correlated-step-1-control",
    "href": "generated/residual_dependent.html#correlated-step-1-control",
    "title": "Generated Independent",
    "section": "Correlated Step 1 control",
    "text": "Correlated Step 1 control\nThe basic problem is that in the two-step approach researchers regularly include z in the first regression and not in the second regression. If z and x are correlated, the two-step procedure will be biased downwards if the second step does not control for z. This can be seen in the simulated example below. I also show that the bias can be avoided by running the one-step regression, add z as a control variable, or use z to residualise both x and y.\n\nd1 &lt;- tibble(w = rnorm(N)) %&gt;%\n  mutate(z = rnorm(N, w, 1),\n         x = rnorm(N, w, 1),\n         y = rnorm(N, x + z, 10))\nlm1 &lt;- lm(y ~ x + z, data = d1)\nlm1x &lt;- lm(y ~ z, data = d1)\nlm1y &lt;- lm(x ~ z, data = d1)\nd1 &lt;- mutate(d1,\n             resid_y = resid(lm1x),\n             resid_x = resid(lm1y))\nlm1resy &lt;- lm(resid_y ~ x, data = d1)\nlm1resyz &lt;- lm(resid_y ~ x + z, data = d1)\nlm1resyx &lt;- lm(resid_y ~ resid_x, data = d1)\nmodelsummary(list(onestep = lm1, no_control = lm1resy,\n                  with_control = lm1resyz, double_resid = lm1resyx),\n             gof_map = gof_map)\n\n\n\n\n\nonestep\nno_control\nwith_control\ndouble_resid\n\n\n\n\n(Intercept)\n−0.070\n0.001\n0.002\n0.000\n\n\n\n(0.305)\n(0.306)\n(0.305)\n(0.305)\n\n\nx\n1.294\n0.938\n1.294\n\n\n\n\n(0.255)\n(0.218)\n(0.255)\n\n\n\nz\n0.495\n\n−0.672\n\n\n\n\n(0.253)\n\n(0.253)\n\n\n\nresid_x\n\n\n\n1.294\n\n\n\n\n\n\n(0.255)\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n\n\nR2\n0.052\n0.018\n0.025\n0.025"
  },
  {
    "objectID": "generated/residual_dependent.html#extra-controls-in-step-2",
    "href": "generated/residual_dependent.html#extra-controls-in-step-2",
    "title": "Generated Independent",
    "section": "Extra controls in step 2",
    "text": "Extra controls in step 2\nIn the next step, we can include extra controls, z2, in the second step. The bias will now depend on the correlations that the extra control has with the first stage controls and x and y. More importantly, the bias can now be an overestimation or underestimation. If the additional control is not correlated to x or z1, we still have the downward bias from before.\n\nd2 &lt;- d1 %&gt;%\n  rename(z1 = z) %&gt;%\n  mutate(z2 = rnorm(N, 0, 1),\n         y = rnorm(N, x + z1 + z2, 10))\nlm2 &lt;- lm(y ~ x + z1 + z2, data = d2)\nlm2y &lt;- lm(y ~ z1, data = d2)\nd2 &lt;- d2 %&gt;%\n  mutate(resid_y = resid(lm2y))\nlm2resy &lt;- lm(resid_y ~ x + z2, data = d2)\nmodelsummary(list(onestep = lm2, twostep = lm2resy),\n             gof_map = gof_map)\n\n\n\n\n\nonestep\ntwostep\n\n\n\n\n(Intercept)\n0.123\n0.000\n\n\n\n(0.320)\n(0.321)\n\n\nx\n0.961\n0.682\n\n\n\n(0.269)\n(0.229)\n\n\nz1\n1.438\n\n\n\n\n(0.266)\n\n\n\nz2\n0.938\n0.929\n\n\n\n(0.318)\n(0.318)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.089\n0.018"
  },
  {
    "objectID": "generated/residual_dependent.html#extra-correlated-controls-in-step-2",
    "href": "generated/residual_dependent.html#extra-correlated-controls-in-step-2",
    "title": "Generated Independent",
    "section": "Extra correlated controls in step 2",
    "text": "Extra correlated controls in step 2\nHowever, when the additional control is negatively correlated to both z1 and x but positively to y, we get an upward bias.\n\nd3 &lt;- d2 %&gt;%\n  mutate(z3 = rnorm(N, - 2 * w, 1),\n         y = rnorm(N, x + z1 + z3, 10)) %&gt;%\n  mutate(resid_y = resid(lm(y ~ z1, data = .)))\nlm3 &lt;- lm(y ~ x + z1 + z3, data = d3)\nlm3resy &lt;- lm(resid_y ~ x + z3, data = d3)\nmodelsummary(list(onestep = lm3, twostep = lm3resy),\n             gof_map = gof_map)\n\n\n\n\n\nonestep\ntwostep\n\n\n\n\n(Intercept)\n0.080\n−0.012\n\n\n\n(0.321)\n(0.321)\n\n\nx\n0.929\n1.009\n\n\n\n(0.297)\n(0.291)\n\n\nz1\n0.773\n\n\n\n\n(0.306)\n\n\n\nz3\n0.850\n0.717\n\n\n\n(0.207)\n(0.180)\n\n\nNum.Obs.\n1000\n1000\n\n\nR2\n0.021\n0.017"
  },
  {
    "objectID": "generated/residual_dependent.html#run-simulations-as-a-large-dataframe",
    "href": "generated/residual_dependent.html#run-simulations-as-a-large-dataframe",
    "title": "Generated Independent",
    "section": "Run simulations as a large dataframe",
    "text": "Run simulations as a large dataframe\nWe can use a simulation approach to see whether that bias is persistent and not just a coincedence in the simulated data. First, we create the two functions that run the two step and one step approach and return the estimate and the p-value from the coefficient table.\n\ntwo_step &lt;- function(data){\n  lm1 &lt;- lm(y ~ z1, data)\n  lm2 &lt;- lm(resid(lm1) ~ x + z3, data = data)\n  coefs &lt;- summary(lm2)$coefficients\n  result &lt;- coefs[\"x\", c(1, 4)]\n  names(result) &lt;- c(\"estimate\", \"pvalue\")\n  return(result)\n}\none_step &lt;- function(data){\n  lm &lt;- lm(y ~ x + z1 + z3, data = data)\n  coefs &lt;- summary(lm)$coefficients\n  result &lt;- coefs[\"x\", c(1, 4)]\n  names(result) &lt;- c(\"estimate\", \"pvalue\")\n  return(result)\n}\ntwo_step(d3)\n\n    estimate       pvalue \n1.0094585099 0.0005442113 \n\none_step(d3)\n\n   estimate      pvalue \n0.929023153 0.001822299 \n\n\nNext, we set up the simulation for 1000 simulations with 100 observations per data set. One trick for efficient simulations is to generate the data in one big data frame. You can see that we can still use the two step approach with tibble and mutate for exogenous and endogenous variables. We just need to be careful with the number of observations and use ntotal.\n\nN &lt;- 100\nnsim &lt;- 1000\nntotal &lt;- nsim * N\nsimdata &lt;-\n  tibble(\n    sample = rep(1:nsim, each = N),\n    w = rnorm(ntotal)) %&gt;%\n  mutate(\n    z1 = rnorm(ntotal, w, 1),\n    x = rnorm(ntotal, w, 1),\n    z3 = rnorm(ntotal, - 2 * w, 1),\n    y = rnorm(ntotal, x + z1 + z3, 10))\n\nWe can than use nest to create separate data by sample and calculate the estimate and p-value for each sample with our functions. The results are a vector with two values and we use unnest_wider to create separate columns. Finally, I calculate the difference between the two estimates.\n\nresults &lt;- simdata %&gt;%\n  nest(.by = sample) %&gt;%\n  mutate(two = map(.x = data, .f = ~ two_step(.x)),\n         one = map(.x = data, .f = ~ one_step(.x))) %&gt;%\n  select(-data) %&gt;%\n  unnest_wider(c(one, two), names_sep = \"_\") %&gt;%\n  mutate(bias = two_estimate - one_estimate)\n\nNext, we can summarise the results and we see that the two step approach is likely to overestimate the effect of x.\n\nresults %&gt;%\n  summarise(M = mean(bias), se = sd(bias)/sqrt(n()), \n            M_one = mean(one_estimate), M_two = mean(two_estimate))\n\n# A tibble: 1 × 4\n       M      se M_one M_two\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.0860 0.00427  1.02  1.10\n\n\nThe last part of the code calculates the percentage of simulated samples that gives a significant effect. If we are worried that we might not find a significant effect while there is one, this might be a legitimate problem. In this case, the bad approach is more likely to report a significant effect. It’s not always easy doing the right thing.\n\nresults %&gt;% select(sample, two_pvalue, one_pvalue) %&gt;%\n  pivot_longer(c(two_pvalue, one_pvalue),\n               names_to = \"var\", values_to = \"pvalue\") %&gt;%\n  mutate(is_sign = if_else(pvalue &lt; 0.05, 1, 0)) %&gt;%\n  summarise(mean = mean(is_sign), .by = var)\n\n# A tibble: 2 × 2\n  var         mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 two_pvalue 0.21 \n2 one_pvalue 0.173"
  }
]